{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 8000 8000 8000\n",
      "The system as described above has its greatest application in an arrayed configuration of antenna elements 7 12 15\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import codecs\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque  \n",
    "relation2id = {\n",
    "\"Other\": 0,\n",
    "\"Cause-Effect\": 1,\n",
    "\"Instrument-Agency\":2,\n",
    "\"Product-Producer\":3,\n",
    "\"Content-Container\":4,\n",
    "\"Entity-Origin\":5,\n",
    "\"Entity-Destination\":6,\n",
    "\"Component-Whole\":7,\n",
    "\"Member-Collection\":8,\n",
    "\"Message-Topic\":9\n",
    "}\n",
    "datas = []\n",
    "labels = []\n",
    "entity1 = []\n",
    "entity2 = []\n",
    "with codecs.open('/Users/denhiroshi/Downloads/ChineseNRE-master/data/SemEval2010_task8_all_data/TRAIN_FILE.TXT','r','utf-8') as tra:\n",
    "    linenum = 0\n",
    "    for line in tra:\n",
    "        linenum+=1\n",
    "        if linenum%4==1:\n",
    "            line = line.split('\\t')[1]\n",
    "            word_arr = line[1:-4].split()\n",
    "            for index in range(len(word_arr)):\n",
    "                if \"<e1>\" in word_arr[index]:\n",
    "                    entity1.append(index)\n",
    "                elif \"<e2>\" in word_arr[index]:\n",
    "                    entity2.append(index)\n",
    "\n",
    "            line = line.replace(\"<e1>\",\"\")\n",
    "            line = line.replace(\"</e1>\",\"\")\n",
    "            line = line.replace(\"<e2>\",\"\")\n",
    "            line = line.replace(\"</e2>\",\"\")\n",
    "            line = re.sub(r'[^\\w\\s]','',line)\n",
    "            datas.append(line[0:-2])\n",
    "        elif linenum%4==2:\n",
    "            if line==\"Other\\r\\n\":\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                line = line.split('(')\n",
    "                labels.append(relation2id[line[0]])\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "print(len(datas),len(labels),len(entity1),len(entity2))\n",
    "print(datas[0],labels[0],entity1[0],entity2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "from torch.nn import init\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv = []\n",
    "for i in range(len(datas)):\n",
    "    write_csv.append((str(i), datas[i], labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "headers = ['id', 'Text', 'label']\n",
    "with open('glove_WV_2010_task8.csv','w')as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    f_csv.writerow(headers)\n",
    "    f_csv.writerows(write_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8000it [00:00, 49304.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torchtext.data.dataset.Dataset at 0x10b70e9b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dataset(csv_data, text_field, label_field, test=False):\n",
    "\n",
    "    fields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"Text\", text_field), (\"label\", label_field)]       \n",
    "    examples = []\n",
    "\n",
    "    if test:\n",
    "        # 如果为测试集，则不加载label\n",
    "        for text in tqdm(csv_data['Text']):\n",
    "            examples.append(data.Example.fromlist([None, text, None], fields))\n",
    "    else:\n",
    "        for text, label in tqdm(zip(csv_data['Text'], csv_data['label'])):\n",
    "            examples.append(data.Example.fromlist([None, text, label], fields))\n",
    "    return examples, fields\n",
    "tokenize = lambda x: x.split()\n",
    "train_data = pd.read_csv('glove_WV_2010_task8.csv')\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "# 得到构建Dataset所需的examples和fields\n",
    "train_examples, train_fields = get_dataset(train_data, TEXT, LABEL)\n",
    "# 构建Dataset数据集\n",
    "train = data.Dataset(train_examples, train_fields)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'system',\n",
       " 'as',\n",
       " 'described',\n",
       " 'above',\n",
       " 'has',\n",
       " 'its',\n",
       " 'greatest',\n",
       " 'application',\n",
       " 'in',\n",
       " 'an',\n",
       " 'arrayed',\n",
       " 'configuration',\n",
       " 'of',\n",
       " 'antenna',\n",
       " 'elements']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.examples[0].Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 13014),\n",
       " ('of', 5488),\n",
       " ('a', 5376),\n",
       " ('and', 3372),\n",
       " ('in', 3071),\n",
       " ('to', 2590),\n",
       " ('is', 1645),\n",
       " ('from', 1446),\n",
       " ('was', 1439),\n",
       " ('by', 1267)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.build_vocab(train)\n",
    "vocab = TEXT.vocab\n",
    "vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "cache = '.vector_cache'\n",
    "if not os.path.exists(cache):\n",
    "    os.mkdir(cache)\n",
    "vectors = Vectors(name='/Users/denhiroshi/Downloads/glove/glove.6B.200d.txt', cache=cache)\n",
    "TEXT.build_vocab(train, vectors=vectors)\n",
    "torch.sum(TEXT.vocab.vectors != 0)/200,TEXT.vocab.vectors.shape\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "train_iter = Iterator(dataset = train, batch_size=64, device=-1, shuffle=False, sort_within_batch=False, repeat=False)\n",
    "weight_matrix = TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "weight_matrix = TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WV(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(WV, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(len(TEXT.vocab), 200)  # embedding之后的shape: torch.Size([200, 8, 300])\n",
    "        self.word_embeddings.weight.data.copy_(weight_matrix)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WV()\n",
    "emm = []\n",
    "model.train()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "loss_funtion = F.cross_entropy\n",
    "for step, batch_train in enumerate(train_iter):\n",
    "    predicted = model(batch_train.Text)\n",
    "    emm.append(predicted)\n",
    "len(emm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 0\n",
    "for i in range(len(emm)):\n",
    "    MAX_LENGTH = max(MAX_LENGTH, emm[i].shape[0])\n",
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = []\n",
    "for temp in emm:\n",
    "    for i in range(temp.shape[1]):\n",
    "        now = temp[:,i,:]\n",
    "        now = np.array(now.tolist())\n",
    "        if now.shape[0] < MAX_LENGTH:\n",
    "            add = MAX_LENGTH - now.shape[0]\n",
    "            adds = np.zeros((add,200))\n",
    "            now = np.concatenate((now, adds))\n",
    "        save.append(now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 85, 200)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save = np.stack(save)\n",
    "save.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save.tofile('tensors_2010'+'.dat',format='%s')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.stack(list(labels))\n",
    "entity1 = np.stack(list(entity1))\n",
    "entity2 = np.stack(list(entity2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7, 0, 2, ..., 6, 0, 3]),\n",
       " array([12,  1,  1, ...,  7, 17,  1]),\n",
       " array([15,  9,  7, ..., 10, 18,  5]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label,entity1,entity2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.tofile('tensors_2010_labels'+'.dat',format='%s')\n",
    "entity1.tofile('tensors_2010_entity1'+'.dat',format='%s')\n",
    "entity2.tofile('tensors_2010_entity2'+'.dat',format='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
