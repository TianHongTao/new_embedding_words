{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function set_backend in module tensorly:\n",
      "\n",
      "set_backend(backend_name)\n",
      "    Sets the backend for TensorLy\n",
      "    \n",
      "        The backend will be set as specified and operations will used that backend\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    backend_name : {'mxnet', 'numpy', 'pytorch', 'tensorflow', 'cupy'}, default is 'numpy'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pytorch backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.decomposition import tucker\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import preprocessing\n",
    "from random import shuffle\n",
    "BATCH_SIZE = 128\n",
    "tl.set_backend('pytorch')\n",
    "help(tl.set_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467,)\n"
     ]
    }
   ],
   "source": [
    "tag = [72, 71, 84, 54, 57, 65, 64]\n",
    "Y = np.zeros(467)\n",
    "print(Y.shape)\n",
    "Y[0 : tag[0]] = 0\n",
    "Y[tag[0] : tag[0]+tag[1]] = 1\n",
    "Y[tag[0]+tag[1] : tag[0]+tag[1]+tag[2]] = 2\n",
    "Y[tag[0]+tag[1]+tag[2] : tag[0]+tag[1]+tag[2]+tag[3]] = 3\n",
    "Y[tag[0]+tag[1]+tag[2]+tag[3] : tag[0]+tag[1]+tag[2]+tag[3]+tag[4]] = 4\n",
    "Y[tag[0]+tag[1]+tag[2]+tag[3]+tag[4] : tag[0]+tag[1]+tag[2]+tag[3]+tag[4]+tag[5]] = 5\n",
    "Y[tag[0]+tag[1]+tag[2]+tag[3]+tag[4]+tag[5] : tag[0]+tag[1]+tag[2]+tag[3]+tag[4]+tag[5]+tag[6]] = 6\n",
    "Y = Y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "max_length = 0\n",
    "for i in range(0,467):\n",
    "    a = np.loadtxt('/Users/denhiroshi/Downloads/Embeding/ans_1.0_'+str(i)+'.csv',dtype=np.float64,delimiter=',')\n",
    "    a = a.transpose(1,0)\n",
    "    a = np.exp(a)\n",
    "    a = np.sum(a,axis=0)\n",
    "    a = np.log(a)\n",
    "    inputs.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use = np.stack(inputs)\n",
    "mean = use.mean()         #计算平均数\n",
    "deviation = use.std()     #计算标准差\n",
    "# 标准化数据的公式: (数据值 - 平均数) / 标准差\n",
    "use = (use - mean) / deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.fromfile('glove_WV.dat',dtype=np.float64).reshape((467,-1,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((467, 53, 200), (467, 467))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape,use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for i in range(467):\n",
    "    inputs.append((torch.from_numpy(b[i]),torch.from_numpy(use[i]),Y[i]))\n",
    "shuffle(inputs)\n",
    "flag = int(len(inputs)*0.7)\n",
    "train = inputs[:flag]\n",
    "test = inputs[flag:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([141, 467]), torch.Size([141, 53, 200]), torch.Size([141]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_s = []\n",
    "t_e = []\n",
    "t_y = []\n",
    "for E,S,Y in test:\n",
    "    t_s.append(S)\n",
    "    t_e.append(E)\n",
    "    t_y.append(Y)\n",
    "t_s = torch.stack(t_s)\n",
    "t_e = torch.stack(t_e)\n",
    "t_y = torch.from_numpy(np.array(t_y))\n",
    "t_s.shape, t_e.shape, t_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glove_rnn_max: 0.6453900709219859 24轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN_max(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size = 200,       # rnn input\n",
    "            hidden_size = 64,     # rnn hidden unit\n",
    "            num_layers = 1,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=64,out_features=7)\n",
    "        self.maxpool = torch.nn.MaxPool2d((53,1))\n",
    "\n",
    "    def forward(self,sen,x):\n",
    "        output,(h_n,c_n)=self.rnn(x.float())\n",
    "        output_in_last_timestep = self.maxpool(output)\n",
    "        output_in_last_timestep = output_in_last_timestep.reshape((output_in_last_timestep.shape[0],-1))\n",
    "        x=self.out(output_in_last_timestep)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_max(\n",
      "  (rnn): LSTM(200, 64, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=7, bias=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=(53, 1), stride=(53, 1), padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "1: accuracy:0.23404255319148937 loss: 1.900092601776123\n",
      "2: accuracy:0.36879432624113473 loss: 1.774414300918579\n",
      "3: accuracy:0.3900709219858156 loss: 1.5358353853225708\n",
      "4: accuracy:0.425531914893617 loss: 1.3111639022827148\n",
      "5: accuracy:0.5531914893617021 loss: 1.1252729892730713\n",
      "6: accuracy:0.574468085106383 loss: 0.908991813659668\n",
      "7: accuracy:0.6099290780141844 loss: 0.7280575633049011\n",
      "8: accuracy:0.5957446808510638 loss: 0.4345526695251465\n",
      "9: accuracy:0.5886524822695035 loss: 0.4153765141963959\n",
      "10: accuracy:0.5177304964539007 loss: 0.2946902811527252\n",
      "11: accuracy:0.574468085106383 loss: 0.29680508375167847\n",
      "12: accuracy:0.6028368794326241 loss: 0.24011000990867615\n",
      "13: accuracy:0.6099290780141844 loss: 0.20296014845371246\n",
      "14: accuracy:0.5886524822695035 loss: 0.09348371624946594\n",
      "15: accuracy:0.6312056737588653 loss: 0.08753255009651184\n",
      "16: accuracy:0.6028368794326241 loss: 0.06971685588359833\n",
      "17: accuracy:0.6170212765957447 loss: 0.037679366767406464\n",
      "18: accuracy:0.6382978723404256 loss: 0.035973433405160904\n",
      "19: accuracy:0.624113475177305 loss: 0.01908108778297901\n",
      "20: accuracy:0.5957446808510638 loss: 0.01621018536388874\n",
      "21: accuracy:0.624113475177305 loss: 0.014411943033337593\n",
      "22: accuracy:0.6170212765957447 loss: 0.009667103178799152\n",
      "23: accuracy:0.6382978723404256 loss: 0.010378854349255562\n",
      "24: accuracy:0.6453900709219859 loss: 0.010467938147485256\n",
      "25: accuracy:0.624113475177305 loss: 0.006173379253596067\n",
      "26: accuracy:0.6028368794326241 loss: 0.004763017408549786\n",
      "27: accuracy:0.6099290780141844 loss: 0.004925714340060949\n",
      "28: accuracy:0.6099290780141844 loss: 0.005597291514277458\n",
      "29: accuracy:0.6099290780141844 loss: 0.0033958640415221453\n",
      "30: accuracy:0.6170212765957447 loss: 0.003988974262028933\n",
      "31: accuracy:0.6170212765957447 loss: 0.0033880097325891256\n",
      "32: accuracy:0.6099290780141844 loss: 0.003099727677181363\n",
      "33: accuracy:0.6099290780141844 loss: 0.0022556576877832413\n",
      "34: accuracy:0.6099290780141844 loss: 0.0033023767173290253\n",
      "35: accuracy:0.6170212765957447 loss: 0.002286979230120778\n",
      "36: accuracy:0.6170212765957447 loss: 0.0026391437277197838\n",
      "37: accuracy:0.6099290780141844 loss: 0.002332019852474332\n",
      "38: accuracy:0.6099290780141844 loss: 0.0019149507861584425\n",
      "39: accuracy:0.6099290780141844 loss: 0.0019693919457495213\n",
      "40: accuracy:0.6099290780141844 loss: 0.0018998145824298263\n",
      "41: accuracy:0.6170212765957447 loss: 0.0017071111360564828\n",
      "42: accuracy:0.6170212765957447 loss: 0.0017073085764423013\n",
      "43: accuracy:0.6170212765957447 loss: 0.0013753618113696575\n",
      "44: accuracy:0.6170212765957447 loss: 0.0016972337616607547\n",
      "45: accuracy:0.6170212765957447 loss: 0.0012844903394579887\n",
      "46: accuracy:0.624113475177305 loss: 0.0013254302321001887\n",
      "47: accuracy:0.624113475177305 loss: 0.0014568192418664694\n",
      "48: accuracy:0.624113475177305 loss: 0.0010605608113110065\n",
      "49: accuracy:0.624113475177305 loss: 0.0012028489727526903\n",
      "50: accuracy:0.624113475177305 loss: 0.0012841293355450034\n",
      "51: accuracy:0.624113475177305 loss: 0.0010692051146179438\n",
      "52: accuracy:0.624113475177305 loss: 0.0010371480602771044\n",
      "53: accuracy:0.624113475177305 loss: 0.0013224942376837134\n",
      "54: accuracy:0.6312056737588653 loss: 0.001312303589656949\n",
      "55: accuracy:0.6312056737588653 loss: 0.0009306566789746284\n",
      "56: accuracy:0.6312056737588653 loss: 0.0010512556182220578\n",
      "57: accuracy:0.6312056737588653 loss: 0.0009425367461517453\n",
      "58: accuracy:0.6312056737588653 loss: 0.0010609899181872606\n",
      "59: accuracy:0.6312056737588653 loss: 0.0008630139636807144\n",
      "60: accuracy:0.6312056737588653 loss: 0.0009057726128958166\n",
      "61: accuracy:0.6312056737588653 loss: 0.0008456979412585497\n",
      "62: accuracy:0.6382978723404256 loss: 0.0008064747089520097\n",
      "63: accuracy:0.6382978723404256 loss: 0.0006464754114858806\n",
      "64: accuracy:0.6382978723404256 loss: 0.0007238524267449975\n",
      "65: accuracy:0.6382978723404256 loss: 0.0008459091186523438\n",
      "66: accuracy:0.6382978723404256 loss: 0.0005761487409472466\n",
      "67: accuracy:0.6382978723404256 loss: 0.0007379259332083166\n",
      "68: accuracy:0.6312056737588653 loss: 0.0006215980974957347\n",
      "69: accuracy:0.6312056737588653 loss: 0.0007391520775854588\n",
      "70: accuracy:0.6312056737588653 loss: 0.0006240435759536922\n",
      "71: accuracy:0.6312056737588653 loss: 0.0006869860808365047\n",
      "72: accuracy:0.6312056737588653 loss: 0.0006702286773361266\n",
      "73: accuracy:0.6312056737588653 loss: 0.0006224428070709109\n",
      "74: accuracy:0.6312056737588653 loss: 0.0006296021747402847\n",
      "75: accuracy:0.6312056737588653 loss: 0.0005532946088351309\n",
      "76: accuracy:0.6312056737588653 loss: 0.0005330698913894594\n",
      "77: accuracy:0.6312056737588653 loss: 0.0005834375042468309\n",
      "78: accuracy:0.6312056737588653 loss: 0.0005558150005526841\n",
      "79: accuracy:0.6312056737588653 loss: 0.0005477087688632309\n",
      "80: accuracy:0.624113475177305 loss: 0.0005597251001745462\n",
      "81: accuracy:0.6170212765957447 loss: 0.0005695819854736328\n",
      "82: accuracy:0.6170212765957447 loss: 0.0005217483849264681\n",
      "83: accuracy:0.6170212765957447 loss: 0.0005690574762411416\n",
      "84: accuracy:0.6170212765957447 loss: 0.0005107198376208544\n",
      "85: accuracy:0.6170212765957447 loss: 0.0005471161566674709\n",
      "86: accuracy:0.6170212765957447 loss: 0.0004887580871582031\n",
      "87: accuracy:0.6170212765957447 loss: 0.000530617602635175\n",
      "88: accuracy:0.6170212765957447 loss: 0.0005680697504431009\n",
      "89: accuracy:0.6170212765957447 loss: 0.0003720079257618636\n",
      "90: accuracy:0.6170212765957447 loss: 0.0004774161789100617\n",
      "91: accuracy:0.6170212765957447 loss: 0.0004719802236650139\n",
      "92: accuracy:0.6170212765957447 loss: 0.00046036584535613656\n",
      "93: accuracy:0.6170212765957447 loss: 0.0004435198788996786\n",
      "94: accuracy:0.6170212765957447 loss: 0.0004419531032908708\n",
      "95: accuracy:0.624113475177305 loss: 0.00043948719394393265\n",
      "96: accuracy:0.624113475177305 loss: 0.00033623832860030234\n",
      "97: accuracy:0.624113475177305 loss: 0.0004011562850791961\n",
      "98: accuracy:0.624113475177305 loss: 0.0003836631658487022\n",
      "99: accuracy:0.6170212765957447 loss: 0.0004351820389274508\n",
      "100: accuracy:0.6170212765957447 loss: 0.0003673757892102003\n",
      "101: accuracy:0.6170212765957447 loss: 0.0004533699539024383\n",
      "102: accuracy:0.6099290780141844 loss: 0.00035297530121169984\n",
      "103: accuracy:0.6170212765957447 loss: 0.0003968511300627142\n",
      "104: accuracy:0.6170212765957447 loss: 0.0004089559952262789\n",
      "105: accuracy:0.6170212765957447 loss: 0.0003769397735595703\n",
      "106: accuracy:0.6170212765957447 loss: 0.0003250871377531439\n",
      "107: accuracy:0.6170212765957447 loss: 0.0003406456671655178\n",
      "108: accuracy:0.6170212765957447 loss: 0.0003778185055125505\n",
      "109: accuracy:0.6170212765957447 loss: 0.0003233160241506994\n",
      "110: accuracy:0.6170212765957447 loss: 0.0003076621505897492\n",
      "111: accuracy:0.6170212765957447 loss: 0.00030276435427367687\n",
      "112: accuracy:0.6170212765957447 loss: 0.0003501892206259072\n",
      "113: accuracy:0.6099290780141844 loss: 0.00032477377681061625\n",
      "114: accuracy:0.6099290780141844 loss: 0.0002925668377429247\n",
      "115: accuracy:0.6099290780141844 loss: 0.00029183796141296625\n",
      "116: accuracy:0.6099290780141844 loss: 0.0003263882244937122\n",
      "117: accuracy:0.6099290780141844 loss: 0.0002709933614823967\n",
      "118: accuracy:0.6099290780141844 loss: 0.00028394971741363406\n",
      "119: accuracy:0.6099290780141844 loss: 0.00028624534024856985\n",
      "120: accuracy:0.6099290780141844 loss: 0.00034107480314560235\n",
      "121: accuracy:0.6028368794326241 loss: 0.00022192001051735133\n",
      "122: accuracy:0.6028368794326241 loss: 0.0002349853457417339\n",
      "123: accuracy:0.6028368794326241 loss: 0.0002740451309364289\n",
      "124: accuracy:0.6028368794326241 loss: 0.0002847058349289\n",
      "125: accuracy:0.6028368794326241 loss: 0.0002913475036621094\n",
      "126: accuracy:0.6028368794326241 loss: 0.0002985886239912361\n",
      "127: accuracy:0.6028368794326241 loss: 0.00027538707945495844\n",
      "128: accuracy:0.6028368794326241 loss: 0.00027293479070067406\n",
      "129: accuracy:0.6028368794326241 loss: 0.00031172888702712953\n",
      "130: accuracy:0.6028368794326241 loss: 0.0002821309317369014\n",
      "131: accuracy:0.6028368794326241 loss: 0.0002535207022447139\n",
      "132: accuracy:0.6028368794326241 loss: 0.00023685183259658515\n",
      "133: accuracy:0.6028368794326241 loss: 0.0002324649249203503\n",
      "134: accuracy:0.6028368794326241 loss: 0.00023600032727699727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135: accuracy:0.6028368794326241 loss: 0.00021398407989181578\n",
      "136: accuracy:0.6028368794326241 loss: 0.0002589430077932775\n",
      "137: accuracy:0.6028368794326241 loss: 0.0002392223832430318\n",
      "138: accuracy:0.6028368794326241 loss: 0.00023429734574165195\n",
      "139: accuracy:0.6028368794326241 loss: 0.00023956298537086695\n",
      "140: accuracy:0.6028368794326241 loss: 0.00023491722822654992\n",
      "141: accuracy:0.6028368794326241 loss: 0.0002507959143258631\n",
      "142: accuracy:0.6028368794326241 loss: 0.0002563544549047947\n",
      "143: accuracy:0.6028368794326241 loss: 0.00019389561202842742\n",
      "144: accuracy:0.6028368794326241 loss: 0.00021977424330543727\n",
      "145: accuracy:0.6028368794326241 loss: 0.0001906463148770854\n",
      "146: accuracy:0.6028368794326241 loss: 0.00022589138825424016\n",
      "147: accuracy:0.6028368794326241 loss: 0.00021901812579017133\n",
      "148: accuracy:0.6099290780141844 loss: 0.00020920889801345766\n",
      "149: accuracy:0.6099290780141844 loss: 0.00021446772734634578\n",
      "150: accuracy:0.6099290780141844 loss: 0.00021012169599998742\n",
      "151: accuracy:0.6099290780141844 loss: 0.00019586426788009703\n",
      "152: accuracy:0.6099290780141844 loss: 0.00016546249389648438\n",
      "153: accuracy:0.6099290780141844 loss: 0.00018738338258117437\n",
      "154: accuracy:0.6099290780141844 loss: 0.00016751971270423383\n",
      "155: accuracy:0.6099290780141844 loss: 0.00018254689348395914\n",
      "156: accuracy:0.6099290780141844 loss: 0.00020792143186554313\n",
      "157: accuracy:0.6099290780141844 loss: 0.0001812594273360446\n",
      "158: accuracy:0.6099290780141844 loss: 0.0001621246337890625\n",
      "159: accuracy:0.6028368794326241 loss: 0.00016890253755263984\n",
      "160: accuracy:0.6028368794326241 loss: 0.00018338476365897804\n",
      "161: accuracy:0.6028368794326241 loss: 0.00020689282973762602\n",
      "162: accuracy:0.6028368794326241 loss: 0.00016223362763412297\n",
      "163: accuracy:0.6028368794326241 loss: 0.00017504692368675023\n",
      "164: accuracy:0.6028368794326241 loss: 0.00019791466183960438\n",
      "165: accuracy:0.6028368794326241 loss: 0.00015816006634850055\n",
      "166: accuracy:0.6028368794326241 loss: 0.00016147068527061492\n",
      "167: accuracy:0.6099290780141844 loss: 0.00019665445142891258\n",
      "168: accuracy:0.6099290780141844 loss: 0.00018064635514747351\n",
      "169: accuracy:0.6099290780141844 loss: 0.00019623892148956656\n",
      "170: accuracy:0.6099290780141844 loss: 0.00015948840882629156\n",
      "171: accuracy:0.6099290780141844 loss: 0.0001698834530543536\n",
      "172: accuracy:0.6099290780141844 loss: 0.0001549788867123425\n",
      "173: accuracy:0.6099290780141844 loss: 0.00016694750229362398\n",
      "174: accuracy:0.6099290780141844 loss: 0.00016350064834114164\n",
      "175: accuracy:0.6099290780141844 loss: 0.00016113689343910664\n",
      "176: accuracy:0.6099290780141844 loss: 0.00013608932204078883\n",
      "177: accuracy:0.6099290780141844 loss: 0.00015224047820083797\n",
      "178: accuracy:0.6099290780141844 loss: 0.00015104157500900328\n",
      "179: accuracy:0.6099290780141844 loss: 0.00017320088227279484\n",
      "180: accuracy:0.6099290780141844 loss: 0.00016048294492065907\n",
      "181: accuracy:0.6170212765957447 loss: 0.00013217244122643024\n",
      "182: accuracy:0.6170212765957447 loss: 0.00013360977754928172\n",
      "183: accuracy:0.6170212765957447 loss: 0.0001372677943436429\n",
      "184: accuracy:0.6170212765957447 loss: 0.00014581679715774953\n",
      "185: accuracy:0.6170212765957447 loss: 0.00017360960191581398\n",
      "186: accuracy:0.6170212765957447 loss: 0.00012380736006889492\n",
      "187: accuracy:0.6170212765957447 loss: 0.0001388413511449471\n",
      "188: accuracy:0.6170212765957447 loss: 0.00016517638869117945\n",
      "189: accuracy:0.6170212765957447 loss: 0.00015132086991798133\n",
      "190: accuracy:0.6170212765957447 loss: 0.0001224926527356729\n",
      "191: accuracy:0.6170212765957447 loss: 0.00012763567792717367\n",
      "192: accuracy:0.6170212765957447 loss: 0.00013770375517196953\n",
      "193: accuracy:0.6170212765957447 loss: 0.00011689322127494961\n",
      "194: accuracy:0.6170212765957447 loss: 0.00014467920118477196\n",
      "195: accuracy:0.6170212765957447 loss: 0.00012718609650619328\n",
      "196: accuracy:0.6170212765957447 loss: 0.00015584401262458414\n",
      "197: accuracy:0.6170212765957447 loss: 9.752681944519281e-05\n",
      "198: accuracy:0.6170212765957447 loss: 0.00014287403610069305\n",
      "199: accuracy:0.6170212765957447 loss: 0.000123950419947505\n",
      "200: accuracy:0.624113475177305 loss: 0.00014002663374412805\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    ")\n",
    "net = RNN_max()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "for epoch in range(1,201): \n",
    "    for step, (batch_x, sen, batch_y) in enumerate(loader):\n",
    "        pred = net(sen, batch_x)\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 2: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_s, t_e)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glove_bi-rnn_max: 0.7021276595744681 11轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class bi_RNN_max(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size = 200,       # rnn input\n",
    "            hidden_size = 64,     # rnn hidden unit\n",
    "            num_layers = 1,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=128,out_features=7)\n",
    "        self.maxpool = torch.nn.MaxPool2d((53,1))\n",
    "\n",
    "    def forward(self,sen,x):\n",
    "        output,(h_n,c_n)=self.rnn(x.float())\n",
    "        output_in_last_timestep = self.maxpool(output)\n",
    "        output_in_last_timestep = output_in_last_timestep.reshape((output_in_last_timestep.shape[0],-1))\n",
    "        x=self.out(output_in_last_timestep)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi_RNN_max(\n",
      "  (rnn): LSTM(200, 64, batch_first=True, bidirectional=True)\n",
      "  (out): Linear(in_features=128, out_features=7, bias=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=(53, 1), stride=(53, 1), padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "1: accuracy:0.23404255319148937 loss: 1.870179295539856\n",
      "2: accuracy:0.5390070921985816 loss: 1.6827722787857056\n",
      "3: accuracy:0.574468085106383 loss: 1.3002525568008423\n",
      "4: accuracy:0.5460992907801419 loss: 0.9892982244491577\n",
      "5: accuracy:0.624113475177305 loss: 0.7408919334411621\n",
      "6: accuracy:0.5957446808510638 loss: 0.47758299112319946\n",
      "7: accuracy:0.6382978723404256 loss: 0.3864317238330841\n",
      "8: accuracy:0.6524822695035462 loss: 0.2614588737487793\n",
      "9: accuracy:0.6879432624113475 loss: 0.1430911272764206\n",
      "10: accuracy:0.6595744680851063 loss: 0.11513009667396545\n",
      "11: accuracy:0.7021276595744681 loss: 0.06006189063191414\n",
      "12: accuracy:0.7021276595744681 loss: 0.027230773121118546\n",
      "13: accuracy:0.6808510638297872 loss: 0.019386181607842445\n",
      "14: accuracy:0.6666666666666666 loss: 0.010906879790127277\n",
      "15: accuracy:0.6666666666666666 loss: 0.006045021116733551\n",
      "16: accuracy:0.6595744680851063 loss: 0.006292731501162052\n",
      "17: accuracy:0.6524822695035462 loss: 0.004500985145568848\n",
      "18: accuracy:0.6524822695035462 loss: 0.003589568892493844\n",
      "19: accuracy:0.6595744680851063 loss: 0.003046975703909993\n",
      "20: accuracy:0.6595744680851063 loss: 0.0022687299642711878\n",
      "21: accuracy:0.6666666666666666 loss: 0.0019163608085364103\n",
      "22: accuracy:0.6595744680851063 loss: 0.001585081685334444\n",
      "23: accuracy:0.6666666666666666 loss: 0.0014048576122149825\n",
      "24: accuracy:0.6666666666666666 loss: 0.0011480468092486262\n",
      "25: accuracy:0.6666666666666666 loss: 0.0013492993311956525\n",
      "26: accuracy:0.6666666666666666 loss: 0.0011247294023633003\n",
      "27: accuracy:0.6595744680851063 loss: 0.0011131218634545803\n",
      "28: accuracy:0.6666666666666666 loss: 0.001049150712788105\n",
      "29: accuracy:0.6666666666666666 loss: 0.0009446961339563131\n",
      "30: accuracy:0.6666666666666666 loss: 0.0008761133649386466\n",
      "31: accuracy:0.6524822695035462 loss: 0.0008138179546222091\n",
      "32: accuracy:0.6524822695035462 loss: 0.0008101940038613975\n",
      "33: accuracy:0.6524822695035462 loss: 0.0009200777276419103\n",
      "34: accuracy:0.6524822695035462 loss: 0.0006344182183966041\n",
      "35: accuracy:0.6524822695035462 loss: 0.0006614548619836569\n",
      "36: accuracy:0.6524822695035462 loss: 0.0005976813263259828\n",
      "37: accuracy:0.6524822695035462 loss: 0.0007361548487097025\n",
      "38: accuracy:0.6524822695035462 loss: 0.0006072929827496409\n",
      "39: accuracy:0.6524822695035462 loss: 0.0005370753351598978\n",
      "40: accuracy:0.6524822695035462 loss: 0.0005218642181716859\n",
      "41: accuracy:0.6524822695035462 loss: 0.0005610329681076109\n",
      "42: accuracy:0.6524822695035462 loss: 0.00051697320304811\n",
      "43: accuracy:0.6524822695035462 loss: 0.00043664660188369453\n",
      "44: accuracy:0.6524822695035462 loss: 0.0005039011011831462\n",
      "45: accuracy:0.6524822695035462 loss: 0.00040240288944914937\n",
      "46: accuracy:0.6524822695035462 loss: 0.0004880836931988597\n",
      "47: accuracy:0.6524822695035462 loss: 0.0004853725549764931\n",
      "48: accuracy:0.6453900709219859 loss: 0.00040582928340882063\n",
      "49: accuracy:0.6453900709219859 loss: 0.0004225594748277217\n",
      "50: accuracy:0.6453900709219859 loss: 0.00040725979488343\n",
      "51: accuracy:0.6453900709219859 loss: 0.00038700102595612407\n",
      "52: accuracy:0.6453900709219859 loss: 0.0004101140075363219\n",
      "53: accuracy:0.6382978723404256 loss: 0.0003194740856997669\n",
      "54: accuracy:0.6382978723404256 loss: 0.0003838539123535156\n",
      "55: accuracy:0.6382978723404256 loss: 0.0003742217959370464\n",
      "56: accuracy:0.6382978723404256 loss: 0.0003577913448680192\n",
      "57: accuracy:0.6453900709219859 loss: 0.00035386765375733376\n",
      "58: accuracy:0.6453900709219859 loss: 0.00030379975214600563\n",
      "59: accuracy:0.6453900709219859 loss: 0.0003067493380513042\n",
      "60: accuracy:0.6453900709219859 loss: 0.0003020082076545805\n",
      "61: accuracy:0.6453900709219859 loss: 0.00036099296994507313\n",
      "62: accuracy:0.6524822695035462 loss: 0.0003444535250309855\n",
      "63: accuracy:0.6524822695035462 loss: 0.00033906527096405625\n",
      "64: accuracy:0.6524822695035462 loss: 0.0002875191858038306\n",
      "65: accuracy:0.6524822695035462 loss: 0.000298322964226827\n",
      "66: accuracy:0.6524822695035462 loss: 0.0002716609451454133\n",
      "67: accuracy:0.6524822695035462 loss: 0.0002620152081362903\n",
      "68: accuracy:0.6524822695035462 loss: 0.0002556528488639742\n",
      "69: accuracy:0.6524822695035462 loss: 0.00022947447723709047\n",
      "70: accuracy:0.6524822695035462 loss: 0.00024830273468978703\n",
      "71: accuracy:0.6524822695035462 loss: 0.0002237047447124496\n",
      "72: accuracy:0.6524822695035462 loss: 0.0002811704471241683\n",
      "73: accuracy:0.6524822695035462 loss: 0.0002535547537263483\n",
      "74: accuracy:0.6524822695035462 loss: 0.00022686549345962703\n",
      "75: accuracy:0.6524822695035462 loss: 0.0002113614755216986\n",
      "76: accuracy:0.6453900709219859 loss: 0.0002599171129986644\n",
      "77: accuracy:0.6453900709219859 loss: 0.00023006711853668094\n",
      "78: accuracy:0.6382978723404256 loss: 0.00022210393217392266\n",
      "79: accuracy:0.6382978723404256 loss: 0.0002247129159513861\n",
      "80: accuracy:0.6382978723404256 loss: 0.00021069390641059726\n",
      "81: accuracy:0.6382978723404256 loss: 0.0001831940171541646\n",
      "82: accuracy:0.6382978723404256 loss: 0.0001858234463725239\n",
      "83: accuracy:0.6382978723404256 loss: 0.0002184391050832346\n",
      "84: accuracy:0.6382978723404256 loss: 0.00016839844465721399\n",
      "85: accuracy:0.6382978723404256 loss: 0.000188371108379215\n",
      "86: accuracy:0.6382978723404256 loss: 0.00020267622312530875\n",
      "87: accuracy:0.6382978723404256 loss: 0.00018237659242004156\n",
      "88: accuracy:0.6382978723404256 loss: 0.00020155224774498492\n",
      "89: accuracy:0.6382978723404256 loss: 0.00017672947433311492\n",
      "90: accuracy:0.6382978723404256 loss: 0.00022335733228828758\n",
      "91: accuracy:0.6382978723404256 loss: 0.0001593453489476815\n",
      "92: accuracy:0.6382978723404256 loss: 0.0001703398593235761\n",
      "93: accuracy:0.6382978723404256 loss: 0.0001755373814376071\n",
      "94: accuracy:0.6453900709219859 loss: 0.00015063285536598414\n",
      "95: accuracy:0.6453900709219859 loss: 0.00017286709044128656\n",
      "96: accuracy:0.6453900709219859 loss: 0.0001653262588661164\n",
      "97: accuracy:0.6453900709219859 loss: 0.00016627993318252265\n",
      "98: accuracy:0.6453900709219859 loss: 0.00014410019502975047\n",
      "99: accuracy:0.6453900709219859 loss: 0.00014158656995277852\n",
      "100: accuracy:0.6382978723404256 loss: 0.0001442500506527722\n",
      "101: accuracy:0.6382978723404256 loss: 0.00017300334002356976\n",
      "102: accuracy:0.6382978723404256 loss: 0.0001536164927529171\n",
      "103: accuracy:0.6382978723404256 loss: 0.00014081683184485883\n",
      "104: accuracy:0.6382978723404256 loss: 0.00013136863708496094\n",
      "105: accuracy:0.6382978723404256 loss: 0.00016036033048294485\n",
      "106: accuracy:0.6382978723404256 loss: 0.00015551703108940274\n",
      "107: accuracy:0.6382978723404256 loss: 0.00015442712174262851\n",
      "108: accuracy:0.6382978723404256 loss: 0.00013476099411491305\n",
      "109: accuracy:0.6312056737588653 loss: 0.00014698164886794984\n",
      "110: accuracy:0.6312056737588653 loss: 0.00011475427163532004\n",
      "111: accuracy:0.6312056737588653 loss: 0.0001255920942639932\n",
      "112: accuracy:0.6312056737588653 loss: 0.00012457711272872984\n",
      "113: accuracy:0.6312056737588653 loss: 0.0001343182084383443\n",
      "114: accuracy:0.6312056737588653 loss: 0.00013078961637802422\n",
      "115: accuracy:0.6312056737588653 loss: 0.000124917714856565\n",
      "116: accuracy:0.6312056737588653 loss: 0.000118739262688905\n",
      "117: accuracy:0.6312056737588653 loss: 0.00012190682900836691\n",
      "118: accuracy:0.6312056737588653 loss: 0.0001167024893220514\n",
      "119: accuracy:0.6312056737588653 loss: 0.00011215890845051035\n",
      "120: accuracy:0.6312056737588653 loss: 0.00012111663818359375\n",
      "121: accuracy:0.6312056737588653 loss: 0.00011755398008972406\n",
      "122: accuracy:0.6312056737588653 loss: 0.00012884140596725047\n",
      "123: accuracy:0.6312056737588653 loss: 0.00012621197674889117\n",
      "124: accuracy:0.6312056737588653 loss: 0.00011728150275303051\n",
      "125: accuracy:0.6312056737588653 loss: 9.819439583225176e-05\n",
      "126: accuracy:0.6312056737588653 loss: 0.00010455676238052547\n",
      "127: accuracy:0.6312056737588653 loss: 0.00011128016922157258\n",
      "128: accuracy:0.6312056737588653 loss: 9.412084182258695e-05\n",
      "129: accuracy:0.6312056737588653 loss: 0.00010873931023525074\n",
      "130: accuracy:0.6312056737588653 loss: 0.00011099406401626766\n",
      "131: accuracy:0.6312056737588653 loss: 9.681156370788813e-05\n",
      "132: accuracy:0.6312056737588653 loss: 8.406639244640246e-05\n",
      "133: accuracy:0.6312056737588653 loss: 9.324210259364918e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134: accuracy:0.6312056737588653 loss: 9.211131691699848e-05\n",
      "135: accuracy:0.6312056737588653 loss: 0.00010104179091285914\n",
      "136: accuracy:0.6312056737588653 loss: 9.264946129405871e-05\n",
      "137: accuracy:0.6312056737588653 loss: 8.654594421386719e-05\n",
      "138: accuracy:0.624113475177305 loss: 8.63824607222341e-05\n",
      "139: accuracy:0.624113475177305 loss: 0.00010326249321224168\n",
      "140: accuracy:0.624113475177305 loss: 9.602138015907258e-05\n",
      "141: accuracy:0.624113475177305 loss: 9.869167115539312e-05\n",
      "142: accuracy:0.624113475177305 loss: 9.86303566605784e-05\n",
      "143: accuracy:0.624113475177305 loss: 7.38348244340159e-05\n",
      "144: accuracy:0.624113475177305 loss: 8.80922598298639e-05\n",
      "145: accuracy:0.624113475177305 loss: 8.296285523101687e-05\n",
      "146: accuracy:0.624113475177305 loss: 0.00010580335219856352\n",
      "147: accuracy:0.624113475177305 loss: 7.696151442360133e-05\n",
      "148: accuracy:0.624113475177305 loss: 7.896423630882055e-05\n",
      "149: accuracy:0.624113475177305 loss: 6.988389213802293e-05\n",
      "150: accuracy:0.624113475177305 loss: 8.35691171232611e-05\n",
      "151: accuracy:0.6170212765957447 loss: 8.69001669343561e-05\n",
      "152: accuracy:0.6170212765957447 loss: 8.905955473892391e-05\n",
      "153: accuracy:0.6170212765957447 loss: 7.877349707996473e-05\n",
      "154: accuracy:0.6170212765957447 loss: 6.758145173080266e-05\n",
      "155: accuracy:0.6170212765957447 loss: 6.292888429015875e-05\n",
      "156: accuracy:0.6170212765957447 loss: 9.025846520671621e-05\n",
      "157: accuracy:0.6170212765957447 loss: 8.579662971897051e-05\n",
      "158: accuracy:0.6170212765957447 loss: 7.453645957866684e-05\n",
      "159: accuracy:0.6170212765957447 loss: 7.893698784755543e-05\n",
      "160: accuracy:0.6170212765957447 loss: 6.608281546505168e-05\n",
      "161: accuracy:0.6170212765957447 loss: 8.234977576648816e-05\n",
      "162: accuracy:0.6170212765957447 loss: 6.423677405109629e-05\n",
      "163: accuracy:0.6170212765957447 loss: 7.305145118152723e-05\n",
      "164: accuracy:0.6170212765957447 loss: 6.237711204448715e-05\n",
      "165: accuracy:0.6170212765957447 loss: 6.816046516178176e-05\n",
      "166: accuracy:0.6170212765957447 loss: 6.80855373502709e-05\n",
      "167: accuracy:0.6170212765957447 loss: 6.769725587219e-05\n",
      "168: accuracy:0.6170212765957447 loss: 7.861682388465852e-05\n",
      "169: accuracy:0.6170212765957447 loss: 6.949561065994203e-05\n",
      "170: accuracy:0.6170212765957447 loss: 5.799021164420992e-05\n",
      "171: accuracy:0.6170212765957447 loss: 6.653922173427418e-05\n",
      "172: accuracy:0.6170212765957447 loss: 5.5980683100642636e-05\n",
      "173: accuracy:0.6099290780141844 loss: 6.538799789268523e-05\n",
      "174: accuracy:0.6099290780141844 loss: 6.757463415851817e-05\n",
      "175: accuracy:0.6099290780141844 loss: 6.383487198036164e-05\n",
      "176: accuracy:0.6099290780141844 loss: 7.654598448425531e-05\n",
      "177: accuracy:0.6170212765957447 loss: 5.7526998716639355e-05\n",
      "178: accuracy:0.6170212765957447 loss: 5.384854011936113e-05\n",
      "179: accuracy:0.6170212765957447 loss: 4.5755932660540566e-05\n",
      "180: accuracy:0.6170212765957447 loss: 6.310598837444559e-05\n",
      "181: accuracy:0.6170212765957447 loss: 6.008148193359375e-05\n",
      "182: accuracy:0.624113475177305 loss: 6.040845619281754e-05\n",
      "183: accuracy:0.624113475177305 loss: 6.597382889594883e-05\n",
      "184: accuracy:0.624113475177305 loss: 6.288800796028227e-05\n",
      "185: accuracy:0.624113475177305 loss: 5.3146908612689e-05\n",
      "186: accuracy:0.624113475177305 loss: 6.984302308410406e-05\n",
      "187: accuracy:0.624113475177305 loss: 5.8903013268718496e-05\n",
      "188: accuracy:0.624113475177305 loss: 5.704334762413055e-05\n",
      "189: accuracy:0.624113475177305 loss: 4.740442454931326e-05\n",
      "190: accuracy:0.624113475177305 loss: 6.316730286926031e-05\n",
      "191: accuracy:0.624113475177305 loss: 6.44956307951361e-05\n",
      "192: accuracy:0.624113475177305 loss: 5.3991589084034786e-05\n",
      "193: accuracy:0.624113475177305 loss: 6.239755020942539e-05\n",
      "194: accuracy:0.624113475177305 loss: 5.9917994803981856e-05\n",
      "195: accuracy:0.6170212765957447 loss: 5.0864899094449356e-05\n",
      "196: accuracy:0.624113475177305 loss: 4.813330451725051e-05\n",
      "197: accuracy:0.624113475177305 loss: 4.560606976156123e-05\n",
      "198: accuracy:0.624113475177305 loss: 5.258832607069053e-05\n",
      "199: accuracy:0.624113475177305 loss: 5.2227293053874746e-05\n",
      "200: accuracy:0.624113475177305 loss: 5.036081711295992e-05\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    ")\n",
    "net = bi_RNN_max()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "for epoch in range(1,201): \n",
    "    for step, (batch_x, sen, batch_y) in enumerate(loader):\n",
    "        pred = net(sen, batch_x)\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 2: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_s, t_e)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glove:0.44680851063829785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN_old(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size = 200,       # rnn input\n",
    "            hidden_size = 64,     # rnn hidden unit\n",
    "            num_layers = 1,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=64,out_features=7)\n",
    "\n",
    "    def forward(self,sen,x):\n",
    "        output,(h_n,c_n)=self.rnn(x.float())\n",
    "        output_in_last_timestep=h_n[-1,:,:]\n",
    "        x=self.out(output_in_last_timestep)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_old(\n",
      "  (rnn): LSTM(200, 64, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=7, bias=True)\n",
      ")\n",
      "1: accuracy:0.19148936170212766 loss: 1.9497759342193604\n",
      "2: accuracy:0.19148936170212766 loss: 1.9573512077331543\n",
      "3: accuracy:0.10638297872340426 loss: 1.9416414499282837\n",
      "4: accuracy:0.10638297872340426 loss: 1.9456526041030884\n",
      "5: accuracy:0.10638297872340426 loss: 1.9020291566848755\n",
      "6: accuracy:0.10638297872340426 loss: 1.95263671875\n",
      "7: accuracy:0.19148936170212766 loss: 1.9122650623321533\n",
      "8: accuracy:0.19148936170212766 loss: 1.942103385925293\n",
      "9: accuracy:0.1773049645390071 loss: 1.905679702758789\n",
      "10: accuracy:0.19148936170212766 loss: 1.9215768575668335\n",
      "11: accuracy:0.19148936170212766 loss: 1.9475725889205933\n",
      "12: accuracy:0.19148936170212766 loss: 1.92251455783844\n",
      "13: accuracy:0.2198581560283688 loss: 2.0140671730041504\n",
      "14: accuracy:0.19148936170212766 loss: 1.923090934753418\n",
      "15: accuracy:0.19148936170212766 loss: 1.9338258504867554\n",
      "16: accuracy:0.2127659574468085 loss: 1.9195982217788696\n",
      "17: accuracy:0.18439716312056736 loss: 1.9715944528579712\n",
      "18: accuracy:0.19148936170212766 loss: 1.9200189113616943\n",
      "19: accuracy:0.19148936170212766 loss: 1.9543890953063965\n",
      "20: accuracy:0.19148936170212766 loss: 1.8772979974746704\n",
      "21: accuracy:0.19148936170212766 loss: 1.8782485723495483\n",
      "22: accuracy:0.2127659574468085 loss: 1.9051316976547241\n",
      "23: accuracy:0.2127659574468085 loss: 1.8643295764923096\n",
      "24: accuracy:0.23404255319148937 loss: 1.8030341863632202\n",
      "25: accuracy:0.2127659574468085 loss: 1.7911995649337769\n",
      "26: accuracy:0.19148936170212766 loss: 1.9101523160934448\n",
      "27: accuracy:0.19858156028368795 loss: 1.6489224433898926\n",
      "28: accuracy:0.24822695035460993 loss: 1.7067375183105469\n",
      "29: accuracy:0.2198581560283688 loss: 1.7473819255828857\n",
      "30: accuracy:0.24113475177304963 loss: 1.742708444595337\n",
      "31: accuracy:0.24113475177304963 loss: 1.661673903465271\n",
      "32: accuracy:0.2624113475177305 loss: 1.5366086959838867\n",
      "33: accuracy:0.1773049645390071 loss: 1.6240702867507935\n",
      "34: accuracy:0.22695035460992907 loss: 1.407139539718628\n",
      "35: accuracy:0.2127659574468085 loss: 1.6438850164413452\n",
      "36: accuracy:0.2553191489361702 loss: 1.6757781505584717\n",
      "37: accuracy:0.24113475177304963 loss: 1.459233045578003\n",
      "38: accuracy:0.24822695035460993 loss: 1.5425993204116821\n",
      "39: accuracy:0.2765957446808511 loss: 1.46796715259552\n",
      "40: accuracy:0.2765957446808511 loss: 1.5110487937927246\n",
      "41: accuracy:0.2127659574468085 loss: 1.4548957347869873\n",
      "42: accuracy:0.2198581560283688 loss: 1.5312119722366333\n",
      "43: accuracy:0.22695035460992907 loss: 1.5150175094604492\n",
      "44: accuracy:0.2127659574468085 loss: 1.5474488735198975\n",
      "45: accuracy:0.2127659574468085 loss: 1.4588773250579834\n",
      "46: accuracy:0.18439716312056736 loss: 1.414627194404602\n",
      "47: accuracy:0.19858156028368795 loss: 1.4207357168197632\n",
      "48: accuracy:0.24822695035460993 loss: 1.3153647184371948\n",
      "49: accuracy:0.18439716312056736 loss: 1.4317306280136108\n",
      "50: accuracy:0.16312056737588654 loss: 1.477914810180664\n",
      "51: accuracy:0.2127659574468085 loss: 1.3091962337493896\n",
      "52: accuracy:0.19858156028368795 loss: 1.528412938117981\n",
      "53: accuracy:0.2198581560283688 loss: 1.1788536310195923\n",
      "54: accuracy:0.2198581560283688 loss: 1.4023222923278809\n",
      "55: accuracy:0.20567375886524822 loss: 1.35259211063385\n",
      "56: accuracy:0.20567375886524822 loss: 1.13905930519104\n",
      "57: accuracy:0.23404255319148937 loss: 1.3800044059753418\n",
      "58: accuracy:0.2198581560283688 loss: 1.39790940284729\n",
      "59: accuracy:0.2765957446808511 loss: 1.2326538562774658\n",
      "60: accuracy:0.2624113475177305 loss: 1.1941686868667603\n",
      "61: accuracy:0.24822695035460993 loss: 1.1503939628601074\n",
      "62: accuracy:0.2695035460992908 loss: 1.0221067667007446\n",
      "63: accuracy:0.3049645390070922 loss: 1.1935139894485474\n",
      "64: accuracy:0.28368794326241137 loss: 1.0903059244155884\n",
      "65: accuracy:0.2907801418439716 loss: 0.8135754466056824\n",
      "66: accuracy:0.2624113475177305 loss: 1.0263948440551758\n",
      "67: accuracy:0.2765957446808511 loss: 0.9186239242553711\n",
      "68: accuracy:0.28368794326241137 loss: 1.0359857082366943\n",
      "69: accuracy:0.2907801418439716 loss: 0.7658241391181946\n",
      "70: accuracy:0.3049645390070922 loss: 1.079100489616394\n",
      "71: accuracy:0.3191489361702128 loss: 0.8278374075889587\n",
      "72: accuracy:0.3191489361702128 loss: 0.8478338122367859\n",
      "73: accuracy:0.3262411347517731 loss: 0.599803626537323\n",
      "74: accuracy:0.3191489361702128 loss: 0.6611388325691223\n",
      "75: accuracy:0.3191489361702128 loss: 0.7033557891845703\n",
      "76: accuracy:0.3120567375886525 loss: 0.8158667087554932\n",
      "77: accuracy:0.3191489361702128 loss: 0.3945814073085785\n",
      "78: accuracy:0.3191489361702128 loss: 0.5283842086791992\n",
      "79: accuracy:0.3191489361702128 loss: 0.4770180881023407\n",
      "80: accuracy:0.3404255319148936 loss: 0.55471271276474\n",
      "81: accuracy:0.3617021276595745 loss: 0.40822020173072815\n",
      "82: accuracy:0.2907801418439716 loss: 0.4393712282180786\n",
      "83: accuracy:0.3475177304964539 loss: 0.47763001918792725\n",
      "84: accuracy:0.3546099290780142 loss: 0.4135240614414215\n",
      "85: accuracy:0.3262411347517731 loss: 0.41692930459976196\n",
      "86: accuracy:0.3404255319148936 loss: 0.30732131004333496\n",
      "87: accuracy:0.3546099290780142 loss: 0.2521333694458008\n",
      "88: accuracy:0.3617021276595745 loss: 0.4467814266681671\n",
      "89: accuracy:0.3404255319148936 loss: 0.37859442830085754\n",
      "90: accuracy:0.3475177304964539 loss: 0.5307285189628601\n",
      "91: accuracy:0.3404255319148936 loss: 0.5483061671257019\n",
      "92: accuracy:0.3120567375886525 loss: 0.5008566975593567\n",
      "93: accuracy:0.3546099290780142 loss: 0.49066609144210815\n",
      "94: accuracy:0.2978723404255319 loss: 0.287882924079895\n",
      "95: accuracy:0.2978723404255319 loss: 0.4176187217235565\n",
      "96: accuracy:0.3262411347517731 loss: 0.4000369608402252\n",
      "97: accuracy:0.3333333333333333 loss: 0.35614141821861267\n",
      "98: accuracy:0.3262411347517731 loss: 0.4450908899307251\n",
      "99: accuracy:0.2978723404255319 loss: 0.36900627613067627\n",
      "100: accuracy:0.3404255319148936 loss: 0.3527863621711731\n",
      "101: accuracy:0.3475177304964539 loss: 0.3618052899837494\n",
      "102: accuracy:0.3333333333333333 loss: 0.42128682136535645\n",
      "103: accuracy:0.3404255319148936 loss: 0.34733104705810547\n",
      "104: accuracy:0.3404255319148936 loss: 0.37658777832984924\n",
      "105: accuracy:0.3546099290780142 loss: 0.27397435903549194\n",
      "106: accuracy:0.3475177304964539 loss: 0.6282720565795898\n",
      "107: accuracy:0.3191489361702128 loss: 0.3632335066795349\n",
      "108: accuracy:0.3049645390070922 loss: 0.4290418326854706\n",
      "109: accuracy:0.3191489361702128 loss: 0.4057824909687042\n",
      "110: accuracy:0.3475177304964539 loss: 0.2949407398700714\n",
      "111: accuracy:0.2978723404255319 loss: 0.3303987681865692\n",
      "112: accuracy:0.3191489361702128 loss: 0.4533204436302185\n",
      "113: accuracy:0.3049645390070922 loss: 0.3603441119194031\n",
      "114: accuracy:0.2907801418439716 loss: 0.31654125452041626\n",
      "115: accuracy:0.3120567375886525 loss: 0.31769561767578125\n",
      "116: accuracy:0.3333333333333333 loss: 0.3753281235694885\n",
      "117: accuracy:0.3333333333333333 loss: 0.3327934741973877\n",
      "118: accuracy:0.3262411347517731 loss: 0.2581031322479248\n",
      "119: accuracy:0.3191489361702128 loss: 0.3890663683414459\n",
      "120: accuracy:0.3120567375886525 loss: 0.25376588106155396\n",
      "121: accuracy:0.3262411347517731 loss: 0.3987680971622467\n",
      "122: accuracy:0.3049645390070922 loss: 0.47042301297187805\n",
      "123: accuracy:0.3475177304964539 loss: 0.2960227131843567\n",
      "124: accuracy:0.3049645390070922 loss: 0.3538949489593506\n",
      "125: accuracy:0.2907801418439716 loss: 0.3178577721118927\n",
      "126: accuracy:0.3120567375886525 loss: 0.5643218159675598\n",
      "127: accuracy:0.3617021276595745 loss: 0.7350677847862244\n",
      "128: accuracy:0.2907801418439716 loss: 0.19598288834095\n",
      "129: accuracy:0.2624113475177305 loss: 0.599024772644043\n",
      "130: accuracy:0.24822695035460993 loss: 0.3677677512168884\n",
      "131: accuracy:0.3049645390070922 loss: 0.46156370639801025\n",
      "132: accuracy:0.2978723404255319 loss: 0.3305310904979706\n",
      "133: accuracy:0.3546099290780142 loss: 0.38898998498916626\n",
      "134: accuracy:0.36879432624113473 loss: 0.37188971042633057\n",
      "135: accuracy:0.36879432624113473 loss: 0.29195210337638855\n",
      "136: accuracy:0.3617021276595745 loss: 0.3508046269416809\n",
      "137: accuracy:0.3475177304964539 loss: 0.34620001912117004\n",
      "138: accuracy:0.3262411347517731 loss: 0.23789270222187042\n",
      "139: accuracy:0.3191489361702128 loss: 0.18963445723056793\n",
      "140: accuracy:0.3191489361702128 loss: 0.3626370131969452\n",
      "141: accuracy:0.3262411347517731 loss: 0.24909567832946777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142: accuracy:0.3333333333333333 loss: 0.29946187138557434\n",
      "143: accuracy:0.3120567375886525 loss: 0.08196274936199188\n",
      "144: accuracy:0.3049645390070922 loss: 0.3095340132713318\n",
      "145: accuracy:0.3404255319148936 loss: 0.32889673113822937\n",
      "146: accuracy:0.3262411347517731 loss: 0.1622338891029358\n",
      "147: accuracy:0.3404255319148936 loss: 0.18762880563735962\n",
      "148: accuracy:0.3546099290780142 loss: 0.1792835146188736\n",
      "149: accuracy:0.3404255319148936 loss: 0.3286314904689789\n",
      "150: accuracy:0.3475177304964539 loss: 0.10441148281097412\n",
      "151: accuracy:0.36879432624113473 loss: 0.19653062522411346\n",
      "152: accuracy:0.3617021276595745 loss: 0.149704247713089\n",
      "153: accuracy:0.3617021276595745 loss: 0.11561275273561478\n",
      "154: accuracy:0.3546099290780142 loss: 0.18378540873527527\n",
      "155: accuracy:0.3475177304964539 loss: 0.16312937438488007\n",
      "156: accuracy:0.3475177304964539 loss: 0.2698037922382355\n",
      "157: accuracy:0.3404255319148936 loss: 0.16241727769374847\n",
      "158: accuracy:0.3333333333333333 loss: 0.2321598380804062\n",
      "159: accuracy:0.3475177304964539 loss: 0.14723631739616394\n",
      "160: accuracy:0.3262411347517731 loss: 0.07712789624929428\n",
      "161: accuracy:0.3333333333333333 loss: 0.2199249416589737\n",
      "162: accuracy:0.3404255319148936 loss: 0.05849512666463852\n",
      "163: accuracy:0.3404255319148936 loss: 0.058884281665086746\n",
      "164: accuracy:0.3617021276595745 loss: 0.12949737906455994\n",
      "165: accuracy:0.3900709219858156 loss: 0.12208139151334763\n",
      "166: accuracy:0.3971631205673759 loss: 0.07555787265300751\n",
      "167: accuracy:0.3829787234042553 loss: 0.14007222652435303\n",
      "168: accuracy:0.3617021276595745 loss: 0.16502881050109863\n",
      "169: accuracy:0.3617021276595745 loss: 0.0786537304520607\n",
      "170: accuracy:0.3475177304964539 loss: 0.11425916105508804\n",
      "171: accuracy:0.3404255319148936 loss: 0.10176148265600204\n",
      "172: accuracy:0.3404255319148936 loss: 0.19072489440441132\n",
      "173: accuracy:0.41843971631205673 loss: 0.24632620811462402\n",
      "174: accuracy:0.3829787234042553 loss: 0.1783534735441208\n",
      "175: accuracy:0.3475177304964539 loss: 0.16825389862060547\n",
      "176: accuracy:0.3049645390070922 loss: 0.016184717416763306\n",
      "177: accuracy:0.2907801418439716 loss: 0.06708876043558121\n",
      "178: accuracy:0.3191489361702128 loss: 0.08535372465848923\n",
      "179: accuracy:0.3191489361702128 loss: 0.12364129722118378\n",
      "180: accuracy:0.3617021276595745 loss: 0.07256387174129486\n",
      "181: accuracy:0.3829787234042553 loss: 0.1121954545378685\n",
      "182: accuracy:0.3971631205673759 loss: 0.10795331001281738\n",
      "183: accuracy:0.40425531914893614 loss: 0.08110344409942627\n",
      "184: accuracy:0.3900709219858156 loss: 0.11300697177648544\n",
      "185: accuracy:0.40425531914893614 loss: 0.056459277868270874\n",
      "186: accuracy:0.3829787234042553 loss: 0.07940534502267838\n",
      "187: accuracy:0.3546099290780142 loss: 0.16022199392318726\n",
      "188: accuracy:0.3617021276595745 loss: 0.06147104129195213\n",
      "189: accuracy:0.3617021276595745 loss: 0.053849801421165466\n",
      "190: accuracy:0.3404255319148936 loss: 0.07520565390586853\n",
      "191: accuracy:0.3120567375886525 loss: 0.15922559797763824\n",
      "192: accuracy:0.3475177304964539 loss: 0.7711988091468811\n",
      "193: accuracy:0.3829787234042553 loss: 0.5329934358596802\n",
      "194: accuracy:0.375886524822695 loss: 0.4078083634376526\n",
      "195: accuracy:0.3900709219858156 loss: 0.17744098603725433\n",
      "196: accuracy:0.375886524822695 loss: 0.25561439990997314\n",
      "197: accuracy:0.375886524822695 loss: 0.09726287424564362\n",
      "198: accuracy:0.41134751773049644 loss: 0.20952200889587402\n",
      "199: accuracy:0.3971631205673759 loss: 0.15772227942943573\n",
      "200: accuracy:0.3971631205673759 loss: 0.09517969191074371\n",
      "201: accuracy:0.375886524822695 loss: 0.03956681489944458\n",
      "202: accuracy:0.375886524822695 loss: 0.10447775572538376\n",
      "203: accuracy:0.41134751773049644 loss: 0.10332336276769638\n",
      "204: accuracy:0.41843971631205673 loss: 0.06398163735866547\n",
      "205: accuracy:0.41843971631205673 loss: 0.15402396023273468\n",
      "206: accuracy:0.425531914893617 loss: 0.1374719738960266\n",
      "207: accuracy:0.425531914893617 loss: 0.03285035118460655\n",
      "208: accuracy:0.41843971631205673 loss: 0.08666053414344788\n",
      "209: accuracy:0.40425531914893614 loss: 0.08189426362514496\n",
      "210: accuracy:0.3971631205673759 loss: 0.12132585048675537\n",
      "211: accuracy:0.3900709219858156 loss: 0.12816786766052246\n",
      "212: accuracy:0.3900709219858156 loss: 0.040698736906051636\n",
      "213: accuracy:0.40425531914893614 loss: 0.029215730726718903\n",
      "214: accuracy:0.3900709219858156 loss: 0.1258244514465332\n",
      "215: accuracy:0.3900709219858156 loss: 0.10369765758514404\n",
      "216: accuracy:0.3900709219858156 loss: 0.014497566036880016\n",
      "217: accuracy:0.3829787234042553 loss: 0.1578684151172638\n",
      "218: accuracy:0.3829787234042553 loss: 0.02559072896838188\n",
      "219: accuracy:0.3829787234042553 loss: 0.07569229602813721\n",
      "220: accuracy:0.3900709219858156 loss: 0.08084964752197266\n",
      "221: accuracy:0.375886524822695 loss: 0.05872955918312073\n",
      "222: accuracy:0.375886524822695 loss: 0.03801227733492851\n",
      "223: accuracy:0.3829787234042553 loss: 0.017108583822846413\n",
      "224: accuracy:0.36879432624113473 loss: 0.14281533658504486\n",
      "225: accuracy:0.3475177304964539 loss: 0.03234163671731949\n",
      "226: accuracy:0.3475177304964539 loss: 0.04913431406021118\n",
      "227: accuracy:0.375886524822695 loss: 0.08735419809818268\n",
      "228: accuracy:0.3971631205673759 loss: 0.3946346640586853\n",
      "229: accuracy:0.3900709219858156 loss: 0.2251390665769577\n",
      "230: accuracy:0.3546099290780142 loss: 0.19174395501613617\n",
      "231: accuracy:0.3475177304964539 loss: 0.1088748350739479\n",
      "232: accuracy:0.375886524822695 loss: 0.1566934883594513\n",
      "233: accuracy:0.3617021276595745 loss: 0.040388353168964386\n",
      "234: accuracy:0.375886524822695 loss: 0.1720670908689499\n",
      "235: accuracy:0.36879432624113473 loss: 0.12397810816764832\n",
      "236: accuracy:0.3617021276595745 loss: 0.1341276317834854\n",
      "237: accuracy:0.3546099290780142 loss: 0.09116889536380768\n",
      "238: accuracy:0.3333333333333333 loss: 0.12451191991567612\n",
      "239: accuracy:0.3404255319148936 loss: 0.0729178935289383\n",
      "240: accuracy:0.3262411347517731 loss: 0.07902006059885025\n",
      "241: accuracy:0.3333333333333333 loss: 0.0804925411939621\n",
      "242: accuracy:0.3404255319148936 loss: 0.06462593376636505\n",
      "243: accuracy:0.3475177304964539 loss: 0.09135840833187103\n",
      "244: accuracy:0.3617021276595745 loss: 0.07638255506753922\n",
      "245: accuracy:0.3404255319148936 loss: 0.0917077362537384\n",
      "246: accuracy:0.3333333333333333 loss: 0.06611219048500061\n",
      "247: accuracy:0.3262411347517731 loss: 0.07278018444776535\n",
      "248: accuracy:0.3262411347517731 loss: 0.07198542356491089\n",
      "249: accuracy:0.3333333333333333 loss: 0.01578395627439022\n",
      "250: accuracy:0.3333333333333333 loss: 0.05890773981809616\n",
      "251: accuracy:0.3333333333333333 loss: 0.05080733448266983\n",
      "252: accuracy:0.3404255319148936 loss: 0.05239109694957733\n",
      "253: accuracy:0.3404255319148936 loss: 0.034942205995321274\n",
      "254: accuracy:0.3475177304964539 loss: 0.021611124277114868\n",
      "255: accuracy:0.3475177304964539 loss: 0.06873976439237595\n",
      "256: accuracy:0.3546099290780142 loss: 0.014633328653872013\n",
      "257: accuracy:0.3546099290780142 loss: 0.025733286514878273\n",
      "258: accuracy:0.3546099290780142 loss: 0.08709720522165298\n",
      "259: accuracy:0.3546099290780142 loss: 0.040002286434173584\n",
      "260: accuracy:0.3475177304964539 loss: 0.10900688916444778\n",
      "261: accuracy:0.3546099290780142 loss: 0.15424375236034393\n",
      "262: accuracy:0.3475177304964539 loss: 0.04791165515780449\n",
      "263: accuracy:0.3475177304964539 loss: 0.10656935721635818\n",
      "264: accuracy:0.3404255319148936 loss: 0.06255771219730377\n",
      "265: accuracy:0.3404255319148936 loss: 0.08598095178604126\n",
      "266: accuracy:0.3404255319148936 loss: 0.05063827708363533\n",
      "267: accuracy:0.3404255319148936 loss: 0.09045970439910889\n",
      "268: accuracy:0.3404255319148936 loss: 0.00940824206918478\n",
      "269: accuracy:0.3404255319148936 loss: 0.05451040714979172\n",
      "270: accuracy:0.3404255319148936 loss: 0.07761520892381668\n",
      "271: accuracy:0.3333333333333333 loss: 0.06469720602035522\n",
      "272: accuracy:0.3333333333333333 loss: 0.0838790312409401\n",
      "273: accuracy:0.3546099290780142 loss: 0.03443241864442825\n",
      "274: accuracy:0.3262411347517731 loss: 0.18818765878677368\n",
      "275: accuracy:0.2907801418439716 loss: 0.137985959649086\n",
      "276: accuracy:0.3120567375886525 loss: 0.0898834839463234\n",
      "277: accuracy:0.3049645390070922 loss: 0.02561645582318306\n",
      "278: accuracy:0.2978723404255319 loss: 0.1436174064874649\n",
      "279: accuracy:0.3191489361702128 loss: 0.10005177557468414\n",
      "280: accuracy:0.3333333333333333 loss: 0.144448384642601\n",
      "281: accuracy:0.3333333333333333 loss: 0.06831787526607513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282: accuracy:0.3262411347517731 loss: 0.019658265635371208\n",
      "283: accuracy:0.3191489361702128 loss: 0.06760052591562271\n",
      "284: accuracy:0.3262411347517731 loss: 0.07734581083059311\n",
      "285: accuracy:0.3262411347517731 loss: 0.05724101886153221\n",
      "286: accuracy:0.3333333333333333 loss: 0.08135704696178436\n",
      "287: accuracy:0.3333333333333333 loss: 0.06198201701045036\n",
      "288: accuracy:0.3475177304964539 loss: 0.040318407118320465\n",
      "289: accuracy:0.3333333333333333 loss: 0.08047070354223251\n",
      "290: accuracy:0.3333333333333333 loss: 0.044031694531440735\n",
      "291: accuracy:0.3120567375886525 loss: 0.061553213745355606\n",
      "292: accuracy:0.3120567375886525 loss: 0.024109888821840286\n",
      "293: accuracy:0.3120567375886525 loss: 0.03153865411877632\n",
      "294: accuracy:0.3191489361702128 loss: 0.00930163823068142\n",
      "295: accuracy:0.3262411347517731 loss: 0.01284746453166008\n",
      "296: accuracy:0.3404255319148936 loss: 0.04430127143859863\n",
      "297: accuracy:0.3404255319148936 loss: 0.03439498692750931\n",
      "298: accuracy:0.3404255319148936 loss: 0.02145756222307682\n",
      "299: accuracy:0.3475177304964539 loss: 0.055760886520147324\n",
      "300: accuracy:0.3404255319148936 loss: 0.006384784821420908\n",
      "301: accuracy:0.3404255319148936 loss: 0.007482140325009823\n",
      "302: accuracy:0.3404255319148936 loss: 0.006604508031159639\n",
      "303: accuracy:0.3475177304964539 loss: 0.0031283991411328316\n",
      "304: accuracy:0.3475177304964539 loss: 0.0048480648547410965\n",
      "305: accuracy:0.3404255319148936 loss: 0.013182122260332108\n",
      "306: accuracy:0.3475177304964539 loss: 0.0110166622325778\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    ")\n",
    "net = RNN_old()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "for epoch in range(1,501): \n",
    "    for step, (batch_x, sen, batch_y) in enumerate(loader):\n",
    "        pred = net(sen, batch_x)\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 2: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_s, t_e)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-new 0.6879432624113475 112轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "MAX_LENGTH = 53\n",
    "for i in range(0,467):\n",
    "    a = np.loadtxt('/Users/denhiroshi/Downloads/Embeding/ans_1.0_'+str(i)+'.csv',dtype=np.float64,delimiter=',')\n",
    "    a = a.transpose(1,0)\n",
    "    a = np.exp(a)\n",
    "    a = np.sum(a,axis=1)\n",
    "    a = np.log(a)\n",
    "    if a.shape[0] < MAX_LENGTH:\n",
    "        add = MAX_LENGTH - a.shape[0]\n",
    "        adds = np.zeros(add)\n",
    "        a = np.concatenate((a, adds))\n",
    "    inputs.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use = np.stack(inputs)\n",
    "mean = use.mean()         #计算平均数\n",
    "deviation = use.std()     #计算标准差\n",
    "# 标准化数据的公式: (数据值 - 平均数) / 标准差\n",
    "use = (use - mean) / deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.fromfile('glove_WV.dat',dtype=np.float64).reshape((467,-1,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((467, 53, 200), (467, 53))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape,use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for i in range(467):\n",
    "    inputs.append((torch.from_numpy(b[i]),torch.from_numpy(use[i]),Y[i]))\n",
    "shuffle(inputs)\n",
    "flag = int(len(inputs)*0.7)\n",
    "train = inputs[:flag]\n",
    "test = inputs[flag:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([141, 53]), torch.Size([141, 53, 200]), torch.Size([141]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_s = []\n",
    "t_e = []\n",
    "t_y = []\n",
    "for E,S,Y in test:\n",
    "    t_s.append(S)\n",
    "    t_e.append(E)\n",
    "    t_y.append(Y)\n",
    "t_s = torch.stack(t_s)\n",
    "t_e = torch.stack(t_e)\n",
    "t_y = torch.from_numpy(np.array(t_y))\n",
    "t_s.shape, t_e.shape, t_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size = 200,       # rnn input\n",
    "            hidden_size = 64,     # rnn hidden unit\n",
    "            num_layers = 1,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=65 ,out_features=7)\n",
    "        self.maxpool = torch.nn.MaxPool2d((53,1))\n",
    "\n",
    "    def forward(self,sen,x):\n",
    "        output,(h_n,c_n)=self.rnn(x.float())\n",
    "        sen = sen.reshape((sen.shape[0],sen.shape[1],1))  # (BATCH_SIZE,53,1)\n",
    "        output1 = torch.bmm(torch.transpose(output,1,2).float(),sen.float())  # (BATCH_SIZE,64,1)\n",
    "        output1 = torch.sigmoid(output1)\n",
    "        output1 = torch.softmax(output1,1)\n",
    "        output1 = torch.bmm(output,output1) # (BATCH_SIZE,53,1)\n",
    "        output = torch.cat((output1,output),2) # (BATCH_SIZE,53,65)\n",
    "        output = self.maxpool(output)   # (BATCH_SIZE,1,65)\n",
    "        output = output.reshape((output.shape[0],-1)) # (BATCH_SIZE,7)\n",
    "        x1 = self.out(output.float())\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): LSTM(200, 64, batch_first=True)\n",
      "  (out): Linear(in_features=65, out_features=7, bias=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=(53, 1), stride=(53, 1), padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "1: accuracy:0.3049645390070922 loss: 1.9433010816574097\n",
      "2: accuracy:0.3333333333333333 loss: 1.8462063074111938\n",
      "3: accuracy:0.4326241134751773 loss: 1.6321767568588257\n",
      "4: accuracy:0.49645390070921985 loss: 1.5053471326828003\n",
      "5: accuracy:0.5106382978723404 loss: 1.2786084413528442\n",
      "6: accuracy:0.5390070921985816 loss: 1.175156831741333\n",
      "7: accuracy:0.524822695035461 loss: 1.0732951164245605\n",
      "8: accuracy:0.5531914893617021 loss: 0.9134953022003174\n",
      "9: accuracy:0.5815602836879432 loss: 0.7435367107391357\n",
      "10: accuracy:0.5815602836879432 loss: 0.6175999641418457\n",
      "11: accuracy:0.5815602836879432 loss: 0.5391039252281189\n",
      "12: accuracy:0.5815602836879432 loss: 0.4725273847579956\n",
      "13: accuracy:0.5886524822695035 loss: 0.34439972043037415\n",
      "14: accuracy:0.6028368794326241 loss: 0.24604764580726624\n",
      "15: accuracy:0.6170212765957447 loss: 0.1664118468761444\n",
      "16: accuracy:0.6028368794326241 loss: 0.16386084258556366\n",
      "17: accuracy:0.6099290780141844 loss: 0.1274433135986328\n",
      "18: accuracy:0.6099290780141844 loss: 0.3185354471206665\n",
      "19: accuracy:0.5319148936170213 loss: 0.11869361996650696\n",
      "20: accuracy:0.5673758865248227 loss: 0.26219844818115234\n",
      "21: accuracy:0.624113475177305 loss: 0.08491356670856476\n",
      "22: accuracy:0.6312056737588653 loss: 0.09954380989074707\n",
      "23: accuracy:0.6666666666666666 loss: 0.09217797219753265\n",
      "24: accuracy:0.6170212765957447 loss: 0.05951058864593506\n",
      "25: accuracy:0.624113475177305 loss: 0.04984144866466522\n",
      "26: accuracy:0.6595744680851063 loss: 0.032645922154188156\n",
      "27: accuracy:0.6382978723404256 loss: 0.027983970940113068\n",
      "28: accuracy:0.624113475177305 loss: 0.022928498685359955\n",
      "29: accuracy:0.5815602836879432 loss: 0.02700561098754406\n",
      "30: accuracy:0.5673758865248227 loss: 0.01825540140271187\n",
      "31: accuracy:0.524822695035461 loss: 0.056767143309116364\n",
      "32: accuracy:0.5531914893617021 loss: 0.3699510991573334\n",
      "33: accuracy:0.574468085106383 loss: 0.33102405071258545\n",
      "34: accuracy:0.524822695035461 loss: 0.20961178839206696\n",
      "35: accuracy:0.5531914893617021 loss: 0.2133644074201584\n",
      "36: accuracy:0.5886524822695035 loss: 0.11304041743278503\n",
      "37: accuracy:0.5815602836879432 loss: 0.12906071543693542\n",
      "38: accuracy:0.6312056737588653 loss: 0.109769806265831\n",
      "39: accuracy:0.6595744680851063 loss: 0.06828761100769043\n",
      "40: accuracy:0.6382978723404256 loss: 0.05610523000359535\n",
      "41: accuracy:0.6524822695035462 loss: 0.045178353786468506\n",
      "42: accuracy:0.6524822695035462 loss: 0.05034245550632477\n",
      "43: accuracy:0.6524822695035462 loss: 0.03926897794008255\n",
      "44: accuracy:0.6595744680851063 loss: 0.03444347530603409\n",
      "45: accuracy:0.6595744680851063 loss: 0.022242646664381027\n",
      "46: accuracy:0.6666666666666666 loss: 0.01924644038081169\n",
      "47: accuracy:0.6808510638297872 loss: 0.02072754129767418\n",
      "48: accuracy:0.6879432624113475 loss: 0.016258327290415764\n",
      "49: accuracy:0.6808510638297872 loss: 0.013864755630493164\n",
      "50: accuracy:0.6737588652482269 loss: 0.012233339250087738\n",
      "51: accuracy:0.6666666666666666 loss: 0.010937189683318138\n",
      "52: accuracy:0.6666666666666666 loss: 0.009786445647478104\n",
      "53: accuracy:0.6666666666666666 loss: 0.009069938212633133\n",
      "54: accuracy:0.6524822695035462 loss: 0.008711166679859161\n",
      "55: accuracy:0.6453900709219859 loss: 0.009028449654579163\n",
      "56: accuracy:0.6524822695035462 loss: 0.006858125329017639\n",
      "57: accuracy:0.6453900709219859 loss: 0.00597110390663147\n",
      "58: accuracy:0.6453900709219859 loss: 0.006728433072566986\n",
      "59: accuracy:0.6524822695035462 loss: 0.006042366847395897\n",
      "60: accuracy:0.6595744680851063 loss: 0.005372414365410805\n",
      "61: accuracy:0.6595744680851063 loss: 0.005431022495031357\n",
      "62: accuracy:0.6595744680851063 loss: 0.004236515611410141\n",
      "63: accuracy:0.6666666666666666 loss: 0.005127556622028351\n",
      "64: accuracy:0.6595744680851063 loss: 0.00448116660118103\n",
      "65: accuracy:0.6595744680851063 loss: 0.003843197599053383\n",
      "66: accuracy:0.6666666666666666 loss: 0.004150060936808586\n",
      "67: accuracy:0.6666666666666666 loss: 0.004219941794872284\n",
      "68: accuracy:0.6595744680851063 loss: 0.004353407770395279\n",
      "69: accuracy:0.6524822695035462 loss: 0.003758316859602928\n",
      "70: accuracy:0.6595744680851063 loss: 0.003563877195119858\n",
      "71: accuracy:0.6595744680851063 loss: 0.003087500110268593\n",
      "72: accuracy:0.6595744680851063 loss: 0.003020498901605606\n",
      "73: accuracy:0.6595744680851063 loss: 0.0033670254051685333\n",
      "74: accuracy:0.6524822695035462 loss: 0.003099406138062477\n",
      "75: accuracy:0.6524822695035462 loss: 0.0031638778746128082\n",
      "76: accuracy:0.6524822695035462 loss: 0.0027914047241210938\n",
      "77: accuracy:0.6453900709219859 loss: 0.002969413995742798\n",
      "78: accuracy:0.6453900709219859 loss: 0.0030260123312473297\n",
      "79: accuracy:0.6524822695035462 loss: 0.002844393253326416\n",
      "80: accuracy:0.6524822695035462 loss: 0.0026828795671463013\n",
      "81: accuracy:0.6524822695035462 loss: 0.0028370358049869537\n",
      "82: accuracy:0.6453900709219859 loss: 0.002687990665435791\n",
      "83: accuracy:0.6453900709219859 loss: 0.002780245617032051\n",
      "84: accuracy:0.6453900709219859 loss: 0.002489496022462845\n",
      "85: accuracy:0.6453900709219859 loss: 0.0024884194135665894\n",
      "86: accuracy:0.6453900709219859 loss: 0.0022682324051856995\n",
      "87: accuracy:0.6453900709219859 loss: 0.002631470561027527\n",
      "88: accuracy:0.6453900709219859 loss: 0.002343989908695221\n",
      "89: accuracy:0.6453900709219859 loss: 0.0023718923330307007\n",
      "90: accuracy:0.6453900709219859 loss: 0.002213023602962494\n",
      "91: accuracy:0.6524822695035462 loss: 0.0024929270148277283\n",
      "92: accuracy:0.6524822695035462 loss: 0.002314060926437378\n",
      "93: accuracy:0.6524822695035462 loss: 0.002442695200443268\n",
      "94: accuracy:0.6524822695035462 loss: 0.0019778795540332794\n",
      "95: accuracy:0.6524822695035462 loss: 0.00248570553958416\n",
      "96: accuracy:0.6524822695035462 loss: 0.0025307834148406982\n",
      "97: accuracy:0.6524822695035462 loss: 0.002057807520031929\n",
      "98: accuracy:0.6595744680851063 loss: 0.0019707903265953064\n",
      "99: accuracy:0.6666666666666666 loss: 0.0016860663890838623\n",
      "100: accuracy:0.6737588652482269 loss: 0.0018322505056858063\n",
      "101: accuracy:0.6524822695035462 loss: 0.0018260665237903595\n",
      "102: accuracy:0.6453900709219859 loss: 0.0020616836845874786\n",
      "103: accuracy:0.6595744680851063 loss: 0.002443954348564148\n",
      "104: accuracy:0.6666666666666666 loss: 0.007084069773554802\n",
      "105: accuracy:0.6595744680851063 loss: 0.0031951647251844406\n",
      "106: accuracy:0.6737588652482269 loss: 0.002370476722717285\n",
      "107: accuracy:0.6737588652482269 loss: 0.002110429108142853\n",
      "108: accuracy:0.6666666666666666 loss: 0.0021999329328536987\n",
      "109: accuracy:0.6666666666666666 loss: 0.002436697483062744\n",
      "110: accuracy:0.6737588652482269 loss: 0.0026341434568166733\n",
      "111: accuracy:0.6737588652482269 loss: 0.002287890762090683\n",
      "112: accuracy:0.6879432624113475 loss: 0.0018811039626598358\n",
      "113: accuracy:0.6879432624113475 loss: 0.0018499977886676788\n",
      "114: accuracy:0.6808510638297872 loss: 0.0015340261161327362\n",
      "115: accuracy:0.6808510638297872 loss: 0.0016307719051837921\n",
      "116: accuracy:0.6879432624113475 loss: 0.0018967390060424805\n",
      "117: accuracy:0.6808510638297872 loss: 0.0016646049916744232\n",
      "118: accuracy:0.6808510638297872 loss: 0.0015818886458873749\n",
      "119: accuracy:0.6808510638297872 loss: 0.001783996820449829\n",
      "120: accuracy:0.6808510638297872 loss: 0.0015989243984222412\n",
      "121: accuracy:0.6737588652482269 loss: 0.0014537908136844635\n",
      "122: accuracy:0.6737588652482269 loss: 0.0012917742133140564\n",
      "123: accuracy:0.6737588652482269 loss: 0.0014076977968215942\n",
      "124: accuracy:0.6737588652482269 loss: 0.0013692490756511688\n",
      "125: accuracy:0.6737588652482269 loss: 0.0013205669820308685\n",
      "126: accuracy:0.6737588652482269 loss: 0.0012695901095867157\n",
      "127: accuracy:0.6737588652482269 loss: 0.001204710453748703\n",
      "128: accuracy:0.6737588652482269 loss: 0.001134674996137619\n",
      "129: accuracy:0.6737588652482269 loss: 0.0013100095093250275\n",
      "130: accuracy:0.6737588652482269 loss: 0.0013776011765003204\n",
      "131: accuracy:0.6737588652482269 loss: 0.0011471286416053772\n",
      "132: accuracy:0.6737588652482269 loss: 0.0011342726647853851\n",
      "133: accuracy:0.6737588652482269 loss: 0.001140844076871872\n",
      "134: accuracy:0.6737588652482269 loss: 0.001132957637310028\n",
      "135: accuracy:0.6666666666666666 loss: 0.0010447874665260315\n",
      "136: accuracy:0.6666666666666666 loss: 0.0010811761021614075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137: accuracy:0.6666666666666666 loss: 0.0009977295994758606\n",
      "138: accuracy:0.6666666666666666 loss: 0.00104542076587677\n",
      "139: accuracy:0.6666666666666666 loss: 0.0009148344397544861\n",
      "140: accuracy:0.6666666666666666 loss: 0.0008632875978946686\n",
      "141: accuracy:0.6666666666666666 loss: 0.0010071024298667908\n",
      "142: accuracy:0.6666666666666666 loss: 0.0009018294513225555\n",
      "143: accuracy:0.6666666666666666 loss: 0.0010723061859607697\n",
      "144: accuracy:0.6666666666666666 loss: 0.001063574105501175\n",
      "145: accuracy:0.6666666666666666 loss: 0.0008732005953788757\n",
      "146: accuracy:0.6666666666666666 loss: 0.0009153597056865692\n",
      "147: accuracy:0.6666666666666666 loss: 0.0008699037134647369\n",
      "148: accuracy:0.6666666666666666 loss: 0.0009642206132411957\n",
      "149: accuracy:0.6666666666666666 loss: 0.0008136220276355743\n",
      "150: accuracy:0.6595744680851063 loss: 0.0009157955646514893\n",
      "151: accuracy:0.6595744680851063 loss: 0.0007381327450275421\n",
      "152: accuracy:0.6595744680851063 loss: 0.0007743127644062042\n",
      "153: accuracy:0.6595744680851063 loss: 0.000845540314912796\n",
      "154: accuracy:0.6595744680851063 loss: 0.0007657557725906372\n",
      "155: accuracy:0.6595744680851063 loss: 0.0007358789443969727\n",
      "156: accuracy:0.6595744680851063 loss: 0.000827256590127945\n",
      "157: accuracy:0.6595744680851063 loss: 0.0008114762604236603\n",
      "158: accuracy:0.6595744680851063 loss: 0.0008452124893665314\n",
      "159: accuracy:0.6595744680851063 loss: 0.0007660984992980957\n",
      "160: accuracy:0.6595744680851063 loss: 0.0007523335516452789\n",
      "161: accuracy:0.6595744680851063 loss: 0.0007281042635440826\n",
      "162: accuracy:0.6595744680851063 loss: 0.0007149875164031982\n",
      "163: accuracy:0.6595744680851063 loss: 0.0007968731224536896\n",
      "164: accuracy:0.6595744680851063 loss: 0.0008462779223918915\n",
      "165: accuracy:0.6595744680851063 loss: 0.0007119141519069672\n",
      "166: accuracy:0.6595744680851063 loss: 0.0008151009678840637\n",
      "167: accuracy:0.6595744680851063 loss: 0.0006587579846382141\n",
      "168: accuracy:0.6595744680851063 loss: 0.0006743855774402618\n",
      "169: accuracy:0.6595744680851063 loss: 0.0007705427706241608\n",
      "170: accuracy:0.6595744680851063 loss: 0.0006128326058387756\n",
      "171: accuracy:0.6595744680851063 loss: 0.0005637817084789276\n",
      "172: accuracy:0.6595744680851063 loss: 0.0006308406591415405\n",
      "173: accuracy:0.6595744680851063 loss: 0.0007073953747749329\n",
      "174: accuracy:0.6595744680851063 loss: 0.0006887689232826233\n",
      "175: accuracy:0.6595744680851063 loss: 0.0006721131503582001\n",
      "176: accuracy:0.6595744680851063 loss: 0.0006576254963874817\n",
      "177: accuracy:0.6524822695035462 loss: 0.0006757080554962158\n",
      "178: accuracy:0.6524822695035462 loss: 0.0006686821579933167\n",
      "179: accuracy:0.6524822695035462 loss: 0.0005606040358543396\n",
      "180: accuracy:0.6524822695035462 loss: 0.0006209723651409149\n",
      "181: accuracy:0.6524822695035462 loss: 0.0005630850791931152\n",
      "182: accuracy:0.6524822695035462 loss: 0.0006117671728134155\n",
      "183: accuracy:0.6524822695035462 loss: 0.0006004087626934052\n",
      "184: accuracy:0.6524822695035462 loss: 0.0005598664283752441\n",
      "185: accuracy:0.6524822695035462 loss: 0.000602230429649353\n",
      "186: accuracy:0.6524822695035462 loss: 0.000531323254108429\n",
      "187: accuracy:0.6524822695035462 loss: 0.00046457722783088684\n",
      "188: accuracy:0.6524822695035462 loss: 0.0006042495369911194\n",
      "189: accuracy:0.6524822695035462 loss: 0.0005611181259155273\n",
      "190: accuracy:0.6524822695035462 loss: 0.0005367770791053772\n",
      "191: accuracy:0.6524822695035462 loss: 0.0005759336054325104\n",
      "192: accuracy:0.6524822695035462 loss: 0.0005970969796180725\n",
      "193: accuracy:0.6524822695035462 loss: 0.0005143024027347565\n",
      "194: accuracy:0.6524822695035462 loss: 0.00046384334564208984\n",
      "195: accuracy:0.6524822695035462 loss: 0.0005215741693973541\n",
      "196: accuracy:0.6524822695035462 loss: 0.0004964061081409454\n",
      "197: accuracy:0.6524822695035462 loss: 0.0004875399172306061\n",
      "198: accuracy:0.6524822695035462 loss: 0.0005205757915973663\n",
      "199: accuracy:0.6524822695035462 loss: 0.0005607940256595612\n",
      "200: accuracy:0.6524822695035462 loss: 0.0005382075905799866\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    "    drop_last=True,\n",
    ")\n",
    "net = RNN()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "for epoch in range(1,201): \n",
    "    for step, (batch_x, sen, batch_y) in enumerate(loader):\n",
    "        pred = net(sen, batch_x)\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 1: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_s, t_e)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-new-bi 0.6879432624113475 112轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class bi_RNN_new(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size = 200,       # rnn input\n",
    "            hidden_size = 64,     # rnn hidden unit\n",
    "            num_layers = 1,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=129 ,out_features=7)\n",
    "        self.maxpool = torch.nn.MaxPool2d((53,1))\n",
    "\n",
    "    def forward(self,sen,x):\n",
    "        output,(h_n,c_n)=self.rnn(x.float())\n",
    "        sen = sen.reshape((sen.shape[0],sen.shape[1],1))  # (BATCH_SIZE,53,1)\n",
    "        output1 = torch.bmm(torch.transpose(output,1,2).float(),sen.float())  # (BATCH_SIZE,64,1)\n",
    "        output1 = torch.sigmoid(output1)\n",
    "        output1 = torch.softmax(output1,1)\n",
    "        output1 = torch.bmm(output,output1) # (BATCH_SIZE,53,1)\n",
    "        output = torch.cat((output1,output),2) # (BATCH_SIZE,53,65)\n",
    "        output = self.maxpool(output)   # (BATCH_SIZE,1,65)\n",
    "        output = output.reshape((output.shape[0],-1)) # (BATCH_SIZE,7)\n",
    "        x1 = self.out(output.float())\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi_RNN_new(\n",
      "  (rnn): LSTM(200, 64, batch_first=True, bidirectional=True)\n",
      "  (out): Linear(in_features=129, out_features=7, bias=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=(53, 1), stride=(53, 1), padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "1: accuracy:0.2198581560283688 loss: 1.9409806728363037\n",
      "2: accuracy:0.3829787234042553 loss: 1.7989925146102905\n",
      "3: accuracy:0.41134751773049644 loss: 1.6326566934585571\n",
      "4: accuracy:0.425531914893617 loss: 1.2530981302261353\n",
      "5: accuracy:0.46808510638297873 loss: 1.2234959602355957\n",
      "6: accuracy:0.574468085106383 loss: 1.0577131509780884\n",
      "7: accuracy:0.6099290780141844 loss: 0.8930600881576538\n",
      "8: accuracy:0.6028368794326241 loss: 0.6624197363853455\n",
      "9: accuracy:0.6099290780141844 loss: 0.5008664131164551\n",
      "10: accuracy:0.574468085106383 loss: 0.41010335087776184\n",
      "11: accuracy:0.6453900709219859 loss: 0.34178754687309265\n",
      "12: accuracy:0.6170212765957447 loss: 0.2322080135345459\n",
      "13: accuracy:0.6312056737588653 loss: 0.15362492203712463\n",
      "14: accuracy:0.6170212765957447 loss: 0.11592675745487213\n",
      "15: accuracy:0.6028368794326241 loss: 0.12168031185865402\n",
      "16: accuracy:0.6524822695035462 loss: 0.05084272101521492\n",
      "17: accuracy:0.6312056737588653 loss: 0.04704565182328224\n",
      "18: accuracy:0.6099290780141844 loss: 0.06971948593854904\n",
      "19: accuracy:0.5957446808510638 loss: 0.03799133747816086\n",
      "20: accuracy:0.5815602836879432 loss: 0.04296228289604187\n",
      "21: accuracy:0.624113475177305 loss: 0.02433195896446705\n",
      "22: accuracy:0.6382978723404256 loss: 0.022006982937455177\n",
      "23: accuracy:0.624113475177305 loss: 0.019580094143748283\n",
      "24: accuracy:0.6312056737588653 loss: 0.01374783180654049\n",
      "25: accuracy:0.6099290780141844 loss: 0.010905632749199867\n",
      "26: accuracy:0.6099290780141844 loss: 0.007534109055995941\n",
      "27: accuracy:0.6170212765957447 loss: 0.008159514516592026\n",
      "28: accuracy:0.6099290780141844 loss: 0.005154317244887352\n",
      "29: accuracy:0.624113475177305 loss: 0.005106009542942047\n",
      "30: accuracy:0.6312056737588653 loss: 0.004588190466165543\n",
      "31: accuracy:0.6382978723404256 loss: 0.004159417003393173\n",
      "32: accuracy:0.6382978723404256 loss: 0.003484882414340973\n",
      "33: accuracy:0.624113475177305 loss: 0.003395289182662964\n",
      "34: accuracy:0.6453900709219859 loss: 0.0031434930860996246\n",
      "35: accuracy:0.6453900709219859 loss: 0.002432607114315033\n",
      "36: accuracy:0.6524822695035462 loss: 0.0024480298161506653\n",
      "37: accuracy:0.6595744680851063 loss: 0.0024709776043891907\n",
      "38: accuracy:0.6382978723404256 loss: 0.0019002407789230347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ccb1ebe290c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m201\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_F\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 计算loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ad8997d7ee9c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sen, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0msen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (BATCH_SIZE,53,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (BATCH_SIZE,64,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    "    drop_last=True,\n",
    ")\n",
    "net = bi_RNN_new()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "for epoch in range(1,201): \n",
    "    for step, (batch_x, sen, batch_y) in enumerate(loader):\n",
    "        pred = net(sen, batch_x)\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 1: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_s, t_e)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
