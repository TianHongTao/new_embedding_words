{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function set_backend in module tensorly:\n",
      "\n",
      "set_backend(backend_name)\n",
      "    Sets the backend for TensorLy\n",
      "    \n",
      "        The backend will be set as specified and operations will used that backend\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    backend_name : {'mxnet', 'numpy', 'pytorch', 'tensorflow', 'cupy'}, default is 'numpy'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pytorch backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.decomposition import tucker\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import preprocessing\n",
    "from random import shuffle\n",
    "BATCH_SIZE = 128\n",
    "tl.set_backend('pytorch')\n",
    "help(tl.set_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 85, 200), (8000,), (8000,), (8000,), (8000, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.fromfile('tensors_2010.dat',dtype=np.float64).reshape((8000,-1,200))\n",
    "Y = np.fromfile('tensors_2010_labels.dat',dtype=np.int)\n",
    "position_tag_1 = np.fromfile('tensors_2010_entity1.dat',dtype=np.int)\n",
    "position_tag_2 = np.fromfile('tensors_2010_entity2.dat',dtype=np.int)\n",
    "position_tag = np.concatenate((position_tag_1,position_tag_2)).reshape((8000,-1))\n",
    "b.shape,Y.shape,position_tag_1.shape,position_tag_2.shape,position_tag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for i in range(8000):\n",
    "    inputs.append((torch.from_numpy(b[i]),Y[i]))\n",
    "shuffle(inputs)\n",
    "flag = int(len(inputs)*0.7)\n",
    "train = inputs[:flag]\n",
    "test = inputs[flag:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2400, 85, 200]), torch.Size([2400]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_e = []\n",
    "t_y = []\n",
    "for E,Y in test:\n",
    "    t_e.append(E)\n",
    "    t_y.append(Y)\n",
    "t_e = torch.stack(t_e)\n",
    "t_y = torch.from_numpy(np.array(t_y))\n",
    "t_e.shape, t_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.6170212765957447 23轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(  # input shape (1, 53, 200)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,      # input height\n",
    "                out_channels=85,    # n_filters\n",
    "                kernel_size=(1,200),      # filter size\n",
    "            ),      # output shape (16, 53, 200)\n",
    "            nn.ReLU(),    # activation\n",
    "            nn.MaxPool2d(kernel_size=(85,1)),    # 在 2x2 空间里向下采样, output shape (16, 53, 1)\n",
    "        )\n",
    "        self.output = nn.Linear(85, 10)   # fully connected layer, output 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        torch.set_default_dtype(torch.double)\n",
    "        x = x.reshape((-1,1,85,200)).double()\n",
    "        x = self.conv1(x.double())\n",
    "        x = x.view(x.size(0), -1)   # 展平\n",
    "#         x = self.out(x)\n",
    "        output = self.output(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 85, kernel_size=(1, 200), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(85, 1), stride=(85, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (output): Linear(in_features=85, out_features=10, bias=True)\n",
      ")\n",
      "1: accuracy:0.17916666666666667 loss: 2.146329321607155 time: 1.1540908813476562\n",
      "2: accuracy:0.525 loss: 1.041457438806205 time: 4.637909889221191\n",
      "3: accuracy:0.5691666666666667 loss: 0.77110157895037 time: 8.3856680393219\n",
      "4: accuracy:0.57875 loss: 0.6551115168887782 time: 12.139734983444214\n",
      "5: accuracy:0.5779166666666666 loss: 0.5794869122309064 time: 16.12717604637146\n",
      "6: accuracy:0.57 loss: 0.38078476767853314 time: 20.473067045211792\n",
      "7: accuracy:0.5441666666666667 loss: 0.4189692483672614 time: 25.96641993522644\n",
      "8: accuracy:0.545 loss: 0.3762888939005211 time: 31.09431791305542\n",
      "9: accuracy:0.55625 loss: 0.27092003395490966 time: 35.53653812408447\n",
      "10: accuracy:0.5404166666666667 loss: 0.13612011870426288 time: 40.857999086380005\n",
      "11: accuracy:0.55125 loss: 0.14290656899362764 time: 45.337767124176025\n",
      "12: accuracy:0.545 loss: 0.09467609937104667 time: 49.711037158966064\n",
      "13: accuracy:0.5479166666666667 loss: 0.05249949872017814 time: 54.14724612236023\n",
      "14: accuracy:0.5433333333333333 loss: 0.0583042235065992 time: 58.164167165756226\n",
      "15: accuracy:0.54375 loss: 0.06459882450408146 time: 62.23339581489563\n",
      "16: accuracy:0.5408333333333334 loss: 0.028487553233269267 time: 66.37303113937378\n",
      "17: accuracy:0.54625 loss: 0.025537683656695728 time: 70.53562998771667\n",
      "18: accuracy:0.545 loss: 0.01417972033817667 time: 74.73487091064453\n",
      "19: accuracy:0.5429166666666667 loss: 0.043287624044106854 time: 78.87269115447998\n",
      "20: accuracy:0.5341666666666667 loss: 0.013782038312204153 time: 83.0571460723877\n",
      "21: accuracy:0.54875 loss: 0.008734223844995281 time: 87.1184549331665\n",
      "22: accuracy:0.5425 loss: 0.009314786124963495 time: 91.32253408432007\n",
      "23: accuracy:0.53875 loss: 0.008174997421815974 time: 95.4273099899292\n",
      "24: accuracy:0.5404166666666667 loss: 0.009530540237678804 time: 99.85798120498657\n",
      "25: accuracy:0.5416666666666666 loss: 0.00880236393715467 time: 104.61912703514099\n",
      "26: accuracy:0.5275 loss: 0.0161596565002026 time: 109.43386220932007\n",
      "27: accuracy:0.5329166666666667 loss: 0.008088015076899619 time: 114.27934789657593\n",
      "28: accuracy:0.5408333333333334 loss: 0.007759517270536328 time: 118.62842583656311\n",
      "29: accuracy:0.5433333333333333 loss: 0.005139183774631786 time: 123.20744395256042\n",
      "30: accuracy:0.53875 loss: 0.006278076061357792 time: 127.24025106430054\n",
      "31: accuracy:0.535 loss: 0.003983084297101737 time: 131.38558506965637\n",
      "32: accuracy:0.5325 loss: 0.0055201247924018286 time: 135.4228799343109\n",
      "33: accuracy:0.5375 loss: 0.005973670042150642 time: 139.57467699050903\n",
      "34: accuracy:0.54125 loss: 0.0064655920959825605 time: 143.74540519714355\n",
      "35: accuracy:0.5375 loss: 0.006300103275045533 time: 148.04964184761047\n",
      "36: accuracy:0.5329166666666667 loss: 0.007136798193222802 time: 152.29947900772095\n",
      "37: accuracy:0.5375 loss: 0.007090044104117194 time: 156.57376885414124\n",
      "38: accuracy:0.5354166666666667 loss: 0.003833631412854893 time: 160.63096117973328\n",
      "39: accuracy:0.5391666666666667 loss: 0.0056753396452374014 time: 164.8080689907074\n",
      "40: accuracy:0.5420833333333334 loss: 0.004274515132804845 time: 169.03863310813904\n",
      "41: accuracy:0.535 loss: 0.004088721794931666 time: 173.15967988967896\n",
      "42: accuracy:0.5316666666666666 loss: 0.007732381830313429 time: 177.4132399559021\n",
      "43: accuracy:0.5433333333333333 loss: 0.005447330870229971 time: 181.5864748954773\n",
      "44: accuracy:0.535 loss: 0.005039988838102921 time: 185.70428800582886\n",
      "45: accuracy:0.5395833333333333 loss: 0.0028373713773715403 time: 189.8530170917511\n",
      "46: accuracy:0.5320833333333334 loss: 0.006457877298697659 time: 194.0082609653473\n",
      "47: accuracy:0.53375 loss: 0.006467581909527986 time: 198.09658312797546\n",
      "48: accuracy:0.5179166666666667 loss: 0.006966525057922929 time: 202.182599067688\n",
      "49: accuracy:0.525 loss: 0.009125183402408758 time: 206.34046006202698\n",
      "50: accuracy:0.5129166666666667 loss: 0.5607093443598251 time: 210.43334913253784\n",
      "51: accuracy:0.5091666666666667 loss: 0.7897397041914701 time: 214.5552110671997\n",
      "52: accuracy:0.5020833333333333 loss: 0.26257780334245107 time: 218.81873512268066\n",
      "53: accuracy:0.535 loss: 0.07322024708125127 time: 223.0516140460968\n",
      "54: accuracy:0.535 loss: 0.06426122239032266 time: 227.17541694641113\n",
      "55: accuracy:0.5320833333333334 loss: 0.013175157136110218 time: 231.22678399085999\n",
      "56: accuracy:0.54125 loss: 0.002919378060488447 time: 235.3214030265808\n",
      "57: accuracy:0.54125 loss: 0.002886231502211058 time: 239.44808387756348\n",
      "58: accuracy:0.53875 loss: 0.004437226454450598 time: 243.51070499420166\n",
      "59: accuracy:0.5391666666666667 loss: 0.0022042822670451617 time: 247.59323692321777\n",
      "60: accuracy:0.54125 loss: 0.002927076835841763 time: 251.69831609725952\n",
      "61: accuracy:0.54375 loss: 0.03418314510593135 time: 255.7139549255371\n",
      "62: accuracy:0.5425 loss: 0.0014191849248178735 time: 259.9303550720215\n",
      "63: accuracy:0.5404166666666667 loss: 0.0025141429978753974 time: 264.0948851108551\n",
      "64: accuracy:0.5383333333333333 loss: 0.0668742107746593 time: 268.3541328907013\n",
      "65: accuracy:0.5404166666666667 loss: 0.001967925466896641 time: 272.4528739452362\n",
      "66: accuracy:0.5454166666666667 loss: 0.0022222328818601793 time: 276.5966372489929\n",
      "67: accuracy:0.54375 loss: 0.0014502017661507122 time: 280.6945471763611\n",
      "68: accuracy:0.5295833333333333 loss: 0.011882349104596908 time: 284.76500391960144\n",
      "69: accuracy:0.5479166666666667 loss: 0.0013492125467020125 time: 288.924987077713\n",
      "70: accuracy:0.5445833333333333 loss: 0.0012028563407753545 time: 292.97138500213623\n",
      "71: accuracy:0.5341666666666667 loss: 0.003885762915540051 time: 297.22397899627686\n",
      "72: accuracy:0.5433333333333333 loss: 0.0035476203715219764 time: 301.3035361766815\n",
      "73: accuracy:0.5470833333333334 loss: 0.0019235189686765473 time: 305.46402406692505\n",
      "74: accuracy:0.54375 loss: 0.001258891419321788 time: 309.55932784080505\n",
      "75: accuracy:0.5454166666666667 loss: 0.0013935927464802638 time: 314.36595010757446\n",
      "76: accuracy:0.54125 loss: 0.0012623546813389716 time: 319.1741421222687\n",
      "77: accuracy:0.5454166666666667 loss: 0.13155420929332448 time: 323.38118386268616\n",
      "78: accuracy:0.5433333333333333 loss: 0.0009639830382973209 time: 327.6127872467041\n",
      "79: accuracy:0.5366666666666666 loss: 0.0010089007537411325 time: 331.6167719364166\n",
      "80: accuracy:0.5454166666666667 loss: 0.0007830504048817468 time: 335.740510225296\n",
      "81: accuracy:0.5441666666666667 loss: 0.00159569816607127 time: 339.98501801490784\n",
      "82: accuracy:0.5370833333333334 loss: 0.0008849823966123847 time: 344.1476800441742\n",
      "83: accuracy:0.5391666666666667 loss: 0.001962551936298411 time: 348.31901001930237\n",
      "84: accuracy:0.5354166666666667 loss: 0.09664510106059976 time: 352.5328118801117\n",
      "85: accuracy:0.535 loss: 0.0007711730950483581 time: 356.6700849533081\n",
      "86: accuracy:0.5375 loss: 0.10957472652467762 time: 360.85110998153687\n",
      "87: accuracy:0.5433333333333333 loss: 0.0006819099531188717 time: 365.1359820365906\n",
      "88: accuracy:0.5283333333333333 loss: 0.0021794134852961 time: 369.20599389076233\n",
      "89: accuracy:0.5370833333333334 loss: 0.0022678146079739303 time: 373.322074174881\n",
      "90: accuracy:0.5441666666666667 loss: 0.0005341532895439313 time: 377.32361483573914\n",
      "91: accuracy:0.5420833333333334 loss: 0.0005885841041255698 time: 381.542680978775\n",
      "92: accuracy:0.5366666666666666 loss: 0.002381120834012136 time: 385.65041995048523\n",
      "93: accuracy:0.54125 loss: 0.001148933122478892 time: 389.80025696754456\n",
      "94: accuracy:0.5358333333333334 loss: 0.0007227445780601457 time: 393.8997731208801\n",
      "95: accuracy:0.5283333333333333 loss: 0.002103157977548074 time: 398.07365012168884\n",
      "96: accuracy:0.53 loss: 0.0022748859233520467 time: 402.19497895240784\n",
      "97: accuracy:0.5383333333333333 loss: 0.0010164333548350608 time: 406.32344698905945\n",
      "98: accuracy:0.5433333333333333 loss: 0.002000995198174629 time: 410.36869287490845\n",
      "99: accuracy:0.5383333333333333 loss: 0.00397817635254138 time: 414.7658660411835\n",
      "100: accuracy:0.5258333333333334 loss: 0.00957661598932058 time: 418.8881368637085\n",
      "101: accuracy:0.53625 loss: 0.002277668664990815 time: 423.0969350337982\n",
      "102: accuracy:0.535 loss: 0.0036587768180917123 time: 427.2864670753479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103: accuracy:0.5383333333333333 loss: 0.0018715206817196302 time: 431.49422788619995\n",
      "104: accuracy:0.5316666666666666 loss: 0.0025289028217073545 time: 435.77449584007263\n",
      "105: accuracy:0.5408333333333334 loss: 0.0015372050808740688 time: 439.9542009830475\n",
      "106: accuracy:0.5425 loss: 0.0007837731214967733 time: 444.0490641593933\n",
      "107: accuracy:0.5279166666666667 loss: 0.04968174517377747 time: 448.2536370754242\n",
      "108: accuracy:0.5391666666666667 loss: 0.10821385528241688 time: 452.34173607826233\n",
      "109: accuracy:0.53875 loss: 0.00737483442413672 time: 456.47475695610046\n",
      "110: accuracy:0.5341666666666667 loss: 0.0014712077090381368 time: 460.5612678527832\n",
      "111: accuracy:0.5275 loss: 0.0034949366017675123 time: 464.5616750717163\n",
      "112: accuracy:0.5254166666666666 loss: 0.00938161999927314 time: 468.7561900615692\n",
      "113: accuracy:0.5108333333333334 loss: 0.5458309414842806 time: 472.81364917755127\n",
      "114: accuracy:0.4845833333333333 loss: 0.6062209291178869 time: 476.9093601703644\n",
      "115: accuracy:0.5041666666666667 loss: 0.3447637074736374 time: 481.12569427490234\n",
      "116: accuracy:0.5225 loss: 0.1989751473271204 time: 485.20099806785583\n",
      "117: accuracy:0.5133333333333333 loss: 0.06993203746991447 time: 489.2874720096588\n",
      "118: accuracy:0.5283333333333333 loss: 0.011367493082965807 time: 493.43493914604187\n",
      "119: accuracy:0.52125 loss: 0.005082524264538471 time: 497.56122303009033\n",
      "120: accuracy:0.5170833333333333 loss: 0.0009428153042849927 time: 501.5609221458435\n",
      "121: accuracy:0.5225 loss: 0.0002746908490216561 time: 505.71412920951843\n",
      "122: accuracy:0.5233333333333333 loss: 0.002133547718494719 time: 509.7448830604553\n",
      "123: accuracy:0.5183333333333333 loss: 0.0006445788074726441 time: 513.9510970115662\n",
      "124: accuracy:0.5229166666666667 loss: 0.0003542213597982173 time: 518.0972259044647\n",
      "125: accuracy:0.52 loss: 0.00036385217279368085 time: 522.2336130142212\n",
      "126: accuracy:0.5225 loss: 0.0005820833114357962 time: 526.2980790138245\n",
      "127: accuracy:0.5229166666666667 loss: 0.0006415883789287315 time: 530.422956943512\n",
      "128: accuracy:0.5183333333333333 loss: 0.08885091102430881 time: 534.6066610813141\n",
      "129: accuracy:0.5195833333333333 loss: 0.0003177860872654783 time: 538.750895023346\n",
      "130: accuracy:0.5220833333333333 loss: 0.0004025130159895588 time: 542.9189171791077\n",
      "131: accuracy:0.5175 loss: 0.0010552733352257523 time: 547.32301902771\n",
      "132: accuracy:0.5195833333333333 loss: 0.0022914464459642316 time: 551.9776191711426\n",
      "133: accuracy:0.5220833333333333 loss: 0.00046700213243737465 time: 557.203852891922\n",
      "134: accuracy:0.5254166666666666 loss: 0.00020256774998253166 time: 562.0133721828461\n",
      "135: accuracy:0.5191666666666667 loss: 0.0011749287565472125 time: 566.651330947876\n",
      "136: accuracy:0.5208333333333334 loss: 0.0013090666630486117 time: 570.7385129928589\n",
      "137: accuracy:0.51875 loss: 0.000345830503335043 time: 574.7770040035248\n",
      "138: accuracy:0.51625 loss: 0.0005910599537003649 time: 578.9269480705261\n",
      "139: accuracy:0.52 loss: 0.00021050134674110814 time: 583.0852782726288\n",
      "140: accuracy:0.5216666666666666 loss: 0.00022644968202055384 time: 587.2648830413818\n",
      "141: accuracy:0.5208333333333334 loss: 0.0011437153663849016 time: 591.4303181171417\n",
      "142: accuracy:0.5179166666666667 loss: 0.0014144657391210597 time: 595.5248339176178\n",
      "143: accuracy:0.52125 loss: 0.0013315201480828707 time: 599.6223080158234\n",
      "144: accuracy:0.5120833333333333 loss: 0.0024458552077488063 time: 603.7162289619446\n",
      "145: accuracy:0.5179166666666667 loss: 0.0012674452062700868 time: 607.8631629943848\n",
      "146: accuracy:0.51875 loss: 0.0009635686098456268 time: 611.9991662502289\n",
      "147: accuracy:0.51875 loss: 0.0004182408847792987 time: 616.1564269065857\n",
      "148: accuracy:0.5183333333333333 loss: 0.0003545053037303271 time: 620.2460358142853\n",
      "149: accuracy:0.5179166666666667 loss: 0.0005816092142501214 time: 624.3810470104218\n",
      "150: accuracy:0.5175 loss: 0.011705076400762439 time: 628.4713981151581\n",
      "151: accuracy:0.5216666666666666 loss: 0.0011343086248015687 time: 632.6363198757172\n",
      "152: accuracy:0.5183333333333333 loss: 0.00018207830347690898 time: 636.7177951335907\n",
      "153: accuracy:0.5170833333333333 loss: 0.00026035109077769333 time: 640.9692080020905\n",
      "154: accuracy:0.51625 loss: 0.0015378958339618935 time: 645.0572588443756\n",
      "155: accuracy:0.5220833333333333 loss: 0.00023085453537359957 time: 649.0885381698608\n",
      "156: accuracy:0.515 loss: 0.0036822872139447276 time: 653.2667219638824\n",
      "157: accuracy:0.5195833333333333 loss: 0.0004456226399819742 time: 657.3969900608063\n",
      "158: accuracy:0.5225 loss: 0.00022576375048059816 time: 661.4315781593323\n",
      "159: accuracy:0.5166666666666667 loss: 0.0003173121852268817 time: 665.599860906601\n",
      "160: accuracy:0.5245833333333333 loss: 0.010820820646834345 time: 669.6970422267914\n",
      "161: accuracy:0.52125 loss: 0.00035002743020461427 time: 674.034548997879\n",
      "162: accuracy:0.5195833333333333 loss: 0.00035534322261982377 time: 678.0733549594879\n",
      "163: accuracy:0.5179166666666667 loss: 0.0031798053891308074 time: 682.2654340267181\n",
      "164: accuracy:0.5154166666666666 loss: 0.12887469111051228 time: 686.4663300514221\n",
      "165: accuracy:0.5141666666666667 loss: 0.002138655629473501 time: 690.6847610473633\n",
      "166: accuracy:0.5145833333333333 loss: 0.0005229930211083714 time: 694.7930960655212\n",
      "167: accuracy:0.5175 loss: 0.00030652369685320834 time: 698.8122742176056\n",
      "168: accuracy:0.5216666666666666 loss: 0.0005518913077931609 time: 703.0163850784302\n",
      "169: accuracy:0.5179166666666667 loss: 0.0006632519712663661 time: 707.2749412059784\n",
      "170: accuracy:0.5104166666666666 loss: 0.0009036509999916682 time: 711.392021894455\n",
      "171: accuracy:0.5208333333333334 loss: 0.003308680060424833 time: 715.5157880783081\n",
      "172: accuracy:0.5166666666666667 loss: 0.0009300859970463149 time: 719.6835949420929\n",
      "173: accuracy:0.52625 loss: 0.0034765871663709766 time: 723.8570621013641\n",
      "174: accuracy:0.4995833333333333 loss: 0.016597211434562133 time: 727.8845679759979\n",
      "175: accuracy:0.49041666666666667 loss: 0.1553462223488083 time: 732.0350320339203\n",
      "176: accuracy:0.4891666666666667 loss: 0.7598955427380146 time: 736.5495002269745\n",
      "177: accuracy:0.5108333333333334 loss: 0.5636638444637777 time: 741.0631821155548\n",
      "178: accuracy:0.48125 loss: 0.19057480858185344 time: 746.3001670837402\n",
      "179: accuracy:0.5033333333333333 loss: 0.20246312922750537 time: 751.2914118766785\n",
      "180: accuracy:0.5116666666666667 loss: 0.011648983197222785 time: 755.4779450893402\n",
      "181: accuracy:0.52375 loss: 0.003932010465527998 time: 759.6787922382355\n",
      "182: accuracy:0.53 loss: 0.00397215402072942 time: 763.7048180103302\n",
      "183: accuracy:0.5195833333333333 loss: 0.009637402501955543 time: 767.9847211837769\n",
      "184: accuracy:0.53125 loss: 0.05528330044589687 time: 772.1753311157227\n",
      "185: accuracy:0.52625 loss: 0.0017656117303844728 time: 776.2687220573425\n",
      "186: accuracy:0.5266666666666666 loss: 0.00028065024101033703 time: 780.4131188392639\n",
      "187: accuracy:0.5283333333333333 loss: 0.0009114374444369067 time: 784.5520560741425\n",
      "188: accuracy:0.5316666666666666 loss: 0.0015343597720762722 time: 788.7454500198364\n",
      "189: accuracy:0.5316666666666666 loss: 0.0009607108242421854 time: 792.8290400505066\n",
      "190: accuracy:0.5329166666666667 loss: 0.0009716669831937846 time: 796.9281640052795\n",
      "191: accuracy:0.5358333333333334 loss: 0.0005407462514054754 time: 801.2006430625916\n",
      "192: accuracy:0.5304166666666666 loss: 0.00011704116190742672 time: 805.388543844223\n",
      "193: accuracy:0.52 loss: 0.0005831246936555678 time: 809.5200979709625\n",
      "194: accuracy:0.52625 loss: 0.1023684426078337 time: 813.5874931812286\n",
      "195: accuracy:0.5333333333333333 loss: 0.00010635912040294834 time: 817.7319889068604\n",
      "196: accuracy:0.5333333333333333 loss: 0.0004227302388942829 time: 821.9003319740295\n",
      "197: accuracy:0.5275 loss: 0.00017524627513679225 time: 825.9110078811646\n",
      "198: accuracy:0.5295833333333333 loss: 0.00033847398125302214 time: 830.0212531089783\n",
      "199: accuracy:0.5191666666666667 loss: 0.08710768409279461 time: 833.9723789691925\n",
      "200: accuracy:0.5266666666666666 loss: 0.00029484307594485407 time: 838.0991542339325\n",
      "201: accuracy:0.5295833333333333 loss: 0.0020181094301290683 time: 842.2135601043701\n",
      "202: accuracy:0.5291666666666667 loss: 0.0002002681034388807 time: 846.3406419754028\n",
      "203: accuracy:0.53125 loss: 8.989568960406265e-05 time: 850.4493601322174\n",
      "204: accuracy:0.5316666666666666 loss: 0.1851946598732741 time: 854.5605540275574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205: accuracy:0.53375 loss: 0.0002216413431419921 time: 858.6821300983429\n",
      "206: accuracy:0.5279166666666667 loss: 0.000314714283563524 time: 862.890340089798\n",
      "207: accuracy:0.5258333333333334 loss: 0.00025114605736566153 time: 867.026242017746\n",
      "208: accuracy:0.5333333333333333 loss: 8.072308817017038e-05 time: 871.0710310935974\n",
      "209: accuracy:0.5329166666666667 loss: 0.0004901917445028388 time: 875.140053987503\n",
      "210: accuracy:0.5333333333333333 loss: 2.7852002276479437e-05 time: 879.2627260684967\n",
      "211: accuracy:0.5375 loss: 0.02955060640756856 time: 883.3182001113892\n",
      "212: accuracy:0.5325 loss: 0.0004897716577101988 time: 887.4345610141754\n",
      "213: accuracy:0.5266666666666666 loss: 0.13052275345282494 time: 891.5544838905334\n",
      "214: accuracy:0.53625 loss: 0.0027314240600619915 time: 895.6745059490204\n",
      "215: accuracy:0.5358333333333334 loss: 0.00016214821318762986 time: 899.9234569072723\n",
      "216: accuracy:0.5341666666666667 loss: 0.00016163343106925798 time: 904.1719110012054\n",
      "217: accuracy:0.5291666666666667 loss: 0.00011294150506835528 time: 908.2642140388489\n",
      "218: accuracy:0.5258333333333334 loss: 0.0016786514387385001 time: 912.4445950984955\n",
      "219: accuracy:0.5295833333333333 loss: 0.002804208504317149 time: 916.4974989891052\n",
      "220: accuracy:0.525 loss: 0.0002378085926245596 time: 920.7575440406799\n",
      "221: accuracy:0.5329166666666667 loss: 0.0005412983774732913 time: 925.0123600959778\n",
      "222: accuracy:0.5245833333333333 loss: 0.0007981317462751802 time: 929.0525200366974\n",
      "223: accuracy:0.53 loss: 0.00019497922947550594 time: 933.1725211143494\n",
      "224: accuracy:0.5329166666666667 loss: 5.8003934366853034e-05 time: 937.2253150939941\n",
      "225: accuracy:0.5325 loss: 0.00012199580505822122 time: 941.3628830909729\n",
      "226: accuracy:0.5320833333333334 loss: 0.00012066366726276936 time: 945.5045049190521\n",
      "227: accuracy:0.5145833333333333 loss: 0.0009208331807945471 time: 949.4884650707245\n",
      "228: accuracy:0.5325 loss: 0.00010585279436462619 time: 953.8731508255005\n",
      "229: accuracy:0.5229166666666667 loss: 0.0062516306839680725 time: 958.8895819187164\n",
      "230: accuracy:0.5304166666666666 loss: 0.007939421980110476 time: 964.3452379703522\n",
      "231: accuracy:0.5229166666666667 loss: 0.05622370993312813 time: 969.1745691299438\n",
      "232: accuracy:0.51125 loss: 0.07696455388475035 time: 973.8383510112762\n",
      "233: accuracy:0.4920833333333333 loss: 0.27997840620495024 time: 978.1746921539307\n",
      "234: accuracy:0.5108333333333334 loss: 0.09890364846640307 time: 983.1589570045471\n",
      "235: accuracy:0.4925 loss: 0.14913965073781305 time: 988.1218001842499\n",
      "236: accuracy:0.505 loss: 0.1369060502051789 time: 993.5867419242859\n",
      "237: accuracy:0.5079166666666667 loss: 0.000813429575167909 time: 998.0376691818237\n",
      "238: accuracy:0.4920833333333333 loss: 0.1275906626356476 time: 1003.0613732337952\n",
      "239: accuracy:0.5179166666666667 loss: 0.016895737688439418 time: 1008.2001719474792\n",
      "240: accuracy:0.5191666666666667 loss: 0.0020864512430852893 time: 1012.4746389389038\n",
      "241: accuracy:0.5266666666666666 loss: 0.00015127216705408588 time: 1016.6121551990509\n",
      "242: accuracy:0.5216666666666666 loss: 0.0003657256585015804 time: 1020.6874890327454\n",
      "243: accuracy:0.52 loss: 0.00026250631481801456 time: 1024.7305040359497\n",
      "244: accuracy:0.5183333333333333 loss: 6.089525731722313e-05 time: 1028.9168648719788\n",
      "245: accuracy:0.5204166666666666 loss: 0.00011451140371226964 time: 1033.0926041603088\n",
      "246: accuracy:0.525 loss: 0.0005694490858335864 time: 1037.128445148468\n",
      "247: accuracy:0.51625 loss: 0.0007891229751851858 time: 1041.4221169948578\n",
      "248: accuracy:0.5170833333333333 loss: 0.0005824600283803293 time: 1045.4981801509857\n",
      "249: accuracy:0.5258333333333334 loss: 4.3406322454201535e-05 time: 1049.6782400608063\n",
      "250: accuracy:0.52875 loss: 0.0001352760751573128 time: 1053.8359458446503\n",
      "251: accuracy:0.5175 loss: 0.00010917013323306912 time: 1057.872407913208\n",
      "252: accuracy:0.5245833333333333 loss: 8.530448028451292e-05 time: 1062.024996995926\n",
      "253: accuracy:0.52 loss: 0.0008909043325605118 time: 1066.0901720523834\n",
      "254: accuracy:0.5208333333333334 loss: 5.053131582927195e-05 time: 1070.2130601406097\n",
      "255: accuracy:0.51375 loss: 9.64108756375693e-05 time: 1074.4906558990479\n",
      "256: accuracy:0.5175 loss: 0.09661579897688673 time: 1078.5163371562958\n",
      "257: accuracy:0.5233333333333333 loss: 8.843239725261909e-05 time: 1082.722382068634\n",
      "258: accuracy:0.52875 loss: 0.0005015213732097947 time: 1086.8095028400421\n",
      "259: accuracy:0.5091666666666667 loss: 0.0006251545442987998 time: 1091.0068991184235\n",
      "260: accuracy:0.5175 loss: 0.013194989658482401 time: 1095.2195661067963\n",
      "261: accuracy:0.5120833333333333 loss: 8.40599677323986e-05 time: 1099.271171092987\n",
      "262: accuracy:0.515 loss: 0.0023451799797577916 time: 1103.5698790550232\n",
      "263: accuracy:0.5195833333333333 loss: 0.00018087698054308245 time: 1107.4931571483612\n",
      "264: accuracy:0.5104166666666666 loss: 0.0031894862057876075 time: 1111.788913011551\n",
      "265: accuracy:0.5075 loss: 4.511963841787904e-05 time: 1116.505928993225\n",
      "266: accuracy:0.5254166666666666 loss: 7.058419137283278e-05 time: 1121.4574809074402\n",
      "267: accuracy:0.50375 loss: 0.0019326582288252195 time: 1126.7071959972382\n",
      "268: accuracy:0.5083333333333333 loss: 0.051893835918237013 time: 1131.3109250068665\n",
      "269: accuracy:0.5195833333333333 loss: 0.0014259215917221072 time: 1135.7762658596039\n",
      "270: accuracy:0.5175 loss: 0.00020591511065522206 time: 1141.0694489479065\n",
      "271: accuracy:0.5154166666666666 loss: 8.184329263354745e-05 time: 1146.4547259807587\n",
      "272: accuracy:0.51875 loss: 0.000939620103857247 time: 1150.844780921936\n",
      "273: accuracy:0.52 loss: 0.00010634368154953389 time: 1154.9104940891266\n",
      "274: accuracy:0.5158333333333334 loss: 0.00020647978874968435 time: 1158.9161100387573\n",
      "275: accuracy:0.5191666666666667 loss: 0.0003252379329650272 time: 1162.9119448661804\n",
      "276: accuracy:0.50625 loss: 0.00024540561615863255 time: 1166.9840359687805\n",
      "277: accuracy:0.5179166666666667 loss: 0.001839755778723473 time: 1171.2345299720764\n",
      "278: accuracy:0.5108333333333334 loss: 0.00025621524889391477 time: 1175.3908970355988\n",
      "279: accuracy:0.5075 loss: 0.011275651232643125 time: 1179.6013460159302\n",
      "280: accuracy:0.50625 loss: 0.006924494131284483 time: 1183.7647640705109\n",
      "281: accuracy:0.5170833333333333 loss: 0.0030666557934842416 time: 1188.0190210342407\n",
      "282: accuracy:0.5066666666666667 loss: 0.2630818933325242 time: 1192.2287662029266\n",
      "283: accuracy:0.4970833333333333 loss: 0.22888509042740057 time: 1196.4207751750946\n",
      "284: accuracy:0.505 loss: 0.38740267307777443 time: 1200.5345890522003\n",
      "285: accuracy:0.50625 loss: 0.12074239865763707 time: 1204.6462790966034\n",
      "286: accuracy:0.5120833333333333 loss: 0.04545980211388092 time: 1208.8081629276276\n",
      "287: accuracy:0.5158333333333334 loss: 0.02342093056844996 time: 1213.0713200569153\n",
      "288: accuracy:0.5170833333333333 loss: 0.05200357895558082 time: 1217.232104063034\n",
      "289: accuracy:0.515 loss: 0.0034242320298214744 time: 1221.4420568943024\n",
      "290: accuracy:0.52375 loss: 0.010066410106437669 time: 1225.5854151248932\n",
      "291: accuracy:0.52875 loss: 0.00027418621625670096 time: 1229.6637349128723\n",
      "292: accuracy:0.5233333333333333 loss: 0.00022481908752768932 time: 1233.7223179340363\n",
      "293: accuracy:0.53125 loss: 2.2257625812163728e-05 time: 1237.712455034256\n",
      "294: accuracy:0.5258333333333334 loss: 8.600659536360522e-06 time: 1241.8690600395203\n",
      "295: accuracy:0.53125 loss: 2.9876089847155618e-05 time: 1246.0674211978912\n",
      "296: accuracy:0.5208333333333334 loss: 0.0002287835465127324 time: 1250.3319342136383\n",
      "297: accuracy:0.5229166666666667 loss: 0.00041785497664761397 time: 1254.5388011932373\n",
      "298: accuracy:0.5258333333333334 loss: 6.583300132973272e-05 time: 1258.5847961902618\n",
      "299: accuracy:0.5320833333333334 loss: 0.02192483176074021 time: 1262.8139791488647\n",
      "300: accuracy:0.5283333333333333 loss: 7.251276255165057e-05 time: 1266.8459990024567\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    ")\n",
    "net = CNN()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "start = time.time()\n",
    "for epoch in range(1,301): \n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        pred = net(batch_x)\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 2: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_e)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss} time: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
