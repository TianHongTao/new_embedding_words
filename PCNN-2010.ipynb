{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function set_backend in module tensorly:\n",
      "\n",
      "set_backend(backend_name)\n",
      "    Sets the backend for TensorLy\n",
      "    \n",
      "        The backend will be set as specified and operations will used that backend\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    backend_name : {'mxnet', 'numpy', 'pytorch', 'tensorflow', 'cupy'}, default is 'numpy'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pytorch backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.decomposition import tucker\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import preprocessing\n",
    "from random import shuffle\n",
    "BATCH_SIZE = 128\n",
    "tl.set_backend('pytorch')\n",
    "help(tl.set_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 85, 200), (8000,), (8000, 1), (8000, 1), (8000, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.fromfile('tensors_2010.dat',dtype=np.float64).reshape((8000,-1,200))\n",
    "Y = np.fromfile('tensors_2010_labels.dat',dtype=np.int)\n",
    "position_tag_1 = np.fromfile('tensors_2010_entity1.dat',dtype=np.int).reshape((-1,1))\n",
    "position_tag_2 = np.fromfile('tensors_2010_entity2.dat',dtype=np.int).reshape((-1,1))\n",
    "position_tag = np.concatenate((position_tag_1,position_tag_2),axis =1)\n",
    "b.shape,Y.shape,position_tag_1.shape,position_tag_2.shape,position_tag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_tag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 85, 202)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [i for i in range(85)]\n",
    "c = []\n",
    "for i in range(8000):\n",
    "    position_a = (np.array(temp) - position_tag[i][0]).reshape((-1,1))\n",
    "    position_b = (np.array(temp) - position_tag[i][1]).reshape((-1,1))\n",
    "    position_tag_now = np.concatenate((position_a,position_b),axis = 1)\n",
    "    c.append(np.concatenate((b[i],position_tag_now),axis = 1))\n",
    "c = np.stack(c)\n",
    "b = c\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for i in range(8000):\n",
    "    inputs.append((torch.from_numpy(b[i]),Y[i],position_tag[i].astype(np.int64)))\n",
    "shuffle(inputs)\n",
    "flag = int(len(inputs)*0.7)\n",
    "train = inputs[:flag]\n",
    "test = inputs[flag:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2400, 85, 202]), torch.Size([2400]), torch.Size([2400, 2]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_e = []\n",
    "t_y = []\n",
    "t_p = []\n",
    "\n",
    "for E,Y,pos in test:\n",
    "    t_e.append(E)\n",
    "    t_y.append(Y)\n",
    "    t_p.append(pos)\n",
    "t_e = torch.stack(t_e)\n",
    "t_p = torch.from_numpy(np.stack(t_p))\n",
    "t_y = torch.from_numpy(np.array(t_y))\n",
    "t_e.shape, t_y.shape, t_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.6608333333333334 3轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(  # input shape (1, 53, 200)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,      # input height\n",
    "                out_channels=85,    # n_filters\n",
    "                kernel_size=(1,202),      # filter size\n",
    "            ),      # output shape (16, 53, 200)\n",
    "            nn.ReLU(),    # activation\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(85,1))   # 在 2x2 空间里向下采样, output shape (16, 53, 1)\n",
    "        self.output = nn.Linear(255, 10)   # fully connected layer, output 10 classes\n",
    "\n",
    "    def forward(self, x, pos):\n",
    "        torch.set_default_dtype(torch.double)\n",
    "        x = x.reshape((-1,1,85,202)).double()\n",
    "        x = self.conv1(x.double())\n",
    "        \n",
    "        x1 = torch.zeros(x.shape)\n",
    "        x2 = torch.zeros(x.shape)\n",
    "        x3 = torch.zeros(x.shape)\n",
    "        \n",
    "        min_x1 = torch.ones(x.shape) * (-1000)\n",
    "        min_x2 = torch.ones(x.shape) * (-1000)\n",
    "        min_x3 = torch.ones(x.shape) * (-1000)\n",
    "        \n",
    "        for i in range(pos.shape[0]):\n",
    "            x1[i, :, :pos[i][0], :]\n",
    "            min_x1[i, :, :pos[i][0], :]  = 0.0\n",
    "            x2[i, :, pos[i][0]:pos[i][1], :]  = 1.0\n",
    "            min_x2[i, :, pos[i][0]:pos[i][1], :] = 0.0\n",
    "            x3[i, :, pos[i][1]:, :] = 1.0\n",
    "            min_x3[i, :, pos[i][1]:, :] = 0.0\n",
    "            \n",
    "         # 卷积结果*mask得到分段后的卷积向量\n",
    "        x1 *= x\n",
    "        x2 *= x\n",
    "        x3 *= x\n",
    "        \n",
    "        # 将无用部分赋予最小值，避免影响maxpool操作\n",
    "        x1 += min_x1\n",
    "        x2 += min_x2\n",
    "        x3 += min_x3\n",
    "        \n",
    "        # 分段池化\n",
    "        x1 = self.maxpool(x1)\n",
    "        x2 = self.maxpool(x2)\n",
    "        x3 = self.maxpool(x3)\n",
    "        \n",
    "        # 展平\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        x3 = x3.view(x3.size(0), -1)\n",
    "        \n",
    "        x = torch.cat((x1,x2,x3),dim=1)\n",
    "\n",
    "        output = self.output(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 85, kernel_size=(1, 202), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=(85, 1), stride=(85, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (output): Linear(in_features=255, out_features=10, bias=True)\n",
      ")\n",
      "1: accuracy:0.17041666666666666 loss: 38.51878670936104 time: 2.446072816848755\n",
      "2: accuracy:0.6104166666666667 loss: 6.373206883226785 time: 9.893219709396362\n",
      "3: accuracy:0.6608333333333334 loss: 46.95810607836431 time: 17.177539825439453\n",
      "4: accuracy:0.625 loss: 15.07727171985417 time: 24.447836875915527\n",
      "5: accuracy:0.64125 loss: 45.81822458598132 time: 31.73485279083252\n",
      "6: accuracy:0.6241666666666666 loss: 27.440087151773756 time: 38.885934829711914\n",
      "7: accuracy:0.6354166666666666 loss: 62.72835027941746 time: 46.34409689903259\n",
      "8: accuracy:0.6070833333333333 loss: 14.905748365720104 time: 54.02220797538757\n",
      "9: accuracy:0.6216666666666667 loss: 34.397942902604655 time: 61.19856572151184\n",
      "10: accuracy:0.63125 loss: 16.137729256458687 time: 69.13264679908752\n",
      "11: accuracy:0.63625 loss: 6.480225970682432 time: 76.58037686347961\n",
      "12: accuracy:0.615 loss: 15.588209878070884 time: 83.50297379493713\n",
      "13: accuracy:0.6370833333333333 loss: 25.58187362068191 time: 90.39916181564331\n",
      "14: accuracy:0.595 loss: 8.403829762139019 time: 97.31103491783142\n",
      "15: accuracy:0.60875 loss: 17.352446960933875 time: 104.16551280021667\n",
      "16: accuracy:0.6233333333333333 loss: 7.551677534059973 time: 111.0101068019867\n",
      "17: accuracy:0.6083333333333333 loss: 13.289824019631792 time: 117.89751291275024\n",
      "18: accuracy:0.6204166666666666 loss: 0.474592731179027 time: 124.77392196655273\n",
      "19: accuracy:0.62 loss: 26.68064635675259 time: 131.6990840435028\n",
      "20: accuracy:0.6225 loss: 6.292849381757098 time: 138.6180338859558\n",
      "21: accuracy:0.625 loss: 18.771443080237585 time: 145.47937989234924\n",
      "22: accuracy:0.61375 loss: 3.2746757279404894 time: 152.40659499168396\n",
      "23: accuracy:0.6141666666666666 loss: 1.4480170230380311 time: 159.3065037727356\n",
      "24: accuracy:0.6070833333333333 loss: 5.627529817873399 time: 166.18466472625732\n",
      "25: accuracy:0.6233333333333333 loss: 1.7553872480723955 time: 173.15568089485168\n",
      "26: accuracy:0.6054166666666667 loss: 1.402830588832641 time: 180.06903886795044\n",
      "27: accuracy:0.6316666666666667 loss: 2.3270592261854683 time: 187.0148847103119\n",
      "28: accuracy:0.6079166666666667 loss: 16.48781537257233 time: 193.9540889263153\n",
      "29: accuracy:0.59875 loss: 6.877064071983216 time: 200.90522289276123\n",
      "30: accuracy:0.6120833333333333 loss: 14.500672401923 time: 208.15754175186157\n",
      "31: accuracy:0.6120833333333333 loss: 12.132953287614974 time: 215.3604919910431\n",
      "32: accuracy:0.6016666666666667 loss: 10.881192567109151 time: 222.47099375724792\n",
      "33: accuracy:0.60625 loss: 12.663923640240975 time: 229.4410228729248\n",
      "34: accuracy:0.6154166666666666 loss: 6.232647131683605 time: 236.9167799949646\n",
      "35: accuracy:0.60625 loss: 2.065172235314484 time: 244.18546390533447\n",
      "36: accuracy:0.61 loss: 0.5263260823634037 time: 251.872407913208\n",
      "37: accuracy:0.62375 loss: 35.870065169554664 time: 259.52464389801025\n",
      "38: accuracy:0.6066666666666667 loss: 0.26555275358568353 time: 266.88998079299927\n",
      "39: accuracy:0.5941666666666666 loss: 9.169683359839892 time: 274.4213581085205\n",
      "40: accuracy:0.6229166666666667 loss: 0.5652129903409316 time: 282.04791498184204\n",
      "41: accuracy:0.6204166666666666 loss: 1.9146175025763916 time: 289.6357219219208\n",
      "42: accuracy:0.6129166666666667 loss: 1.5213942669094782 time: 297.09798192977905\n",
      "43: accuracy:0.62875 loss: 7.662887553345634 time: 304.82850790023804\n",
      "44: accuracy:0.6166666666666667 loss: 0.35240968532232747 time: 312.526752948761\n",
      "45: accuracy:0.6091666666666666 loss: 11.391703850109705 time: 320.03929686546326\n",
      "46: accuracy:0.6154166666666666 loss: 2.483730991975163 time: 327.4476227760315\n",
      "47: accuracy:0.6129166666666667 loss: 25.67289395675965 time: 334.96935987472534\n",
      "48: accuracy:0.6145833333333334 loss: 2.7569155222170467 time: 342.48033380508423\n",
      "49: accuracy:0.6225 loss: 0.06856228803179927 time: 349.94918394088745\n",
      "50: accuracy:0.6204166666666666 loss: 0.003931572690418512 time: 357.45257902145386\n",
      "51: accuracy:0.61875 loss: 6.671097802358588 time: 365.0229797363281\n",
      "52: accuracy:0.6158333333333333 loss: 3.5992653351739765 time: 372.5373990535736\n",
      "53: accuracy:0.6325 loss: 0.3017846422042637 time: 380.1088707447052\n",
      "54: accuracy:0.6133333333333333 loss: 0.4572391072377333 time: 387.6670117378235\n",
      "55: accuracy:0.6158333333333333 loss: 0.23665850924929505 time: 395.27104091644287\n",
      "56: accuracy:0.61875 loss: 11.805053127901258 time: 402.92424607276917\n",
      "57: accuracy:0.6216666666666667 loss: 3.9060651590740454 time: 410.5317487716675\n",
      "58: accuracy:0.615 loss: 28.31308354834448 time: 418.0685420036316\n",
      "59: accuracy:0.6258333333333334 loss: 5.335206582079977e-10 time: 425.76041293144226\n",
      "60: accuracy:0.5870833333333333 loss: 5.243579418051623 time: 433.418741941452\n",
      "61: accuracy:0.61375 loss: 0.18992158881506882 time: 441.14898204803467\n",
      "62: accuracy:0.6245833333333334 loss: 1.716963747063656 time: 449.0896677970886\n",
      "63: accuracy:0.6133333333333333 loss: 4.733244548557136 time: 456.56814098358154\n",
      "64: accuracy:0.6020833333333333 loss: 6.074103260783436 time: 464.0238678455353\n",
      "65: accuracy:0.6245833333333334 loss: 16.730656284331342 time: 472.1198101043701\n",
      "66: accuracy:0.6129166666666667 loss: 3.689324307248661 time: 479.68321776390076\n",
      "67: accuracy:0.6275 loss: 2.7549374169486427 time: 487.1185338497162\n",
      "68: accuracy:0.615 loss: 0.08040058822857955 time: 494.76749897003174\n",
      "69: accuracy:0.6216666666666667 loss: 0.10420005571096536 time: 502.4501898288727\n",
      "70: accuracy:0.61625 loss: 3.6977803425746756 time: 510.4472870826721\n",
      "71: accuracy:0.6154166666666666 loss: 14.000962427538036 time: 518.3245208263397\n",
      "72: accuracy:0.6208333333333333 loss: 0.22874340288062867 time: 525.9669368267059\n",
      "73: accuracy:0.6075 loss: 0.614870166556197 time: 533.7142820358276\n",
      "74: accuracy:0.6141666666666666 loss: 0.001014012215923829 time: 541.5020170211792\n",
      "75: accuracy:0.63125 loss: 13.41694321355574 time: 549.075131893158\n",
      "76: accuracy:0.6154166666666666 loss: 4.253312257177184 time: 556.4048638343811\n",
      "77: accuracy:0.6216666666666667 loss: 0.08051664135058376 time: 564.0012700557709\n",
      "78: accuracy:0.6220833333333333 loss: 0.45792813000748434 time: 571.5906910896301\n",
      "79: accuracy:0.6216666666666667 loss: 0.09544312700045973 time: 579.3109078407288\n",
      "80: accuracy:0.6225 loss: 2.2225332685366084e-11 time: 587.0880799293518\n",
      "81: accuracy:0.6216666666666667 loss: 0.0009027474858996354 time: 594.4521968364716\n",
      "82: accuracy:0.62125 loss: 0.14113418433316777 time: 601.8329658508301\n",
      "83: accuracy:0.6191666666666666 loss: 4.570896523890464e-06 time: 609.5633149147034\n",
      "84: accuracy:0.6225 loss: 0.3230161259477182 time: 617.402382850647\n",
      "85: accuracy:0.61875 loss: 0.02448937952695107 time: 624.8180878162384\n",
      "86: accuracy:0.6183333333333333 loss: 2.07151376398357 time: 632.5063769817352\n",
      "87: accuracy:0.6120833333333333 loss: 0.0015070868823920947 time: 640.117280960083\n",
      "88: accuracy:0.6220833333333333 loss: 18.21654711967752 time: 647.60613489151\n",
      "89: accuracy:0.6304166666666666 loss: 6.475116830404143 time: 654.8526449203491\n",
      "90: accuracy:0.6225 loss: 0.0617265586836295 time: 662.1824278831482\n",
      "91: accuracy:0.6179166666666667 loss: 7.812565348471005 time: 669.7438929080963\n",
      "92: accuracy:0.6175 loss: 0.06014806827434178 time: 676.970871925354\n",
      "93: accuracy:0.6204166666666666 loss: 19.73318856688192 time: 684.5256209373474\n",
      "94: accuracy:0.6308333333333334 loss: 0.46015751408625855 time: 692.2277460098267\n",
      "95: accuracy:0.6054166666666667 loss: 0.5026075803308516 time: 700.1385779380798\n",
      "96: accuracy:0.6154166666666666 loss: 0.2349415342090248 time: 707.3585159778595\n",
      "97: accuracy:0.6191666666666666 loss: 0.09276483738423752 time: 714.7294657230377\n",
      "98: accuracy:0.6154166666666666 loss: 0.01877047567448905 time: 722.421618938446\n",
      "99: accuracy:0.6245833333333334 loss: 0.009850046893115659 time: 730.1847269535065\n",
      "100: accuracy:0.6175 loss: 0.22726202688315514 time: 737.7588078975677\n",
      "101: accuracy:0.6229166666666667 loss: 0.7014309710063675 time: 745.2673070430756\n",
      "102: accuracy:0.6104166666666667 loss: 0.9289453354532231 time: 752.787811756134\n",
      "103: accuracy:0.6195833333333334 loss: 2.9870495017790244 time: 761.0796899795532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104: accuracy:0.6191666666666666 loss: 3.2954997007950126e-06 time: 769.0078718662262\n",
      "105: accuracy:0.6208333333333333 loss: 1.0200174038743626e-14 time: 776.6239819526672\n",
      "106: accuracy:0.61875 loss: 24.1119405228546 time: 784.6740329265594\n",
      "107: accuracy:0.6241666666666666 loss: 2.5942655046407026e-07 time: 792.5353949069977\n",
      "108: accuracy:0.6125 loss: 0.48848585291950125 time: 800.0485377311707\n",
      "109: accuracy:0.6008333333333333 loss: 31.945406165961643 time: 807.8726189136505\n",
      "110: accuracy:0.6058333333333333 loss: 3.8396946360982205 time: 815.1402349472046\n",
      "111: accuracy:0.6154166666666666 loss: 1.146333685379958 time: 822.5600459575653\n",
      "112: accuracy:0.61375 loss: 0.4681683272732505 time: 830.1075267791748\n",
      "113: accuracy:0.6216666666666667 loss: 0.10944453089896644 time: 837.3411557674408\n",
      "114: accuracy:0.61625 loss: 0.00024917584387509906 time: 844.7750589847565\n",
      "115: accuracy:0.6129166666666667 loss: 0.16875865218413644 time: 852.3665940761566\n",
      "116: accuracy:0.6216666666666667 loss: 0.008609821774217097 time: 860.0624568462372\n",
      "117: accuracy:0.6270833333333333 loss: 0.14411999301902817 time: 867.681834936142\n",
      "118: accuracy:0.6316666666666667 loss: 0.7068144537730798 time: 875.1270668506622\n",
      "119: accuracy:0.61875 loss: 0.0 time: 882.7284069061279\n",
      "120: accuracy:0.6179166666666667 loss: 4.940492459581947e-13 time: 891.0244858264923\n",
      "121: accuracy:0.61375 loss: 0.0014959399974647791 time: 898.9858486652374\n",
      "122: accuracy:0.6129166666666667 loss: 0.0014020301994874251 time: 906.6021659374237\n",
      "123: accuracy:0.6208333333333333 loss: 1.1102230246251565e-16 time: 914.07155585289\n",
      "124: accuracy:0.6291666666666667 loss: 1.6616857335893798e-05 time: 921.6107409000397\n",
      "125: accuracy:0.6275 loss: 0.0 time: 929.2340049743652\n",
      "126: accuracy:0.62375 loss: 0.07429970387732909 time: 936.902125120163\n",
      "127: accuracy:0.6220833333333333 loss: 0.0006939020094251375 time: 944.4324278831482\n",
      "128: accuracy:0.6316666666666667 loss: 2.9631244125027933e-08 time: 952.2169280052185\n",
      "129: accuracy:0.6270833333333333 loss: 2.6301616440349562e-09 time: 959.8826439380646\n",
      "130: accuracy:0.615 loss: 0.005017944258711604 time: 967.6865599155426\n",
      "131: accuracy:0.61375 loss: 1.737543442459355e-11 time: 975.5963728427887\n",
      "132: accuracy:0.6170833333333333 loss: 6.557867637813075e-08 time: 983.8001887798309\n",
      "133: accuracy:0.6258333333333334 loss: 3.7773634595711942 time: 991.9712829589844\n",
      "134: accuracy:0.62875 loss: 0.0773394778372718 time: 999.8569419384003\n",
      "135: accuracy:0.6083333333333333 loss: 0.850343642077702 time: 1007.9767789840698\n",
      "136: accuracy:0.6208333333333333 loss: 4.335246745892549 time: 1015.6108689308167\n",
      "137: accuracy:0.6104166666666667 loss: 0.5896210904264187 time: 1023.5778579711914\n",
      "138: accuracy:0.6166666666666667 loss: 0.027150332502905883 time: 1031.3725707530975\n",
      "139: accuracy:0.625 loss: 7.447971336945274 time: 1038.9712839126587\n",
      "140: accuracy:0.6175 loss: 2.306520351637255 time: 1046.8180468082428\n",
      "141: accuracy:0.6229166666666667 loss: 0.10532599602027826 time: 1054.5321748256683\n",
      "142: accuracy:0.6179166666666667 loss: 0.021640431154402062 time: 1062.3407638072968\n",
      "143: accuracy:0.6270833333333333 loss: 0.00012185400804919089 time: 1070.0522718429565\n",
      "144: accuracy:0.6154166666666666 loss: 6.083045631823937 time: 1078.0898728370667\n"
     ]
    }
   ],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    ")\n",
    "net = CNN()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "start = time.time()\n",
    "for epoch in range(1,201): \n",
    "    for step, (batch_x, batch_y,pos) in enumerate(loader):\n",
    "        pred = net(batch_x,pos)\n",
    "        loss = loss_F(pred,batch_y) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 2: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_e,t_p)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss} time: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
