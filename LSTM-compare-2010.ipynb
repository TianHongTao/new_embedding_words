{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function set_backend in module tensorly:\n",
      "\n",
      "set_backend(backend_name)\n",
      "    Sets the backend for TensorLy\n",
      "    \n",
      "        The backend will be set as specified and operations will used that backend\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    backend_name : {'mxnet', 'numpy', 'pytorch', 'tensorflow', 'cupy'}, default is 'numpy'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pytorch backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.decomposition import tucker\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import preprocessing\n",
    "from random import shuffle\n",
    "BATCH_SIZE = 128\n",
    "tl.set_backend('pytorch')\n",
    "help(tl.set_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 85, 200), (8000,), (8000,), (8000,), (8000, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.fromfile('tensors_2010.dat',dtype=np.float64).reshape((8000,-1,200))\n",
    "Y = np.fromfile('tensors_2010_labels.dat',dtype=np.int)\n",
    "position_tag_1 = np.fromfile('tensors_2010_entity1.dat',dtype=np.int)\n",
    "position_tag_2 = np.fromfile('tensors_2010_entity2.dat',dtype=np.int)\n",
    "position_tag = np.concatenate((position_tag_1,position_tag_2)).reshape((8000,-1))\n",
    "b.shape,Y.shape,position_tag_1.shape,position_tag_2.shape,position_tag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for i in range(8000):\n",
    "    inputs.append((torch.from_numpy(b[i]),Y[i]))\n",
    "shuffle(inputs)\n",
    "flag = int(len(inputs)*0.7)\n",
    "train = inputs[:flag]\n",
    "test = inputs[flag:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2400, 85, 200]), torch.Size([2400]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_e = []\n",
    "t_y = []\n",
    "for E,Y in test:\n",
    "    t_e.append(E)\n",
    "    t_y.append(Y)\n",
    "t_e = torch.stack(t_e)\n",
    "t_y = torch.from_numpy(np.array(t_y))\n",
    "t_e.shape, t_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glove_rnn_max: 0.6341666666666667 15轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN_max(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size = 200,       # rnn input\n",
    "            hidden_size = 64,     # rnn hidden unit\n",
    "            num_layers = 1,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=64,out_features=10)\n",
    "        self.maxpool = torch.nn.MaxPool2d((85,1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        output,(h_n,c_n)=self.rnn(x.float())\n",
    "        output_in_last_timestep = self.maxpool(output)\n",
    "        output_in_last_timestep = output_in_last_timestep.reshape((output_in_last_timestep.shape[0],-1))\n",
    "        x=self.out(output_in_last_timestep)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_max(\n",
      "  (rnn): LSTM(200, 64, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=(85, 1), stride=(85, 1), padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "1: accuracy:0.245 loss: 2.2047150135040283\n",
      "2: accuracy:0.5195833333333333 loss: 1.2479912042617798\n",
      "3: accuracy:0.5895833333333333 loss: 0.9295550584793091\n",
      "4: accuracy:0.62125 loss: 0.7006310224533081\n",
      "5: accuracy:0.62125 loss: 0.4652349054813385\n",
      "6: accuracy:0.63 loss: 0.44489288330078125\n",
      "7: accuracy:0.6033333333333334 loss: 0.23127657175064087\n",
      "8: accuracy:0.60875 loss: 0.255601167678833\n",
      "9: accuracy:0.6079166666666667 loss: 0.1630931794643402\n",
      "10: accuracy:0.6304166666666666 loss: 0.07207967340946198\n",
      "11: accuracy:0.615 loss: 0.060785531997680664\n",
      "12: accuracy:0.62375 loss: 0.03939204290509224\n",
      "13: accuracy:0.60875 loss: 0.07682640850543976\n",
      "14: accuracy:0.6058333333333333 loss: 0.056524571031332016\n",
      "15: accuracy:0.6341666666666667 loss: 0.022927533835172653\n",
      "16: accuracy:0.6191666666666666 loss: 0.01411542110145092\n",
      "17: accuracy:0.6170833333333333 loss: 0.008833406493067741\n",
      "18: accuracy:0.6225 loss: 0.005959661677479744\n",
      "19: accuracy:0.6204166666666666 loss: 0.0037894807755947113\n",
      "20: accuracy:0.61875 loss: 0.004162687808275223\n",
      "21: accuracy:0.6220833333333333 loss: 0.002983599901199341\n",
      "22: accuracy:0.6191666666666666 loss: 0.002750251442193985\n",
      "23: accuracy:0.6170833333333333 loss: 0.002330280840396881\n",
      "24: accuracy:0.6175 loss: 0.0019900165498256683\n",
      "25: accuracy:0.615 loss: 0.00668078288435936\n",
      "26: accuracy:0.6029166666666667 loss: 0.004673179239034653\n",
      "27: accuracy:0.6058333333333333 loss: 0.02873196266591549\n",
      "28: accuracy:0.6016666666666667 loss: 0.20490963757038116\n",
      "29: accuracy:0.5870833333333333 loss: 0.22703173756599426\n",
      "30: accuracy:0.5970833333333333 loss: 0.10084551572799683\n",
      "31: accuracy:0.6091666666666666 loss: 0.03768739104270935\n",
      "32: accuracy:0.5966666666666667 loss: 0.018297186121344566\n",
      "33: accuracy:0.6070833333333333 loss: 0.009981894865632057\n",
      "34: accuracy:0.6075 loss: 0.004765139892697334\n",
      "35: accuracy:0.6091666666666666 loss: 0.003725750371813774\n",
      "36: accuracy:0.6095833333333334 loss: 0.003242937847971916\n",
      "37: accuracy:0.605 loss: 0.002693258225917816\n",
      "38: accuracy:0.6083333333333333 loss: 0.0022726505994796753\n",
      "39: accuracy:0.6091666666666666 loss: 0.0020871851593255997\n",
      "40: accuracy:0.60625 loss: 0.01111089438199997\n",
      "41: accuracy:0.60375 loss: 0.0017491132020950317\n",
      "42: accuracy:0.6104166666666667 loss: 0.0014654360711574554\n",
      "43: accuracy:0.60875 loss: 0.0015374664217233658\n",
      "44: accuracy:0.6045833333333334 loss: 0.0012744385749101639\n",
      "45: accuracy:0.6054166666666667 loss: 0.0011779628694057465\n",
      "46: accuracy:0.6104166666666667 loss: 0.0010787062346935272\n",
      "47: accuracy:0.60625 loss: 0.0009847506880760193\n",
      "48: accuracy:0.6079166666666667 loss: 0.0010943785309791565\n",
      "49: accuracy:0.60875 loss: 0.0010288320481777191\n",
      "50: accuracy:0.6116666666666667 loss: 0.001026717945933342\n",
      "51: accuracy:0.60875 loss: 0.000953037291765213\n",
      "52: accuracy:0.605 loss: 0.0006111450493335724\n",
      "53: accuracy:0.61125 loss: 0.0009383894503116608\n",
      "54: accuracy:0.60625 loss: 0.0006337389349937439\n",
      "55: accuracy:0.6079166666666667 loss: 0.0008094944059848785\n",
      "56: accuracy:0.60625 loss: 0.0006209835410118103\n",
      "57: accuracy:0.6079166666666667 loss: 0.006845191121101379\n",
      "58: accuracy:0.6075 loss: 0.006554692983627319\n",
      "59: accuracy:0.6129166666666667 loss: 0.0005396585911512375\n",
      "60: accuracy:0.605 loss: 0.0005204081535339355\n",
      "61: accuracy:0.6054166666666667 loss: 0.0005030892789363861\n",
      "62: accuracy:0.6079166666666667 loss: 0.0005219951272010803\n",
      "63: accuracy:0.6075 loss: 0.005814459174871445\n",
      "64: accuracy:0.6070833333333333 loss: 0.0004298463463783264\n",
      "65: accuracy:0.6045833333333334 loss: 0.0003512240946292877\n",
      "66: accuracy:0.6075 loss: 0.00035539641976356506\n",
      "67: accuracy:0.61125 loss: 0.00042759254574775696\n",
      "68: accuracy:0.6079166666666667 loss: 0.0002962052822113037\n",
      "69: accuracy:0.6120833333333333 loss: 0.0003596767783164978\n",
      "70: accuracy:0.6054166666666667 loss: 0.0002779960632324219\n",
      "71: accuracy:0.6020833333333333 loss: 0.00045582279562950134\n",
      "72: accuracy:0.6041666666666666 loss: 0.00032073259353637695\n",
      "73: accuracy:0.6033333333333334 loss: 0.0003119334578514099\n",
      "74: accuracy:0.6033333333333334 loss: 0.00026869773864746094\n",
      "75: accuracy:0.60375 loss: 0.00034369900822639465\n",
      "76: accuracy:0.6029166666666667 loss: 0.0003016814589500427\n",
      "77: accuracy:0.6033333333333334 loss: 0.0003286190330982208\n",
      "78: accuracy:0.605 loss: 0.0002510957419872284\n",
      "79: accuracy:0.6020833333333333 loss: 0.0002514459192752838\n",
      "80: accuracy:0.60625 loss: 0.0002521313726902008\n",
      "81: accuracy:0.6045833333333334 loss: 0.00024336948990821838\n",
      "82: accuracy:0.6029166666666667 loss: 0.00019401684403419495\n",
      "83: accuracy:0.6 loss: 0.015920128673315048\n",
      "84: accuracy:0.57875 loss: 0.48135486245155334\n",
      "85: accuracy:0.5720833333333334 loss: 0.4062497615814209\n",
      "86: accuracy:0.59375 loss: 0.19404087960720062\n",
      "87: accuracy:0.5883333333333334 loss: 0.08377969264984131\n",
      "88: accuracy:0.5866666666666667 loss: 0.028197789564728737\n",
      "89: accuracy:0.60125 loss: 0.010927893221378326\n",
      "90: accuracy:0.6 loss: 0.009001040831208229\n",
      "91: accuracy:0.6 loss: 0.006301946938037872\n",
      "92: accuracy:0.6029166666666667 loss: 0.005740096792578697\n",
      "93: accuracy:0.6041666666666666 loss: 0.005118215456604958\n",
      "94: accuracy:0.6033333333333334 loss: 0.004318563267588615\n",
      "95: accuracy:0.6041666666666666 loss: 0.004714589565992355\n",
      "96: accuracy:0.6041666666666666 loss: 0.0032241996377706528\n",
      "97: accuracy:0.60375 loss: 0.003398081287741661\n",
      "98: accuracy:0.605 loss: 0.003323836252093315\n",
      "99: accuracy:0.6033333333333334 loss: 0.002676408737897873\n",
      "100: accuracy:0.6025 loss: 0.013905942440032959\n",
      "101: accuracy:0.60125 loss: 0.002212553285062313\n",
      "102: accuracy:0.6033333333333334 loss: 0.0020327959209680557\n",
      "103: accuracy:0.6016666666666667 loss: 0.0020150430500507355\n",
      "104: accuracy:0.6008333333333333 loss: 0.0021803490817546844\n",
      "105: accuracy:0.6025 loss: 0.0071731023490428925\n",
      "106: accuracy:0.6 loss: 0.001803496852517128\n",
      "107: accuracy:0.5995833333333334 loss: 0.0014637112617492676\n",
      "108: accuracy:0.6016666666666667 loss: 0.0069001540541648865\n",
      "109: accuracy:0.60125 loss: 0.0016552768647670746\n",
      "110: accuracy:0.60375 loss: 0.001384727656841278\n",
      "111: accuracy:0.6004166666666667 loss: 0.0012911893427371979\n",
      "112: accuracy:0.6058333333333333 loss: 0.0011824164539575577\n",
      "113: accuracy:0.60375 loss: 0.0012107212096452713\n",
      "114: accuracy:0.6054166666666667 loss: 0.00148879736661911\n",
      "115: accuracy:0.6004166666666667 loss: 0.0011402219533920288\n",
      "116: accuracy:0.6008333333333333 loss: 0.005113724619150162\n",
      "117: accuracy:0.6054166666666667 loss: 0.000733228400349617\n",
      "118: accuracy:0.6020833333333333 loss: 0.0009856577962636948\n",
      "119: accuracy:0.6079166666666667 loss: 0.0007582642138004303\n",
      "120: accuracy:0.60375 loss: 0.001102125272154808\n",
      "121: accuracy:0.6020833333333333 loss: 0.0008109975606203079\n",
      "122: accuracy:0.6033333333333334 loss: 0.0008531492203474045\n",
      "123: accuracy:0.60375 loss: 0.0006937608122825623\n",
      "124: accuracy:0.6045833333333334 loss: 0.0006429683417081833\n",
      "125: accuracy:0.6045833333333334 loss: 0.0008818432688713074\n",
      "126: accuracy:0.6033333333333334 loss: 0.0006356947124004364\n",
      "127: accuracy:0.6041666666666666 loss: 0.000696275383234024\n",
      "128: accuracy:0.6025 loss: 0.0008060596883296967\n",
      "129: accuracy:0.60625 loss: 0.0005959458649158478\n",
      "130: accuracy:0.6041666666666666 loss: 0.0006671734154224396\n",
      "131: accuracy:0.6029166666666667 loss: 0.0006649717688560486\n",
      "132: accuracy:0.6045833333333334 loss: 0.0005363635718822479\n",
      "133: accuracy:0.6041666666666666 loss: 0.004230041056871414\n",
      "134: accuracy:0.5854166666666667 loss: 0.05729284882545471\n",
      "135: accuracy:0.54125 loss: 0.30901142954826355\n",
      "136: accuracy:0.5783333333333334 loss: 0.2896345257759094\n",
      "137: accuracy:0.58 loss: 0.12486997991800308\n",
      "138: accuracy:0.5954166666666667 loss: 0.05352218076586723\n",
      "139: accuracy:0.6033333333333334 loss: 0.022372577339410782\n",
      "140: accuracy:0.59875 loss: 0.010784657672047615\n",
      "141: accuracy:0.60125 loss: 0.006450170651078224\n",
      "142: accuracy:0.60375 loss: 0.004499748349189758\n",
      "143: accuracy:0.6029166666666667 loss: 0.004478629678487778\n",
      "144: accuracy:0.6041666666666666 loss: 0.0030186492949724197\n",
      "145: accuracy:0.60625 loss: 0.003314012661576271\n",
      "146: accuracy:0.6045833333333334 loss: 0.0032229674980044365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147: accuracy:0.6045833333333334 loss: 0.002690548077225685\n",
      "148: accuracy:0.6020833333333333 loss: 0.00649191252887249\n",
      "149: accuracy:0.60375 loss: 0.0020469296723604202\n",
      "150: accuracy:0.6004166666666667 loss: 0.0021061208099126816\n",
      "151: accuracy:0.6016666666666667 loss: 0.009070277214050293\n",
      "152: accuracy:0.6058333333333333 loss: 0.0018863584846258163\n",
      "153: accuracy:0.60125 loss: 0.0017761960625648499\n",
      "154: accuracy:0.6041666666666666 loss: 0.0068917348980903625\n",
      "155: accuracy:0.60375 loss: 0.0017234813421964645\n",
      "156: accuracy:0.6025 loss: 0.0015098769217729568\n",
      "157: accuracy:0.6045833333333334 loss: 0.0013207755982875824\n",
      "158: accuracy:0.60375 loss: 0.00150214321911335\n",
      "159: accuracy:0.6045833333333334 loss: 0.0013091228902339935\n",
      "160: accuracy:0.6045833333333334 loss: 0.0011624302715063095\n",
      "161: accuracy:0.605 loss: 0.0047631217166781425\n",
      "162: accuracy:0.6033333333333334 loss: 0.0010217130184173584\n",
      "163: accuracy:0.6045833333333334 loss: 0.0009601395577192307\n",
      "164: accuracy:0.60375 loss: 0.0010078754276037216\n",
      "165: accuracy:0.6058333333333333 loss: 0.006371831521391869\n",
      "166: accuracy:0.6045833333333334 loss: 0.0007427781820297241\n",
      "167: accuracy:0.6045833333333334 loss: 0.0009944792836904526\n",
      "168: accuracy:0.605 loss: 0.0008224491029977798\n",
      "169: accuracy:0.605 loss: 0.0009021442383527756\n",
      "170: accuracy:0.6045833333333334 loss: 0.00067116878926754\n",
      "171: accuracy:0.6079166666666667 loss: 0.0008995719254016876\n",
      "172: accuracy:0.6054166666666667 loss: 0.0007862299680709839\n",
      "173: accuracy:0.6054166666666667 loss: 0.0007615368813276291\n",
      "174: accuracy:0.6054166666666667 loss: 0.000721486285328865\n",
      "175: accuracy:0.605 loss: 0.0006278492510318756\n",
      "176: accuracy:0.6095833333333334 loss: 0.0006136801093816757\n",
      "177: accuracy:0.6054166666666667 loss: 0.0008192677050828934\n",
      "178: accuracy:0.6045833333333334 loss: 0.0004696361720561981\n",
      "179: accuracy:0.6066666666666667 loss: 0.0004689022898674011\n",
      "180: accuracy:0.6075 loss: 0.0004787854850292206\n",
      "181: accuracy:0.6058333333333333 loss: 0.00045711174607276917\n",
      "182: accuracy:0.60375 loss: 0.0004982408136129379\n",
      "183: accuracy:0.6070833333333333 loss: 0.0004385504871606827\n",
      "184: accuracy:0.6075 loss: 0.00047713518142700195\n",
      "185: accuracy:0.60625 loss: 0.005424899980425835\n",
      "186: accuracy:0.6054166666666667 loss: 0.0004122164100408554\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    ")\n",
    "net = RNN_max()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "for epoch in range(1,301): \n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        pred = net(batch_x)\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 2: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_e)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glove:0.44680851063829785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN_old(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size = 200,       # rnn input\n",
    "            hidden_size = 64,     # rnn hidden unit\n",
    "            num_layers = 1,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=64,out_features=10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output,(h_n,c_n)=self.rnn(x.float())\n",
    "        output_in_last_timestep=h_n[-1,:,:]\n",
    "        x=self.out(output_in_last_timestep)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_old(\n",
      "  (rnn): LSTM(200, 64, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "1: accuracy:0.19858156028368795 loss: 2.2743892669677734\n",
      "2: accuracy:0.1702127659574468 loss: 2.282547950744629\n",
      "3: accuracy:0.1702127659574468 loss: 2.2899293899536133\n",
      "4: accuracy:0.19858156028368795 loss: 2.2691707611083984\n",
      "5: accuracy:0.19858156028368795 loss: 2.262784719467163\n",
      "6: accuracy:0.19858156028368795 loss: 2.256842851638794\n",
      "7: accuracy:0.19858156028368795 loss: 2.2975029945373535\n",
      "8: accuracy:0.19858156028368795 loss: 2.277923345565796\n",
      "9: accuracy:0.19858156028368795 loss: 2.2180051803588867\n",
      "10: accuracy:0.19858156028368795 loss: 2.230205535888672\n",
      "11: accuracy:0.1702127659574468 loss: 2.2795324325561523\n",
      "12: accuracy:0.1702127659574468 loss: 2.2521166801452637\n",
      "13: accuracy:0.1702127659574468 loss: 2.2433652877807617\n",
      "14: accuracy:0.1702127659574468 loss: 2.273130416870117\n",
      "15: accuracy:0.19858156028368795 loss: 2.2797694206237793\n",
      "16: accuracy:0.19858156028368795 loss: 2.3069822788238525\n",
      "17: accuracy:0.19858156028368795 loss: 2.2461001873016357\n",
      "18: accuracy:0.19858156028368795 loss: 2.2546920776367188\n",
      "19: accuracy:0.19858156028368795 loss: 2.2980313301086426\n",
      "20: accuracy:0.19858156028368795 loss: 2.2284762859344482\n",
      "21: accuracy:0.1702127659574468 loss: 2.2560372352600098\n",
      "22: accuracy:0.1702127659574468 loss: 2.2642743587493896\n",
      "23: accuracy:0.1702127659574468 loss: 2.225023031234741\n",
      "24: accuracy:0.1702127659574468 loss: 2.2716760635375977\n",
      "25: accuracy:0.1702127659574468 loss: 2.3097128868103027\n",
      "26: accuracy:0.1702127659574468 loss: 2.242703914642334\n",
      "27: accuracy:0.1702127659574468 loss: 2.325218677520752\n",
      "28: accuracy:0.1702127659574468 loss: 2.2664854526519775\n",
      "29: accuracy:0.1702127659574468 loss: 2.244119882583618\n",
      "30: accuracy:0.1702127659574468 loss: 2.2473907470703125\n",
      "31: accuracy:0.1702127659574468 loss: 2.269350528717041\n",
      "32: accuracy:0.1702127659574468 loss: 2.225036859512329\n",
      "33: accuracy:0.1702127659574468 loss: 2.2701823711395264\n",
      "34: accuracy:0.1702127659574468 loss: 2.257671356201172\n",
      "35: accuracy:0.1702127659574468 loss: 2.275029182434082\n",
      "36: accuracy:0.1702127659574468 loss: 2.2215960025787354\n",
      "37: accuracy:0.19858156028368795 loss: 2.2276222705841064\n",
      "38: accuracy:0.19858156028368795 loss: 2.2639102935791016\n",
      "39: accuracy:0.19858156028368795 loss: 2.3076412677764893\n",
      "40: accuracy:0.19858156028368795 loss: 2.2420437335968018\n",
      "41: accuracy:0.19858156028368795 loss: 2.300605058670044\n",
      "42: accuracy:0.19858156028368795 loss: 2.3077874183654785\n",
      "43: accuracy:0.19858156028368795 loss: 2.2768023014068604\n",
      "44: accuracy:0.1702127659574468 loss: 2.2935519218444824\n",
      "45: accuracy:0.1702127659574468 loss: 2.266929864883423\n",
      "46: accuracy:0.1702127659574468 loss: 2.2788808345794678\n",
      "47: accuracy:0.1702127659574468 loss: 2.254054069519043\n",
      "48: accuracy:0.1702127659574468 loss: 2.219813585281372\n",
      "49: accuracy:0.1702127659574468 loss: 2.2593562602996826\n",
      "50: accuracy:0.1702127659574468 loss: 2.228203058242798\n",
      "51: accuracy:0.1702127659574468 loss: 2.2853829860687256\n",
      "52: accuracy:0.1702127659574468 loss: 2.2586302757263184\n",
      "53: accuracy:0.1702127659574468 loss: 2.2867860794067383\n",
      "54: accuracy:0.1702127659574468 loss: 2.256540060043335\n",
      "55: accuracy:0.19858156028368795 loss: 2.277691125869751\n",
      "56: accuracy:0.19858156028368795 loss: 2.2811882495880127\n",
      "57: accuracy:0.19858156028368795 loss: 2.282367706298828\n",
      "58: accuracy:0.19858156028368795 loss: 2.2812232971191406\n",
      "59: accuracy:0.19858156028368795 loss: 2.300290584564209\n",
      "60: accuracy:0.19858156028368795 loss: 2.274479627609253\n",
      "61: accuracy:0.19858156028368795 loss: 2.267716884613037\n",
      "62: accuracy:0.19858156028368795 loss: 2.308748722076416\n",
      "63: accuracy:0.1702127659574468 loss: 2.248641014099121\n",
      "64: accuracy:0.1702127659574468 loss: 2.309025287628174\n",
      "65: accuracy:0.1702127659574468 loss: 2.2759194374084473\n",
      "66: accuracy:0.1702127659574468 loss: 2.251758337020874\n",
      "67: accuracy:0.1702127659574468 loss: 2.282416343688965\n",
      "68: accuracy:0.1702127659574468 loss: 2.248839855194092\n",
      "69: accuracy:0.19858156028368795 loss: 2.30180025100708\n",
      "70: accuracy:0.19858156028368795 loss: 2.2252659797668457\n",
      "71: accuracy:0.19858156028368795 loss: 2.3117101192474365\n",
      "72: accuracy:0.19858156028368795 loss: 2.2537994384765625\n",
      "73: accuracy:0.19858156028368795 loss: 2.291280746459961\n",
      "74: accuracy:0.19858156028368795 loss: 2.282360076904297\n",
      "75: accuracy:0.19858156028368795 loss: 2.2587099075317383\n",
      "76: accuracy:0.1702127659574468 loss: 2.2385919094085693\n",
      "77: accuracy:0.1702127659574468 loss: 2.2764170169830322\n",
      "78: accuracy:0.1702127659574468 loss: 2.2631306648254395\n",
      "79: accuracy:0.1702127659574468 loss: 2.2893104553222656\n",
      "80: accuracy:0.1702127659574468 loss: 2.2891299724578857\n",
      "81: accuracy:0.1702127659574468 loss: 2.2752537727355957\n",
      "82: accuracy:0.1702127659574468 loss: 2.252147912979126\n",
      "83: accuracy:0.1702127659574468 loss: 2.303673505783081\n",
      "84: accuracy:0.1702127659574468 loss: 2.2207589149475098\n",
      "85: accuracy:0.1702127659574468 loss: 2.2811403274536133\n",
      "86: accuracy:0.1702127659574468 loss: 2.298785924911499\n",
      "87: accuracy:0.1702127659574468 loss: 2.280412197113037\n",
      "88: accuracy:0.1702127659574468 loss: 2.2936689853668213\n",
      "89: accuracy:0.1702127659574468 loss: 2.319286346435547\n",
      "90: accuracy:0.1702127659574468 loss: 2.290252208709717\n",
      "91: accuracy:0.19858156028368795 loss: 2.3227028846740723\n",
      "92: accuracy:0.19858156028368795 loss: 2.2373080253601074\n",
      "93: accuracy:0.19858156028368795 loss: 2.2861149311065674\n",
      "94: accuracy:0.19858156028368795 loss: 2.2367260456085205\n",
      "95: accuracy:0.19858156028368795 loss: 2.3219330310821533\n",
      "96: accuracy:0.19858156028368795 loss: 2.279650926589966\n",
      "97: accuracy:0.19858156028368795 loss: 2.29482102394104\n",
      "98: accuracy:0.1702127659574468 loss: 2.3174307346343994\n",
      "99: accuracy:0.1702127659574468 loss: 2.2727067470550537\n",
      "100: accuracy:0.1702127659574468 loss: 2.2588305473327637\n",
      "101: accuracy:0.1702127659574468 loss: 2.2739834785461426\n",
      "102: accuracy:0.1702127659574468 loss: 2.2520735263824463\n",
      "103: accuracy:0.1702127659574468 loss: 2.247330904006958\n",
      "104: accuracy:0.1702127659574468 loss: 2.2160346508026123\n",
      "105: accuracy:0.1702127659574468 loss: 2.244259834289551\n",
      "106: accuracy:0.1702127659574468 loss: 2.254011631011963\n",
      "107: accuracy:0.1702127659574468 loss: 2.272369146347046\n",
      "108: accuracy:0.19858156028368795 loss: 2.2897002696990967\n",
      "109: accuracy:0.19858156028368795 loss: 2.215641498565674\n",
      "110: accuracy:0.19858156028368795 loss: 2.261630058288574\n",
      "111: accuracy:0.19858156028368795 loss: 2.2835099697113037\n",
      "112: accuracy:0.1702127659574468 loss: 2.282987356185913\n",
      "113: accuracy:0.1702127659574468 loss: 2.331940174102783\n",
      "114: accuracy:0.1702127659574468 loss: 2.2737622261047363\n",
      "115: accuracy:0.1702127659574468 loss: 2.3150713443756104\n",
      "116: accuracy:0.1702127659574468 loss: 2.2283313274383545\n",
      "117: accuracy:0.1702127659574468 loss: 2.29658842086792\n",
      "118: accuracy:0.1702127659574468 loss: 2.2546589374542236\n",
      "119: accuracy:0.1702127659574468 loss: 2.2727651596069336\n",
      "120: accuracy:0.1702127659574468 loss: 2.2976326942443848\n",
      "121: accuracy:0.1702127659574468 loss: 2.268010139465332\n",
      "122: accuracy:0.1702127659574468 loss: 2.2809383869171143\n",
      "123: accuracy:0.1702127659574468 loss: 2.2675700187683105\n",
      "124: accuracy:0.1702127659574468 loss: 2.3199751377105713\n",
      "125: accuracy:0.1702127659574468 loss: 2.2566425800323486\n",
      "126: accuracy:0.1702127659574468 loss: 2.2656731605529785\n",
      "127: accuracy:0.1702127659574468 loss: 2.3190462589263916\n",
      "128: accuracy:0.1702127659574468 loss: 2.2416598796844482\n",
      "129: accuracy:0.19858156028368795 loss: 2.2793471813201904\n",
      "130: accuracy:0.19858156028368795 loss: 2.2630107402801514\n",
      "131: accuracy:0.19858156028368795 loss: 2.2288856506347656\n",
      "132: accuracy:0.19858156028368795 loss: 2.287208080291748\n",
      "133: accuracy:0.19858156028368795 loss: 2.279754161834717\n",
      "134: accuracy:0.19858156028368795 loss: 2.261561155319214\n",
      "135: accuracy:0.19858156028368795 loss: 2.2559969425201416\n",
      "136: accuracy:0.19858156028368795 loss: 2.2443525791168213\n",
      "137: accuracy:0.19858156028368795 loss: 2.2757925987243652\n",
      "138: accuracy:0.19858156028368795 loss: 2.259082555770874\n",
      "139: accuracy:0.19858156028368795 loss: 2.2960498332977295\n",
      "140: accuracy:0.19858156028368795 loss: 2.258301019668579\n",
      "141: accuracy:0.19858156028368795 loss: 2.2902703285217285\n",
      "142: accuracy:0.19858156028368795 loss: 2.3208117485046387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143: accuracy:0.19858156028368795 loss: 2.2842042446136475\n",
      "144: accuracy:0.19858156028368795 loss: 2.254070281982422\n",
      "145: accuracy:0.19858156028368795 loss: 2.2810349464416504\n",
      "146: accuracy:0.19858156028368795 loss: 2.2747700214385986\n",
      "147: accuracy:0.19858156028368795 loss: 2.3059682846069336\n",
      "148: accuracy:0.19858156028368795 loss: 2.229618787765503\n",
      "149: accuracy:0.19858156028368795 loss: 2.288832902908325\n",
      "150: accuracy:0.19858156028368795 loss: 2.2772305011749268\n",
      "151: accuracy:0.1702127659574468 loss: 2.238725185394287\n",
      "152: accuracy:0.1702127659574468 loss: 2.285245895385742\n",
      "153: accuracy:0.1702127659574468 loss: 2.2971010208129883\n",
      "154: accuracy:0.1702127659574468 loss: 2.2709646224975586\n",
      "155: accuracy:0.1702127659574468 loss: 2.2439606189727783\n",
      "156: accuracy:0.1702127659574468 loss: 2.2653632164001465\n",
      "157: accuracy:0.1702127659574468 loss: 2.27630615234375\n",
      "158: accuracy:0.1702127659574468 loss: 2.2898292541503906\n",
      "159: accuracy:0.1702127659574468 loss: 2.288764715194702\n",
      "160: accuracy:0.1702127659574468 loss: 2.2631325721740723\n",
      "161: accuracy:0.19858156028368795 loss: 2.2996795177459717\n",
      "162: accuracy:0.19858156028368795 loss: 2.2717366218566895\n",
      "163: accuracy:0.19858156028368795 loss: 2.2896997928619385\n",
      "164: accuracy:0.19858156028368795 loss: 2.3005530834198\n",
      "165: accuracy:0.19858156028368795 loss: 2.255436420440674\n",
      "166: accuracy:0.19858156028368795 loss: 2.2397561073303223\n",
      "167: accuracy:0.19858156028368795 loss: 2.26194167137146\n",
      "168: accuracy:0.19858156028368795 loss: 2.261167526245117\n",
      "169: accuracy:0.19858156028368795 loss: 2.2551939487457275\n",
      "170: accuracy:0.19858156028368795 loss: 2.260622024536133\n",
      "171: accuracy:0.19858156028368795 loss: 2.2369139194488525\n",
      "172: accuracy:0.1702127659574468 loss: 2.2726924419403076\n",
      "173: accuracy:0.1702127659574468 loss: 2.2956643104553223\n",
      "174: accuracy:0.1702127659574468 loss: 2.2741427421569824\n",
      "175: accuracy:0.1702127659574468 loss: 2.264219045639038\n",
      "176: accuracy:0.1702127659574468 loss: 2.2363510131835938\n",
      "177: accuracy:0.1702127659574468 loss: 2.2524361610412598\n",
      "178: accuracy:0.1702127659574468 loss: 2.238431692123413\n",
      "179: accuracy:0.1702127659574468 loss: 2.317594528198242\n",
      "180: accuracy:0.1702127659574468 loss: 2.2593791484832764\n",
      "181: accuracy:0.1702127659574468 loss: 2.1926164627075195\n",
      "182: accuracy:0.1702127659574468 loss: 2.283245086669922\n",
      "183: accuracy:0.1702127659574468 loss: 2.292205810546875\n",
      "184: accuracy:0.1702127659574468 loss: 2.3375487327575684\n",
      "185: accuracy:0.1702127659574468 loss: 2.349320411682129\n",
      "186: accuracy:0.1702127659574468 loss: 2.297837734222412\n",
      "187: accuracy:0.1702127659574468 loss: 2.225708484649658\n",
      "188: accuracy:0.1702127659574468 loss: 2.229858636856079\n",
      "189: accuracy:0.1702127659574468 loss: 2.2371861934661865\n",
      "190: accuracy:0.1702127659574468 loss: 2.2664878368377686\n",
      "191: accuracy:0.1702127659574468 loss: 2.3070504665374756\n",
      "192: accuracy:0.1702127659574468 loss: 2.2381160259246826\n",
      "193: accuracy:0.1702127659574468 loss: 2.276754140853882\n",
      "194: accuracy:0.19858156028368795 loss: 2.196692943572998\n",
      "195: accuracy:0.19858156028368795 loss: 2.2808990478515625\n",
      "196: accuracy:0.19858156028368795 loss: 2.250103235244751\n",
      "197: accuracy:0.19858156028368795 loss: 2.2086737155914307\n",
      "198: accuracy:0.19858156028368795 loss: 2.3062829971313477\n",
      "199: accuracy:0.1702127659574468 loss: 2.2906363010406494\n",
      "200: accuracy:0.1702127659574468 loss: 2.2903401851654053\n",
      "201: accuracy:0.1702127659574468 loss: 2.301281690597534\n",
      "202: accuracy:0.1702127659574468 loss: 2.2914037704467773\n",
      "203: accuracy:0.1702127659574468 loss: 2.2855098247528076\n",
      "204: accuracy:0.1702127659574468 loss: 2.2957968711853027\n",
      "205: accuracy:0.1702127659574468 loss: 2.231825590133667\n",
      "206: accuracy:0.19858156028368795 loss: 2.254584789276123\n",
      "207: accuracy:0.19858156028368795 loss: 2.271317720413208\n",
      "208: accuracy:0.19858156028368795 loss: 2.2512259483337402\n",
      "209: accuracy:0.19858156028368795 loss: 2.2978765964508057\n",
      "210: accuracy:0.19858156028368795 loss: 2.2795331478118896\n",
      "211: accuracy:0.19858156028368795 loss: 2.274609327316284\n",
      "212: accuracy:0.19858156028368795 loss: 2.287393569946289\n",
      "213: accuracy:0.19858156028368795 loss: 2.290060520172119\n",
      "214: accuracy:0.19858156028368795 loss: 2.2732324600219727\n",
      "215: accuracy:0.19858156028368795 loss: 2.2440543174743652\n",
      "216: accuracy:0.19858156028368795 loss: 2.3146417140960693\n",
      "217: accuracy:0.19858156028368795 loss: 2.2916014194488525\n",
      "218: accuracy:0.19858156028368795 loss: 2.26464581489563\n",
      "219: accuracy:0.19858156028368795 loss: 2.2638587951660156\n",
      "220: accuracy:0.19858156028368795 loss: 2.255520820617676\n",
      "221: accuracy:0.19858156028368795 loss: 2.2634403705596924\n",
      "222: accuracy:0.19858156028368795 loss: 2.271613359451294\n",
      "223: accuracy:0.19858156028368795 loss: 2.295048475265503\n",
      "224: accuracy:0.19858156028368795 loss: 2.3019015789031982\n",
      "225: accuracy:0.19858156028368795 loss: 2.2923545837402344\n",
      "226: accuracy:0.1702127659574468 loss: 2.237032651901245\n",
      "227: accuracy:0.1702127659574468 loss: 2.2958686351776123\n",
      "228: accuracy:0.1702127659574468 loss: 2.302727460861206\n",
      "229: accuracy:0.1702127659574468 loss: 2.276947021484375\n",
      "230: accuracy:0.19858156028368795 loss: 2.25636887550354\n",
      "231: accuracy:0.19858156028368795 loss: 2.2537753582000732\n",
      "232: accuracy:0.19858156028368795 loss: 2.2519209384918213\n",
      "233: accuracy:0.19858156028368795 loss: 2.2768354415893555\n",
      "234: accuracy:0.19858156028368795 loss: 2.2388575077056885\n",
      "235: accuracy:0.19858156028368795 loss: 2.255331039428711\n",
      "236: accuracy:0.19858156028368795 loss: 2.2584362030029297\n",
      "237: accuracy:0.19858156028368795 loss: 2.267406463623047\n",
      "238: accuracy:0.19858156028368795 loss: 2.270350456237793\n",
      "239: accuracy:0.1702127659574468 loss: 2.268435478210449\n",
      "240: accuracy:0.1702127659574468 loss: 2.267578363418579\n",
      "241: accuracy:0.1702127659574468 loss: 2.2802813053131104\n",
      "242: accuracy:0.1702127659574468 loss: 2.304995059967041\n",
      "243: accuracy:0.19858156028368795 loss: 2.2475292682647705\n",
      "244: accuracy:0.19858156028368795 loss: 2.2629058361053467\n",
      "245: accuracy:0.19858156028368795 loss: 2.296055316925049\n",
      "246: accuracy:0.19858156028368795 loss: 2.3019402027130127\n",
      "247: accuracy:0.19858156028368795 loss: 2.2710235118865967\n",
      "248: accuracy:0.19858156028368795 loss: 2.2743008136749268\n",
      "249: accuracy:0.19858156028368795 loss: 2.321711301803589\n",
      "250: accuracy:0.19858156028368795 loss: 2.280472993850708\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    ")\n",
    "net = RNN_old()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "for epoch in range(1,251): \n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        pred = net(batch_x)\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 2: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_e)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-new 0.6879432624113475 112轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "MAX_LENGTH = 53\n",
    "for i in range(0,467):\n",
    "    a = np.loadtxt('/Users/denhiroshi/Downloads/Embeding/ans_1.0_'+str(i)+'.csv',dtype=np.float64,delimiter=',')\n",
    "    a = a.transpose(1,0)\n",
    "    a = np.exp(a)\n",
    "    a = np.sum(a,axis=1)\n",
    "    a = np.log(a)\n",
    "    if a.shape[0] < MAX_LENGTH:\n",
    "        add = MAX_LENGTH - a.shape[0]\n",
    "        adds = np.zeros(add)\n",
    "        a = np.concatenate((a, adds))\n",
    "    inputs.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use = np.stack(inputs)\n",
    "mean = use.mean()         #计算平均数\n",
    "deviation = use.std()     #计算标准差\n",
    "# 标准化数据的公式: (数据值 - 平均数) / 标准差\n",
    "use = (use - mean) / deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.fromfile('glove_WV.dat',dtype=np.float64).reshape((467,-1,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((467, 53, 200), (467, 53))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape,use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for i in range(467):\n",
    "    inputs.append((torch.from_numpy(b[i]),torch.from_numpy(use[i]),Y[i]))\n",
    "shuffle(inputs)\n",
    "flag = int(len(inputs)*0.7)\n",
    "train = inputs[:flag]\n",
    "test = inputs[flag:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([141, 53]), torch.Size([141, 53, 200]), torch.Size([141]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_s = []\n",
    "t_e = []\n",
    "t_y = []\n",
    "for E,S,Y in test:\n",
    "    t_s.append(S)\n",
    "    t_e.append(E)\n",
    "    t_y.append(Y)\n",
    "t_s = torch.stack(t_s)\n",
    "t_e = torch.stack(t_e)\n",
    "t_y = torch.from_numpy(np.array(t_y))\n",
    "t_s.shape, t_e.shape, t_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size = 200,       # rnn input\n",
    "            hidden_size = 64,     # rnn hidden unit\n",
    "            num_layers = 1,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=65 ,out_features=7)\n",
    "        self.maxpool = torch.nn.MaxPool2d((53,1))\n",
    "\n",
    "    def forward(self,sen,x):\n",
    "        output,(h_n,c_n)=self.rnn(x.float())\n",
    "        sen = sen.reshape((sen.shape[0],sen.shape[1],1))  # (BATCH_SIZE,53,1)\n",
    "        output1 = torch.bmm(torch.transpose(output,1,2).float(),sen.float())  # (BATCH_SIZE,64,1)\n",
    "        output1 = torch.sigmoid(output1)\n",
    "        output1 = torch.softmax(output1,1)\n",
    "        output1 = torch.bmm(output,output1) # (BATCH_SIZE,53,1)\n",
    "        output = torch.cat((output1,output),2) # (BATCH_SIZE,53,65)\n",
    "        output = self.maxpool(output)   # (BATCH_SIZE,1,65)\n",
    "        output = output.reshape((output.shape[0],-1)) # (BATCH_SIZE,7)\n",
    "        x1 = self.out(output.float())\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): LSTM(200, 64, batch_first=True)\n",
      "  (out): Linear(in_features=65, out_features=7, bias=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=(53, 1), stride=(53, 1), padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "1: accuracy:0.3049645390070922 loss: 1.9433010816574097\n",
      "2: accuracy:0.3333333333333333 loss: 1.8462063074111938\n",
      "3: accuracy:0.4326241134751773 loss: 1.6321767568588257\n",
      "4: accuracy:0.49645390070921985 loss: 1.5053471326828003\n",
      "5: accuracy:0.5106382978723404 loss: 1.2786084413528442\n",
      "6: accuracy:0.5390070921985816 loss: 1.175156831741333\n",
      "7: accuracy:0.524822695035461 loss: 1.0732951164245605\n",
      "8: accuracy:0.5531914893617021 loss: 0.9134953022003174\n",
      "9: accuracy:0.5815602836879432 loss: 0.7435367107391357\n",
      "10: accuracy:0.5815602836879432 loss: 0.6175999641418457\n",
      "11: accuracy:0.5815602836879432 loss: 0.5391039252281189\n",
      "12: accuracy:0.5815602836879432 loss: 0.4725273847579956\n",
      "13: accuracy:0.5886524822695035 loss: 0.34439972043037415\n",
      "14: accuracy:0.6028368794326241 loss: 0.24604764580726624\n",
      "15: accuracy:0.6170212765957447 loss: 0.1664118468761444\n",
      "16: accuracy:0.6028368794326241 loss: 0.16386084258556366\n",
      "17: accuracy:0.6099290780141844 loss: 0.1274433135986328\n",
      "18: accuracy:0.6099290780141844 loss: 0.3185354471206665\n",
      "19: accuracy:0.5319148936170213 loss: 0.11869361996650696\n",
      "20: accuracy:0.5673758865248227 loss: 0.26219844818115234\n",
      "21: accuracy:0.624113475177305 loss: 0.08491356670856476\n",
      "22: accuracy:0.6312056737588653 loss: 0.09954380989074707\n",
      "23: accuracy:0.6666666666666666 loss: 0.09217797219753265\n",
      "24: accuracy:0.6170212765957447 loss: 0.05951058864593506\n",
      "25: accuracy:0.624113475177305 loss: 0.04984144866466522\n",
      "26: accuracy:0.6595744680851063 loss: 0.032645922154188156\n",
      "27: accuracy:0.6382978723404256 loss: 0.027983970940113068\n",
      "28: accuracy:0.624113475177305 loss: 0.022928498685359955\n",
      "29: accuracy:0.5815602836879432 loss: 0.02700561098754406\n",
      "30: accuracy:0.5673758865248227 loss: 0.01825540140271187\n",
      "31: accuracy:0.524822695035461 loss: 0.056767143309116364\n",
      "32: accuracy:0.5531914893617021 loss: 0.3699510991573334\n",
      "33: accuracy:0.574468085106383 loss: 0.33102405071258545\n",
      "34: accuracy:0.524822695035461 loss: 0.20961178839206696\n",
      "35: accuracy:0.5531914893617021 loss: 0.2133644074201584\n",
      "36: accuracy:0.5886524822695035 loss: 0.11304041743278503\n",
      "37: accuracy:0.5815602836879432 loss: 0.12906071543693542\n",
      "38: accuracy:0.6312056737588653 loss: 0.109769806265831\n",
      "39: accuracy:0.6595744680851063 loss: 0.06828761100769043\n",
      "40: accuracy:0.6382978723404256 loss: 0.05610523000359535\n",
      "41: accuracy:0.6524822695035462 loss: 0.045178353786468506\n",
      "42: accuracy:0.6524822695035462 loss: 0.05034245550632477\n",
      "43: accuracy:0.6524822695035462 loss: 0.03926897794008255\n",
      "44: accuracy:0.6595744680851063 loss: 0.03444347530603409\n",
      "45: accuracy:0.6595744680851063 loss: 0.022242646664381027\n",
      "46: accuracy:0.6666666666666666 loss: 0.01924644038081169\n",
      "47: accuracy:0.6808510638297872 loss: 0.02072754129767418\n",
      "48: accuracy:0.6879432624113475 loss: 0.016258327290415764\n",
      "49: accuracy:0.6808510638297872 loss: 0.013864755630493164\n",
      "50: accuracy:0.6737588652482269 loss: 0.012233339250087738\n",
      "51: accuracy:0.6666666666666666 loss: 0.010937189683318138\n",
      "52: accuracy:0.6666666666666666 loss: 0.009786445647478104\n",
      "53: accuracy:0.6666666666666666 loss: 0.009069938212633133\n",
      "54: accuracy:0.6524822695035462 loss: 0.008711166679859161\n",
      "55: accuracy:0.6453900709219859 loss: 0.009028449654579163\n",
      "56: accuracy:0.6524822695035462 loss: 0.006858125329017639\n",
      "57: accuracy:0.6453900709219859 loss: 0.00597110390663147\n",
      "58: accuracy:0.6453900709219859 loss: 0.006728433072566986\n",
      "59: accuracy:0.6524822695035462 loss: 0.006042366847395897\n",
      "60: accuracy:0.6595744680851063 loss: 0.005372414365410805\n",
      "61: accuracy:0.6595744680851063 loss: 0.005431022495031357\n",
      "62: accuracy:0.6595744680851063 loss: 0.004236515611410141\n",
      "63: accuracy:0.6666666666666666 loss: 0.005127556622028351\n",
      "64: accuracy:0.6595744680851063 loss: 0.00448116660118103\n",
      "65: accuracy:0.6595744680851063 loss: 0.003843197599053383\n",
      "66: accuracy:0.6666666666666666 loss: 0.004150060936808586\n",
      "67: accuracy:0.6666666666666666 loss: 0.004219941794872284\n",
      "68: accuracy:0.6595744680851063 loss: 0.004353407770395279\n",
      "69: accuracy:0.6524822695035462 loss: 0.003758316859602928\n",
      "70: accuracy:0.6595744680851063 loss: 0.003563877195119858\n",
      "71: accuracy:0.6595744680851063 loss: 0.003087500110268593\n",
      "72: accuracy:0.6595744680851063 loss: 0.003020498901605606\n",
      "73: accuracy:0.6595744680851063 loss: 0.0033670254051685333\n",
      "74: accuracy:0.6524822695035462 loss: 0.003099406138062477\n",
      "75: accuracy:0.6524822695035462 loss: 0.0031638778746128082\n",
      "76: accuracy:0.6524822695035462 loss: 0.0027914047241210938\n",
      "77: accuracy:0.6453900709219859 loss: 0.002969413995742798\n",
      "78: accuracy:0.6453900709219859 loss: 0.0030260123312473297\n",
      "79: accuracy:0.6524822695035462 loss: 0.002844393253326416\n",
      "80: accuracy:0.6524822695035462 loss: 0.0026828795671463013\n",
      "81: accuracy:0.6524822695035462 loss: 0.0028370358049869537\n",
      "82: accuracy:0.6453900709219859 loss: 0.002687990665435791\n",
      "83: accuracy:0.6453900709219859 loss: 0.002780245617032051\n",
      "84: accuracy:0.6453900709219859 loss: 0.002489496022462845\n",
      "85: accuracy:0.6453900709219859 loss: 0.0024884194135665894\n",
      "86: accuracy:0.6453900709219859 loss: 0.0022682324051856995\n",
      "87: accuracy:0.6453900709219859 loss: 0.002631470561027527\n",
      "88: accuracy:0.6453900709219859 loss: 0.002343989908695221\n",
      "89: accuracy:0.6453900709219859 loss: 0.0023718923330307007\n",
      "90: accuracy:0.6453900709219859 loss: 0.002213023602962494\n",
      "91: accuracy:0.6524822695035462 loss: 0.0024929270148277283\n",
      "92: accuracy:0.6524822695035462 loss: 0.002314060926437378\n",
      "93: accuracy:0.6524822695035462 loss: 0.002442695200443268\n",
      "94: accuracy:0.6524822695035462 loss: 0.0019778795540332794\n",
      "95: accuracy:0.6524822695035462 loss: 0.00248570553958416\n",
      "96: accuracy:0.6524822695035462 loss: 0.0025307834148406982\n",
      "97: accuracy:0.6524822695035462 loss: 0.002057807520031929\n",
      "98: accuracy:0.6595744680851063 loss: 0.0019707903265953064\n",
      "99: accuracy:0.6666666666666666 loss: 0.0016860663890838623\n",
      "100: accuracy:0.6737588652482269 loss: 0.0018322505056858063\n",
      "101: accuracy:0.6524822695035462 loss: 0.0018260665237903595\n",
      "102: accuracy:0.6453900709219859 loss: 0.0020616836845874786\n",
      "103: accuracy:0.6595744680851063 loss: 0.002443954348564148\n",
      "104: accuracy:0.6666666666666666 loss: 0.007084069773554802\n",
      "105: accuracy:0.6595744680851063 loss: 0.0031951647251844406\n",
      "106: accuracy:0.6737588652482269 loss: 0.002370476722717285\n",
      "107: accuracy:0.6737588652482269 loss: 0.002110429108142853\n",
      "108: accuracy:0.6666666666666666 loss: 0.0021999329328536987\n",
      "109: accuracy:0.6666666666666666 loss: 0.002436697483062744\n",
      "110: accuracy:0.6737588652482269 loss: 0.0026341434568166733\n",
      "111: accuracy:0.6737588652482269 loss: 0.002287890762090683\n",
      "112: accuracy:0.6879432624113475 loss: 0.0018811039626598358\n",
      "113: accuracy:0.6879432624113475 loss: 0.0018499977886676788\n",
      "114: accuracy:0.6808510638297872 loss: 0.0015340261161327362\n",
      "115: accuracy:0.6808510638297872 loss: 0.0016307719051837921\n",
      "116: accuracy:0.6879432624113475 loss: 0.0018967390060424805\n",
      "117: accuracy:0.6808510638297872 loss: 0.0016646049916744232\n",
      "118: accuracy:0.6808510638297872 loss: 0.0015818886458873749\n",
      "119: accuracy:0.6808510638297872 loss: 0.001783996820449829\n",
      "120: accuracy:0.6808510638297872 loss: 0.0015989243984222412\n",
      "121: accuracy:0.6737588652482269 loss: 0.0014537908136844635\n",
      "122: accuracy:0.6737588652482269 loss: 0.0012917742133140564\n",
      "123: accuracy:0.6737588652482269 loss: 0.0014076977968215942\n",
      "124: accuracy:0.6737588652482269 loss: 0.0013692490756511688\n",
      "125: accuracy:0.6737588652482269 loss: 0.0013205669820308685\n",
      "126: accuracy:0.6737588652482269 loss: 0.0012695901095867157\n",
      "127: accuracy:0.6737588652482269 loss: 0.001204710453748703\n",
      "128: accuracy:0.6737588652482269 loss: 0.001134674996137619\n",
      "129: accuracy:0.6737588652482269 loss: 0.0013100095093250275\n",
      "130: accuracy:0.6737588652482269 loss: 0.0013776011765003204\n",
      "131: accuracy:0.6737588652482269 loss: 0.0011471286416053772\n",
      "132: accuracy:0.6737588652482269 loss: 0.0011342726647853851\n",
      "133: accuracy:0.6737588652482269 loss: 0.001140844076871872\n",
      "134: accuracy:0.6737588652482269 loss: 0.001132957637310028\n",
      "135: accuracy:0.6666666666666666 loss: 0.0010447874665260315\n",
      "136: accuracy:0.6666666666666666 loss: 0.0010811761021614075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137: accuracy:0.6666666666666666 loss: 0.0009977295994758606\n",
      "138: accuracy:0.6666666666666666 loss: 0.00104542076587677\n",
      "139: accuracy:0.6666666666666666 loss: 0.0009148344397544861\n",
      "140: accuracy:0.6666666666666666 loss: 0.0008632875978946686\n",
      "141: accuracy:0.6666666666666666 loss: 0.0010071024298667908\n",
      "142: accuracy:0.6666666666666666 loss: 0.0009018294513225555\n",
      "143: accuracy:0.6666666666666666 loss: 0.0010723061859607697\n",
      "144: accuracy:0.6666666666666666 loss: 0.001063574105501175\n",
      "145: accuracy:0.6666666666666666 loss: 0.0008732005953788757\n",
      "146: accuracy:0.6666666666666666 loss: 0.0009153597056865692\n",
      "147: accuracy:0.6666666666666666 loss: 0.0008699037134647369\n",
      "148: accuracy:0.6666666666666666 loss: 0.0009642206132411957\n",
      "149: accuracy:0.6666666666666666 loss: 0.0008136220276355743\n",
      "150: accuracy:0.6595744680851063 loss: 0.0009157955646514893\n",
      "151: accuracy:0.6595744680851063 loss: 0.0007381327450275421\n",
      "152: accuracy:0.6595744680851063 loss: 0.0007743127644062042\n",
      "153: accuracy:0.6595744680851063 loss: 0.000845540314912796\n",
      "154: accuracy:0.6595744680851063 loss: 0.0007657557725906372\n",
      "155: accuracy:0.6595744680851063 loss: 0.0007358789443969727\n",
      "156: accuracy:0.6595744680851063 loss: 0.000827256590127945\n",
      "157: accuracy:0.6595744680851063 loss: 0.0008114762604236603\n",
      "158: accuracy:0.6595744680851063 loss: 0.0008452124893665314\n",
      "159: accuracy:0.6595744680851063 loss: 0.0007660984992980957\n",
      "160: accuracy:0.6595744680851063 loss: 0.0007523335516452789\n",
      "161: accuracy:0.6595744680851063 loss: 0.0007281042635440826\n",
      "162: accuracy:0.6595744680851063 loss: 0.0007149875164031982\n",
      "163: accuracy:0.6595744680851063 loss: 0.0007968731224536896\n",
      "164: accuracy:0.6595744680851063 loss: 0.0008462779223918915\n",
      "165: accuracy:0.6595744680851063 loss: 0.0007119141519069672\n",
      "166: accuracy:0.6595744680851063 loss: 0.0008151009678840637\n",
      "167: accuracy:0.6595744680851063 loss: 0.0006587579846382141\n",
      "168: accuracy:0.6595744680851063 loss: 0.0006743855774402618\n",
      "169: accuracy:0.6595744680851063 loss: 0.0007705427706241608\n",
      "170: accuracy:0.6595744680851063 loss: 0.0006128326058387756\n",
      "171: accuracy:0.6595744680851063 loss: 0.0005637817084789276\n",
      "172: accuracy:0.6595744680851063 loss: 0.0006308406591415405\n",
      "173: accuracy:0.6595744680851063 loss: 0.0007073953747749329\n",
      "174: accuracy:0.6595744680851063 loss: 0.0006887689232826233\n",
      "175: accuracy:0.6595744680851063 loss: 0.0006721131503582001\n",
      "176: accuracy:0.6595744680851063 loss: 0.0006576254963874817\n",
      "177: accuracy:0.6524822695035462 loss: 0.0006757080554962158\n",
      "178: accuracy:0.6524822695035462 loss: 0.0006686821579933167\n",
      "179: accuracy:0.6524822695035462 loss: 0.0005606040358543396\n",
      "180: accuracy:0.6524822695035462 loss: 0.0006209723651409149\n",
      "181: accuracy:0.6524822695035462 loss: 0.0005630850791931152\n",
      "182: accuracy:0.6524822695035462 loss: 0.0006117671728134155\n",
      "183: accuracy:0.6524822695035462 loss: 0.0006004087626934052\n",
      "184: accuracy:0.6524822695035462 loss: 0.0005598664283752441\n",
      "185: accuracy:0.6524822695035462 loss: 0.000602230429649353\n",
      "186: accuracy:0.6524822695035462 loss: 0.000531323254108429\n",
      "187: accuracy:0.6524822695035462 loss: 0.00046457722783088684\n",
      "188: accuracy:0.6524822695035462 loss: 0.0006042495369911194\n",
      "189: accuracy:0.6524822695035462 loss: 0.0005611181259155273\n",
      "190: accuracy:0.6524822695035462 loss: 0.0005367770791053772\n",
      "191: accuracy:0.6524822695035462 loss: 0.0005759336054325104\n",
      "192: accuracy:0.6524822695035462 loss: 0.0005970969796180725\n",
      "193: accuracy:0.6524822695035462 loss: 0.0005143024027347565\n",
      "194: accuracy:0.6524822695035462 loss: 0.00046384334564208984\n",
      "195: accuracy:0.6524822695035462 loss: 0.0005215741693973541\n",
      "196: accuracy:0.6524822695035462 loss: 0.0004964061081409454\n",
      "197: accuracy:0.6524822695035462 loss: 0.0004875399172306061\n",
      "198: accuracy:0.6524822695035462 loss: 0.0005205757915973663\n",
      "199: accuracy:0.6524822695035462 loss: 0.0005607940256595612\n",
      "200: accuracy:0.6524822695035462 loss: 0.0005382075905799866\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    "    drop_last=True,\n",
    ")\n",
    "net = RNN()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "for epoch in range(1,201): \n",
    "    for step, (batch_x, sen, batch_y) in enumerate(loader):\n",
    "        pred = net(sen, batch_x)\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 1: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_s, t_e)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
