{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function set_backend in module tensorly:\n",
      "\n",
      "set_backend(backend_name)\n",
      "    Sets the backend for TensorLy\n",
      "    \n",
      "        The backend will be set as specified and operations will used that backend\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    backend_name : {'mxnet', 'numpy', 'pytorch', 'tensorflow', 'cupy'}, default is 'numpy'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pytorch backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.decomposition import tucker\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import preprocessing\n",
    "from random import shuffle\n",
    "BATCH_SIZE = 128\n",
    "tl.set_backend('pytorch')\n",
    "help(tl.set_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467,)\n"
     ]
    }
   ],
   "source": [
    "tag = [72, 71, 84, 54, 57, 65, 64]\n",
    "Y = np.zeros(467).astype(np.float64)\n",
    "print(Y.shape)\n",
    "Y[0 : tag[0]] = 0\n",
    "Y[tag[0] : tag[0]+tag[1]] = 1\n",
    "Y[tag[0]+tag[1] : tag[0]+tag[1]+tag[2]] = 2\n",
    "Y[tag[0]+tag[1]+tag[2] : tag[0]+tag[1]+tag[2]+tag[3]] = 3\n",
    "Y[tag[0]+tag[1]+tag[2]+tag[3] : tag[0]+tag[1]+tag[2]+tag[3]+tag[4]] = 4\n",
    "Y[tag[0]+tag[1]+tag[2]+tag[3]+tag[4] : tag[0]+tag[1]+tag[2]+tag[3]+tag[4]+tag[5]] = 5\n",
    "Y[tag[0]+tag[1]+tag[2]+tag[3]+tag[4]+tag[5] : tag[0]+tag[1]+tag[2]+tag[3]+tag[4]+tag[5]+tag[6]] = 6\n",
    "Y = Y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = []\n",
    "MAX_LENGTH = 53\n",
    "for i in range(0,467):\n",
    "    a = np.loadtxt('/Users/denhiroshi/Downloads/Embeding/ans_1.0_'+str(i)+'.csv',dtype=np.float64,delimiter=',')\n",
    "    a = a.transpose(1,0)\n",
    "    a = np.exp(a)\n",
    "    a = np.sum(a,axis=1)\n",
    "    a = np.log(a)\n",
    "    if a.shape[0] < MAX_LENGTH:\n",
    "        add = MAX_LENGTH - a.shape[0]\n",
    "        adds = np.zeros(add).astype(np.float64)\n",
    "        a = np.concatenate((a, adds))\n",
    "    inputs.append(a)\n",
    "inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467, 53)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use = np.stack(inputs).astype(np.float32)\n",
    "mean = use.mean()         #计算平均数\n",
    "deviation = use.std()     #计算标准差\n",
    "# 标准化数据的公式: (数据值 - 平均数) / 标准差\n",
    "use = (use - mean) / deviation\n",
    "use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467, 53, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.fromfile('glove_WV.dat',dtype=np.float64).reshape((467,-1,200))\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for i in range(467):\n",
    "    inputs.append((torch.from_numpy(b[i]),torch.from_numpy(use[i]),Y[i]))\n",
    "shuffle(inputs)\n",
    "flag = int(len(inputs)*0.7)\n",
    "train = inputs[:flag]\n",
    "test = inputs[flag:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([141, 53]), torch.Size([141, 53, 200]), torch.Size([141]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_s = []\n",
    "t_e = []\n",
    "t_y = []\n",
    "for E,S,Y in test:\n",
    "    t_s.append(S)\n",
    "    t_e.append(E)\n",
    "    t_y.append(Y)\n",
    "t_s = torch.stack(t_s)\n",
    "t_e = torch.stack(t_e)\n",
    "t_y = torch.from_numpy(np.array(t_y))\n",
    "t_s.shape, t_e.shape, t_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.6879432624113475 67轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(  # input shape (1, 53, 200)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,      # input height\n",
    "                out_channels=53,    # n_filters\n",
    "                kernel_size=(1,201),      # filter size\n",
    "            ),      # output shape (16, 53, 200)\n",
    "            nn.ReLU(),    # activation\n",
    "            nn.MaxPool2d(kernel_size=(53,1))   # 在 2x2 空间里向下采样, output shape (16, 53, 1)\n",
    "        )\n",
    "#         self.out = nn.Linear(8480, 256)   # fully connected layer, output 10 classes\n",
    "        self.output = nn.Linear(53, 7)   # fully connected layer, output 10 classes\n",
    "\n",
    "    def forward(self, sen, x):\n",
    "        sen = sen.reshape((sen.shape[0],1,sen.shape[1])) # (BATCH_SIZE,1,53)\n",
    "        use = torch.bmm(sen.double(),x) # (BATCH_SIZE,1,200)\n",
    "        use = torch.sigmoid(use) # (BATCH_SIZE,1,200)\n",
    "        use = torch.softmax(use,2) # (BATCH_SIZE,1,200)\n",
    "        use = use.reshape((use.shape[0],-1,1))\n",
    "        use = torch.bmm(x,use)\n",
    "        x = torch.cat((x,use),2)\n",
    "        x = x.reshape((x.shape[0],1,x.shape[1],-1))\n",
    "        x = self.conv1(x.double())\n",
    "#         x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "#         sen = sen.reshape((sen.shape[0],1,sen.shape[1]))\n",
    "#         x = torch.cat((x,sen.double()),1)\n",
    "#         x = x.reshape((x.shape[0],x.shape[1],x.shape[2],1))\n",
    "#         x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)   # 展平\n",
    "        output = self.output(x.double())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 53, kernel_size=(1, 201), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(53, 1), stride=(53, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (output): Linear(in_features=53, out_features=7, bias=True)\n",
      ")\n",
      "1: accuracy:0.2695035460992908 loss: 1.8569216676813904 time: 0.21128606796264648\n",
      "2: accuracy:0.40425531914893614 loss: 1.6825805885609249 time: 0.42028069496154785\n",
      "3: accuracy:0.524822695035461 loss: 1.3348510334047614 time: 0.6306407451629639\n",
      "4: accuracy:0.5673758865248227 loss: 1.0152982538653457 time: 0.8642539978027344\n",
      "5: accuracy:0.5815602836879432 loss: 0.7744289725664666 time: 1.1330580711364746\n",
      "6: accuracy:0.6170212765957447 loss: 0.5916774058094098 time: 1.3383207321166992\n",
      "7: accuracy:0.6524822695035462 loss: 0.3927998208724457 time: 1.5535848140716553\n",
      "8: accuracy:0.6312056737588653 loss: 0.2420209302948902 time: 1.762558937072754\n",
      "9: accuracy:0.624113475177305 loss: 0.1598346492561813 time: 1.977104902267456\n",
      "10: accuracy:0.6524822695035462 loss: 0.1203642514514296 time: 2.1789848804473877\n",
      "11: accuracy:0.6382978723404256 loss: 0.07106730592947358 time: 2.382678985595703\n",
      "12: accuracy:0.6312056737588653 loss: 0.05448332661192279 time: 2.5832598209381104\n",
      "13: accuracy:0.6382978723404256 loss: 0.04514512300675133 time: 2.7861838340759277\n",
      "14: accuracy:0.6524822695035462 loss: 0.01756007628187143 time: 2.9874377250671387\n",
      "15: accuracy:0.6524822695035462 loss: 0.015415514113633996 time: 3.1891379356384277\n",
      "16: accuracy:0.6666666666666666 loss: 0.0152787075820826 time: 3.390206813812256\n",
      "17: accuracy:0.6666666666666666 loss: 0.012271452643777136 time: 3.5916857719421387\n",
      "18: accuracy:0.6737588652482269 loss: 0.007316785662023159 time: 3.8087427616119385\n",
      "19: accuracy:0.6595744680851063 loss: 0.007445042691373655 time: 4.116620063781738\n",
      "20: accuracy:0.6666666666666666 loss: 0.007074847736273084 time: 4.447479009628296\n",
      "21: accuracy:0.6737588652482269 loss: 0.00534956536959163 time: 4.662250995635986\n",
      "22: accuracy:0.6737588652482269 loss: 0.004730443847463972 time: 4.8656439781188965\n",
      "23: accuracy:0.6666666666666666 loss: 0.004779901199075346 time: 5.167708873748779\n",
      "24: accuracy:0.6666666666666666 loss: 0.004614487955975344 time: 5.374552011489868\n",
      "25: accuracy:0.6737588652482269 loss: 0.0038866252429894644 time: 5.580013036727905\n",
      "26: accuracy:0.6808510638297872 loss: 0.0033570751093184447 time: 5.822617769241333\n",
      "27: accuracy:0.6808510638297872 loss: 0.004109059277256843 time: 6.027710914611816\n",
      "28: accuracy:0.6808510638297872 loss: 0.002769156594040797 time: 6.235959053039551\n",
      "29: accuracy:0.6737588652482269 loss: 0.0031198492059344093 time: 6.451368808746338\n",
      "30: accuracy:0.6808510638297872 loss: 0.002905070946788462 time: 6.6706297397613525\n",
      "31: accuracy:0.6808510638297872 loss: 0.003267212612017456 time: 6.877065896987915\n",
      "32: accuracy:0.6808510638297872 loss: 0.002685703912901286 time: 7.081979751586914\n",
      "33: accuracy:0.6737588652482269 loss: 0.0025400807643332335 time: 7.367715835571289\n",
      "34: accuracy:0.6737588652482269 loss: 0.003055134639412265 time: 7.582429885864258\n",
      "35: accuracy:0.6666666666666666 loss: 0.002433941314726873 time: 7.785099983215332\n",
      "36: accuracy:0.6666666666666666 loss: 0.002212415247109156 time: 7.988384962081909\n",
      "37: accuracy:0.6666666666666666 loss: 0.002345504786413821 time: 8.204018831253052\n",
      "38: accuracy:0.6666666666666666 loss: 0.002212941482921923 time: 8.408320903778076\n",
      "39: accuracy:0.6737588652482269 loss: 0.0018834487349425214 time: 8.613985061645508\n",
      "40: accuracy:0.6737588652482269 loss: 0.002115792487907951 time: 8.838180780410767\n",
      "41: accuracy:0.6737588652482269 loss: 0.0017795233194017308 time: 9.04352593421936\n",
      "42: accuracy:0.6737588652482269 loss: 0.0019603668788128535 time: 9.319960117340088\n",
      "43: accuracy:0.6737588652482269 loss: 0.001956472779142945 time: 9.52988886833191\n",
      "44: accuracy:0.6737588652482269 loss: 0.0018469341474823915 time: 9.740861892700195\n",
      "45: accuracy:0.6666666666666666 loss: 0.0015332840771929185 time: 10.040591716766357\n",
      "46: accuracy:0.6666666666666666 loss: 0.0015707605706922884 time: 10.304282903671265\n",
      "47: accuracy:0.6666666666666666 loss: 0.0018483229069740758 time: 10.547261953353882\n",
      "48: accuracy:0.6666666666666666 loss: 0.0015226020797359447 time: 10.751380920410156\n",
      "49: accuracy:0.6666666666666666 loss: 0.0014047645797487438 time: 10.956986904144287\n",
      "50: accuracy:0.6737588652482269 loss: 0.0015657142964598668 time: 11.15879487991333\n",
      "51: accuracy:0.6737588652482269 loss: 0.0015595767828725927 time: 11.389894962310791\n",
      "52: accuracy:0.6737588652482269 loss: 0.0013343885149723178 time: 11.591815948486328\n",
      "53: accuracy:0.6737588652482269 loss: 0.0013584587821314235 time: 11.79274582862854\n",
      "54: accuracy:0.6737588652482269 loss: 0.0013435728061505995 time: 12.003957986831665\n",
      "55: accuracy:0.6737588652482269 loss: 0.001388596328045796 time: 12.214883804321289\n",
      "56: accuracy:0.6737588652482269 loss: 0.0015398620652898717 time: 12.41609787940979\n",
      "57: accuracy:0.6737588652482269 loss: 0.001288614226607405 time: 12.615484952926636\n",
      "58: accuracy:0.6737588652482269 loss: 0.0012187759683560577 time: 12.822449922561646\n",
      "59: accuracy:0.6737588652482269 loss: 0.0012452939820949383 time: 13.023559808731079\n",
      "60: accuracy:0.6737588652482269 loss: 0.001221123019914394 time: 13.226962804794312\n",
      "61: accuracy:0.6737588652482269 loss: 0.001291569814703156 time: 13.427834749221802\n",
      "62: accuracy:0.6737588652482269 loss: 0.0009974225489054055 time: 13.63132905960083\n",
      "63: accuracy:0.6737588652482269 loss: 0.0012328510716004969 time: 13.835905075073242\n",
      "64: accuracy:0.6737588652482269 loss: 0.0010176050908595714 time: 14.041492938995361\n",
      "65: accuracy:0.6737588652482269 loss: 0.0011412611790013421 time: 14.243643760681152\n",
      "66: accuracy:0.6808510638297872 loss: 0.0008866023849535942 time: 14.44532585144043\n",
      "67: accuracy:0.6879432624113475 loss: 0.0010139883884203599 time: 14.651310682296753\n",
      "68: accuracy:0.6879432624113475 loss: 0.0009785313822746897 time: 14.859800815582275\n",
      "69: accuracy:0.6879432624113475 loss: 0.0009451287059459318 time: 15.063732862472534\n",
      "70: accuracy:0.6879432624113475 loss: 0.0010373959576161797 time: 15.271765947341919\n",
      "71: accuracy:0.6879432624113475 loss: 0.0009025262130935907 time: 15.478618860244751\n",
      "72: accuracy:0.6879432624113475 loss: 0.0008349212310074349 time: 15.782964944839478\n",
      "73: accuracy:0.6879432624113475 loss: 0.0008007095041163608 time: 16.121424913406372\n",
      "74: accuracy:0.6879432624113475 loss: 0.0007884743932527551 time: 16.351382970809937\n",
      "75: accuracy:0.6879432624113475 loss: 0.0008556399827870185 time: 16.605193853378296\n",
      "76: accuracy:0.6879432624113475 loss: 0.0009954725241460745 time: 16.80980086326599\n",
      "77: accuracy:0.6879432624113475 loss: 0.0007715313758291106 time: 17.022738933563232\n",
      "78: accuracy:0.6879432624113475 loss: 0.0008773395886762181 time: 17.249008893966675\n",
      "79: accuracy:0.6879432624113475 loss: 0.0008634274810246621 time: 17.45511484146118\n",
      "80: accuracy:0.6879432624113475 loss: 0.0007155525860781736 time: 17.660045862197876\n",
      "81: accuracy:0.6879432624113475 loss: 0.0007499106428074497 time: 17.864635944366455\n",
      "82: accuracy:0.6879432624113475 loss: 0.0007938193934178146 time: 18.07254981994629\n",
      "83: accuracy:0.6879432624113475 loss: 0.0006988511158425581 time: 18.27490782737732\n",
      "84: accuracy:0.6879432624113475 loss: 0.0006706125810873789 time: 18.479188919067383\n",
      "85: accuracy:0.6879432624113475 loss: 0.0006764430498791845 time: 18.75752091407776\n",
      "86: accuracy:0.6879432624113475 loss: 0.0007430391805351861 time: 19.098721981048584\n",
      "87: accuracy:0.6879432624113475 loss: 0.0007507343823884278 time: 19.330605030059814\n",
      "88: accuracy:0.6879432624113475 loss: 0.000762968718977167 time: 19.53958296775818\n",
      "89: accuracy:0.6879432624113475 loss: 0.0006147682699791776 time: 19.855515956878662\n",
      "90: accuracy:0.6879432624113475 loss: 0.0006794729434813311 time: 20.06302285194397\n",
      "91: accuracy:0.6879432624113475 loss: 0.0006887188137263475 time: 20.27152681350708\n",
      "92: accuracy:0.6879432624113475 loss: 0.0006441310725517267 time: 20.47640299797058\n",
      "93: accuracy:0.6879432624113475 loss: 0.0006084972313920087 time: 20.76327395439148\n",
      "94: accuracy:0.6879432624113475 loss: 0.0005750499150875348 time: 20.989943027496338\n",
      "95: accuracy:0.6879432624113475 loss: 0.0006409621918329326 time: 21.197244882583618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96: accuracy:0.6879432624113475 loss: 0.00054747220472605 time: 21.43427085876465\n",
      "97: accuracy:0.6879432624113475 loss: 0.0005953486840670002 time: 21.677482843399048\n",
      "98: accuracy:0.6879432624113475 loss: 0.0005330939560191642 time: 21.89757800102234\n",
      "99: accuracy:0.6879432624113475 loss: 0.0005995558921987459 time: 22.106060028076172\n",
      "100: accuracy:0.6879432624113475 loss: 0.0005737499933822221 time: 22.314457893371582\n",
      "101: accuracy:0.6879432624113475 loss: 0.0005431025661342633 time: 22.586254835128784\n",
      "102: accuracy:0.6879432624113475 loss: 0.0005468784422710624 time: 22.793956756591797\n",
      "103: accuracy:0.6879432624113475 loss: 0.0005449203312240668 time: 23.00202488899231\n",
      "104: accuracy:0.6879432624113475 loss: 0.0005402685343339288 time: 23.29014301300049\n",
      "105: accuracy:0.6879432624113475 loss: 0.0005319915355803165 time: 23.525513887405396\n",
      "106: accuracy:0.6879432624113475 loss: 0.0005349829756870277 time: 23.742321968078613\n",
      "107: accuracy:0.6879432624113475 loss: 0.0005480319638433302 time: 23.955850839614868\n",
      "108: accuracy:0.6879432624113475 loss: 0.0004937316518320094 time: 24.200868844985962\n",
      "109: accuracy:0.6879432624113475 loss: 0.00048289471036633914 time: 24.417950868606567\n",
      "110: accuracy:0.6879432624113475 loss: 0.0005097851316298728 time: 24.61998176574707\n",
      "111: accuracy:0.6879432624113475 loss: 0.0004798229958945299 time: 24.821533918380737\n",
      "112: accuracy:0.6879432624113475 loss: 0.00041462339487989195 time: 25.031089782714844\n",
      "113: accuracy:0.6879432624113475 loss: 0.0004579740623741811 time: 25.239135026931763\n",
      "114: accuracy:0.6879432624113475 loss: 0.0004232053957494105 time: 25.441449880599976\n",
      "115: accuracy:0.6879432624113475 loss: 0.00045008157827087304 time: 25.645692825317383\n",
      "116: accuracy:0.6879432624113475 loss: 0.0004268621816906507 time: 25.849597930908203\n",
      "117: accuracy:0.6879432624113475 loss: 0.00044585239685778687 time: 26.050328016281128\n",
      "118: accuracy:0.6879432624113475 loss: 0.00046185773080639997 time: 26.248222827911377\n",
      "119: accuracy:0.6879432624113475 loss: 0.00048297744680699645 time: 26.45062780380249\n",
      "120: accuracy:0.6879432624113475 loss: 0.0004422260813160201 time: 26.653742790222168\n",
      "121: accuracy:0.6879432624113475 loss: 0.000551682074851215 time: 26.85563087463379\n",
      "122: accuracy:0.6879432624113475 loss: 0.00043150672819374235 time: 27.06531572341919\n",
      "123: accuracy:0.6879432624113475 loss: 0.00041712577450821544 time: 27.268980026245117\n",
      "124: accuracy:0.6879432624113475 loss: 0.0004119889091122962 time: 27.47252893447876\n",
      "125: accuracy:0.6879432624113475 loss: 0.0003448156115992188 time: 27.69006896018982\n",
      "126: accuracy:0.6879432624113475 loss: 0.0004554958049904404 time: 27.91041088104248\n",
      "127: accuracy:0.6879432624113475 loss: 0.00040890111509164534 time: 28.11896586418152\n",
      "128: accuracy:0.6879432624113475 loss: 0.00040232417188885706 time: 28.338234901428223\n",
      "129: accuracy:0.6879432624113475 loss: 0.00041060103074355784 time: 28.547231912612915\n",
      "130: accuracy:0.6879432624113475 loss: 0.00035391882972642157 time: 28.757487773895264\n",
      "131: accuracy:0.6879432624113475 loss: 0.000339740857355927 time: 28.983120918273926\n",
      "132: accuracy:0.6879432624113475 loss: 0.00034738048398893335 time: 29.196292877197266\n",
      "133: accuracy:0.6879432624113475 loss: 0.00033298754274257037 time: 29.40069890022278\n",
      "134: accuracy:0.6879432624113475 loss: 0.0003576380233354191 time: 29.6455659866333\n",
      "135: accuracy:0.6879432624113475 loss: 0.00033508785413273657 time: 29.859916925430298\n",
      "136: accuracy:0.6879432624113475 loss: 0.00034900942762028055 time: 30.06673789024353\n",
      "137: accuracy:0.6879432624113475 loss: 0.00037235669609549305 time: 30.281970977783203\n",
      "138: accuracy:0.6879432624113475 loss: 0.00028479735806902695 time: 30.50126004219055\n",
      "139: accuracy:0.6879432624113475 loss: 0.00037384097763339954 time: 30.728933811187744\n",
      "140: accuracy:0.6879432624113475 loss: 0.00036175858075339866 time: 30.95081877708435\n",
      "141: accuracy:0.6879432624113475 loss: 0.0003148345609961721 time: 31.176862955093384\n",
      "142: accuracy:0.6879432624113475 loss: 0.0003805564272209239 time: 31.389808893203735\n",
      "143: accuracy:0.6879432624113475 loss: 0.00032378558566571616 time: 31.63638973236084\n",
      "144: accuracy:0.6879432624113475 loss: 0.0003089668139565635 time: 31.875354766845703\n",
      "145: accuracy:0.6879432624113475 loss: 0.0002758798696781893 time: 32.10457396507263\n",
      "146: accuracy:0.6879432624113475 loss: 0.00027479798803073304 time: 32.32519292831421\n",
      "147: accuracy:0.6879432624113475 loss: 0.0003075323845007892 time: 32.55127286911011\n",
      "148: accuracy:0.6879432624113475 loss: 0.00033939301782583343 time: 32.773902893066406\n",
      "149: accuracy:0.6879432624113475 loss: 0.0003093860084629847 time: 33.0015287399292\n",
      "150: accuracy:0.6879432624113475 loss: 0.00028954106488092077 time: 33.22190475463867\n",
      "151: accuracy:0.6879432624113475 loss: 0.0002686379077686699 time: 33.44996690750122\n",
      "152: accuracy:0.6879432624113475 loss: 0.00026879370692817363 time: 33.669933795928955\n",
      "153: accuracy:0.6879432624113475 loss: 0.0003189662223987406 time: 33.89919471740723\n",
      "154: accuracy:0.6879432624113475 loss: 0.0003089772701102841 time: 34.12214493751526\n",
      "155: accuracy:0.6879432624113475 loss: 0.00027508603305646925 time: 34.35199785232544\n",
      "156: accuracy:0.6879432624113475 loss: 0.0002626822091921126 time: 34.57649779319763\n",
      "157: accuracy:0.6879432624113475 loss: 0.0002807839164883141 time: 34.800827741622925\n",
      "158: accuracy:0.6879432624113475 loss: 0.00029412597977174215 time: 35.023849964141846\n",
      "159: accuracy:0.6879432624113475 loss: 0.00024378730688730295 time: 35.245606899261475\n",
      "160: accuracy:0.6879432624113475 loss: 0.0002466538503222918 time: 35.47117280960083\n",
      "161: accuracy:0.6879432624113475 loss: 0.00027968925657474285 time: 35.694732904434204\n",
      "162: accuracy:0.6879432624113475 loss: 0.00023670902279003973 time: 35.917672872543335\n",
      "163: accuracy:0.6879432624113475 loss: 0.0002856683847237029 time: 36.140132904052734\n",
      "164: accuracy:0.6879432624113475 loss: 0.0002442472525924063 time: 36.36069893836975\n",
      "165: accuracy:0.6879432624113475 loss: 0.0002777288279976808 time: 36.56224489212036\n",
      "166: accuracy:0.6879432624113475 loss: 0.00023801961895661655 time: 36.758094787597656\n",
      "167: accuracy:0.6879432624113475 loss: 0.00025627605264949083 time: 36.9634690284729\n",
      "168: accuracy:0.6879432624113475 loss: 0.0002564157492539465 time: 37.233869791030884\n",
      "169: accuracy:0.6879432624113475 loss: 0.00022098592773537158 time: 37.44686698913574\n",
      "170: accuracy:0.6879432624113475 loss: 0.0002712442512080645 time: 37.65048694610596\n",
      "171: accuracy:0.6879432624113475 loss: 0.00027280734576240153 time: 37.86138892173767\n",
      "172: accuracy:0.6879432624113475 loss: 0.0002422190546015303 time: 38.1437828540802\n",
      "173: accuracy:0.6879432624113475 loss: 0.00021782571285672452 time: 38.37001276016235\n",
      "174: accuracy:0.6879432624113475 loss: 0.0002309055314480319 time: 38.58449578285217\n",
      "175: accuracy:0.6879432624113475 loss: 0.00022086842566531673 time: 38.78628182411194\n",
      "176: accuracy:0.6879432624113475 loss: 0.0002020556188750966 time: 38.99171781539917\n",
      "177: accuracy:0.6879432624113475 loss: 0.0002442512723804389 time: 39.23769187927246\n",
      "178: accuracy:0.6879432624113475 loss: 0.00026676126791847263 time: 39.45765995979309\n",
      "179: accuracy:0.6879432624113475 loss: 0.0002149777163550962 time: 39.671939849853516\n",
      "180: accuracy:0.6879432624113475 loss: 0.0002170292513834049 time: 39.902724742889404\n",
      "181: accuracy:0.6879432624113475 loss: 0.00022131423744248735 time: 40.1167197227478\n",
      "182: accuracy:0.6879432624113475 loss: 0.00021341983301596454 time: 40.33711791038513\n",
      "183: accuracy:0.6879432624113475 loss: 0.00023465867430676467 time: 40.556015729904175\n",
      "184: accuracy:0.6879432624113475 loss: 0.0002225773019714542 time: 40.773056983947754\n",
      "185: accuracy:0.6879432624113475 loss: 0.00019883099406053768 time: 40.98774075508118\n",
      "186: accuracy:0.6879432624113475 loss: 0.0002317079009005114 time: 41.20811176300049\n",
      "187: accuracy:0.6879432624113475 loss: 0.0001923030235831672 time: 41.41980600357056\n",
      "188: accuracy:0.6879432624113475 loss: 0.00019529938145296404 time: 41.636446952819824\n",
      "189: accuracy:0.6879432624113475 loss: 0.00020269746102420224 time: 41.852356910705566\n",
      "190: accuracy:0.6879432624113475 loss: 0.00017738936589688185 time: 42.068795919418335\n",
      "191: accuracy:0.6879432624113475 loss: 0.0001915228620575122 time: 42.28238892555237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192: accuracy:0.6879432624113475 loss: 0.0002306925926394397 time: 42.49889588356018\n",
      "193: accuracy:0.6879432624113475 loss: 0.0001642543347683529 time: 42.70416188240051\n",
      "194: accuracy:0.6879432624113475 loss: 0.00018575276261474373 time: 42.95573973655701\n",
      "195: accuracy:0.6879432624113475 loss: 0.00020266461195188917 time: 43.187559843063354\n",
      "196: accuracy:0.6879432624113475 loss: 0.00017212599014295092 time: 43.390012979507446\n",
      "197: accuracy:0.6879432624113475 loss: 0.00018630702974139015 time: 43.59045886993408\n",
      "198: accuracy:0.6879432624113475 loss: 0.00021025028876033857 time: 43.789438009262085\n",
      "199: accuracy:0.6879432624113475 loss: 0.00015874549797098174 time: 43.99490308761597\n",
      "200: accuracy:0.6879432624113475 loss: 0.00018026706444263597 time: 44.19948697090149\n",
      "201: accuracy:0.6879432624113475 loss: 0.00019364810513414606 time: 44.40020680427551\n",
      "202: accuracy:0.6879432624113475 loss: 0.0001813422987651668 time: 44.60036873817444\n",
      "203: accuracy:0.6879432624113475 loss: 0.00016518029053910359 time: 44.803252935409546\n",
      "204: accuracy:0.6879432624113475 loss: 0.00017132496635042317 time: 45.00260877609253\n",
      "205: accuracy:0.6879432624113475 loss: 0.00017621075629881133 time: 45.204341888427734\n",
      "206: accuracy:0.6879432624113475 loss: 0.00017143337836085238 time: 45.40555286407471\n",
      "207: accuracy:0.6879432624113475 loss: 0.00018764501951400446 time: 45.64528203010559\n",
      "208: accuracy:0.6879432624113475 loss: 0.0001510982289554898 time: 45.85261082649231\n",
      "209: accuracy:0.6879432624113475 loss: 0.0001970818875630696 time: 46.06281089782715\n",
      "210: accuracy:0.6879432624113475 loss: 0.0001956763389645693 time: 46.27695894241333\n",
      "211: accuracy:0.6879432624113475 loss: 0.00018691146560948977 time: 46.48617076873779\n",
      "212: accuracy:0.6879432624113475 loss: 0.00017760467101884153 time: 46.71357083320618\n",
      "213: accuracy:0.6879432624113475 loss: 0.00017912469442906697 time: 46.93073797225952\n",
      "214: accuracy:0.6879432624113475 loss: 0.00018511623241734197 time: 47.13451170921326\n",
      "215: accuracy:0.6879432624113475 loss: 0.00017446364908364488 time: 47.34181499481201\n",
      "216: accuracy:0.6879432624113475 loss: 0.00016951876305320974 time: 47.54071497917175\n",
      "217: accuracy:0.6879432624113475 loss: 0.0001722388896037787 time: 47.74444389343262\n",
      "218: accuracy:0.6879432624113475 loss: 0.00018583251188395117 time: 47.946268796920776\n",
      "219: accuracy:0.6879432624113475 loss: 0.00017735585817185546 time: 48.15178179740906\n",
      "220: accuracy:0.6879432624113475 loss: 0.0001530103357951213 time: 48.35089373588562\n",
      "221: accuracy:0.6879432624113475 loss: 0.0001500221120460919 time: 48.55895781517029\n",
      "222: accuracy:0.6879432624113475 loss: 0.0001499804067678515 time: 48.764642000198364\n",
      "223: accuracy:0.6879432624113475 loss: 0.0001557041310706886 time: 48.967522859573364\n",
      "224: accuracy:0.6879432624113475 loss: 0.0001721318817488969 time: 49.17192077636719\n",
      "225: accuracy:0.6879432624113475 loss: 0.00015606286908862138 time: 49.385562896728516\n",
      "226: accuracy:0.6879432624113475 loss: 0.0001608092857838298 time: 49.58980202674866\n",
      "227: accuracy:0.6879432624113475 loss: 0.00014514245542212985 time: 49.80413889884949\n",
      "228: accuracy:0.6879432624113475 loss: 0.00013455354299084986 time: 50.013816833496094\n",
      "229: accuracy:0.6879432624113475 loss: 0.00014575740311823437 time: 50.220494747161865\n",
      "230: accuracy:0.6879432624113475 loss: 0.00014437975500705135 time: 50.42331290245056\n",
      "231: accuracy:0.6879432624113475 loss: 0.00012612008498089316 time: 50.62455773353577\n",
      "232: accuracy:0.6879432624113475 loss: 0.0001443334051057502 time: 50.83110284805298\n",
      "233: accuracy:0.6879432624113475 loss: 0.0001571017970432972 time: 51.03484582901001\n",
      "234: accuracy:0.6879432624113475 loss: 0.00014475088075828677 time: 51.2341570854187\n",
      "235: accuracy:0.6879432624113475 loss: 0.00013855644871939735 time: 51.45919108390808\n",
      "236: accuracy:0.6879432624113475 loss: 0.00013422532425183065 time: 51.68803691864014\n",
      "237: accuracy:0.6879432624113475 loss: 0.00013397157364074223 time: 51.91479778289795\n",
      "238: accuracy:0.6879432624113475 loss: 0.00013065131383674456 time: 52.150742053985596\n",
      "239: accuracy:0.6879432624113475 loss: 0.00015273461512246606 time: 52.38543391227722\n",
      "240: accuracy:0.6879432624113475 loss: 0.00012663396608883254 time: 52.58828902244568\n",
      "241: accuracy:0.6879432624113475 loss: 0.00013209608201140302 time: 52.793622970581055\n",
      "242: accuracy:0.6879432624113475 loss: 0.00013271724340588876 time: 53.02381181716919\n",
      "243: accuracy:0.6879432624113475 loss: 0.00013308513769265672 time: 53.22666788101196\n",
      "244: accuracy:0.6879432624113475 loss: 0.00012102475495727104 time: 53.455808877944946\n",
      "245: accuracy:0.6879432624113475 loss: 0.000132564973132305 time: 53.65685200691223\n",
      "246: accuracy:0.6879432624113475 loss: 0.00014313813390886158 time: 53.85686182975769\n",
      "247: accuracy:0.6879432624113475 loss: 0.00011143898424510467 time: 54.06270170211792\n",
      "248: accuracy:0.6879432624113475 loss: 0.0001331041915806733 time: 54.348442792892456\n",
      "249: accuracy:0.6879432624113475 loss: 0.00014137359602620794 time: 54.57079195976257\n",
      "250: accuracy:0.6879432624113475 loss: 0.00014051525828748167 time: 54.76918983459473\n",
      "251: accuracy:0.6879432624113475 loss: 0.00012855873042588888 time: 54.97288393974304\n",
      "252: accuracy:0.6879432624113475 loss: 0.00012045184839273304 time: 55.17411780357361\n",
      "253: accuracy:0.6879432624113475 loss: 0.00013093382507479236 time: 55.3734610080719\n",
      "254: accuracy:0.6879432624113475 loss: 0.00011349085268787766 time: 55.57150387763977\n",
      "255: accuracy:0.6879432624113475 loss: 0.000125739956342016 time: 55.772544860839844\n",
      "256: accuracy:0.6879432624113475 loss: 0.00011863306806392566 time: 55.987106800079346\n",
      "257: accuracy:0.6879432624113475 loss: 0.0001286944288583796 time: 56.248146772384644\n",
      "258: accuracy:0.6879432624113475 loss: 0.0001279834138538684 time: 56.45183992385864\n",
      "259: accuracy:0.6879432624113475 loss: 0.0001082347375836409 time: 56.653388023376465\n",
      "260: accuracy:0.6879432624113475 loss: 0.00012563723821266138 time: 56.85643982887268\n",
      "261: accuracy:0.6879432624113475 loss: 0.0001283665443480559 time: 57.06652593612671\n",
      "262: accuracy:0.6879432624113475 loss: 0.00012634480720097636 time: 57.290512800216675\n",
      "263: accuracy:0.6879432624113475 loss: 9.230241330396691e-05 time: 57.496158838272095\n",
      "264: accuracy:0.6879432624113475 loss: 0.00011345570427587453 time: 57.71320605278015\n",
      "265: accuracy:0.6879432624113475 loss: 0.0001189845328119356 time: 57.95587086677551\n",
      "266: accuracy:0.6879432624113475 loss: 0.00010202852489324954 time: 58.15500593185425\n",
      "267: accuracy:0.6879432624113475 loss: 0.00011781384822144858 time: 58.353641986846924\n",
      "268: accuracy:0.6879432624113475 loss: 0.000124226666769367 time: 58.56175374984741\n",
      "269: accuracy:0.6879432624113475 loss: 0.00010805866255762758 time: 58.77540874481201\n",
      "270: accuracy:0.6879432624113475 loss: 0.00012203868393049687 time: 58.97374892234802\n",
      "271: accuracy:0.6879432624113475 loss: 0.00011355729211618585 time: 59.181368827819824\n",
      "272: accuracy:0.6879432624113475 loss: 0.00011900484214641643 time: 59.38593292236328\n",
      "273: accuracy:0.6879432624113475 loss: 0.00011067135270740484 time: 59.58343696594238\n",
      "274: accuracy:0.6879432624113475 loss: 0.0001108866721032799 time: 59.78111386299133\n",
      "275: accuracy:0.6879432624113475 loss: 0.00011247469345877344 time: 59.979498863220215\n",
      "276: accuracy:0.6879432624113475 loss: 0.00010806522765240649 time: 60.1826388835907\n",
      "277: accuracy:0.6879432624113475 loss: 0.00010340611009272495 time: 60.384095907211304\n",
      "278: accuracy:0.6879432624113475 loss: 8.837630497593452e-05 time: 60.587565660476685\n",
      "279: accuracy:0.6879432624113475 loss: 0.0001207710690029451 time: 60.78956079483032\n",
      "280: accuracy:0.6879432624113475 loss: 0.00010093910527366745 time: 60.99989676475525\n",
      "281: accuracy:0.6879432624113475 loss: 0.00010451229704574619 time: 61.20417881011963\n",
      "282: accuracy:0.6879432624113475 loss: 0.00010301939694562025 time: 61.407320737838745\n",
      "283: accuracy:0.6879432624113475 loss: 8.9098874560088e-05 time: 61.60350179672241\n",
      "284: accuracy:0.6879432624113475 loss: 0.00010529284402361735 time: 61.80323100090027\n",
      "285: accuracy:0.6879432624113475 loss: 9.673843120161507e-05 time: 62.004992961883545\n",
      "286: accuracy:0.6879432624113475 loss: 9.693573898196409e-05 time: 62.205190896987915\n",
      "287: accuracy:0.6879432624113475 loss: 0.00010450231948232311 time: 62.4044349193573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288: accuracy:0.6879432624113475 loss: 9.770422985541569e-05 time: 62.601089000701904\n",
      "289: accuracy:0.6879432624113475 loss: 0.00010565254081969907 time: 62.799152851104736\n",
      "290: accuracy:0.6879432624113475 loss: 9.878719426226518e-05 time: 62.99910092353821\n",
      "291: accuracy:0.6879432624113475 loss: 0.00010181135698026367 time: 63.19698095321655\n",
      "292: accuracy:0.6879432624113475 loss: 0.00010676015214592383 time: 63.39206290245056\n",
      "293: accuracy:0.6879432624113475 loss: 0.00010600478555483152 time: 63.587785959243774\n",
      "294: accuracy:0.6879432624113475 loss: 8.953990845533357e-05 time: 63.78749179840088\n",
      "295: accuracy:0.6879432624113475 loss: 9.220393231011822e-05 time: 63.9874050617218\n",
      "296: accuracy:0.6879432624113475 loss: 9.295801770551496e-05 time: 64.1849467754364\n",
      "297: accuracy:0.6879432624113475 loss: 8.087584969154499e-05 time: 64.38666701316833\n",
      "298: accuracy:0.6879432624113475 loss: 7.981540654078095e-05 time: 64.59302496910095\n",
      "299: accuracy:0.6879432624113475 loss: 9.60697712971026e-05 time: 64.79944968223572\n",
      "300: accuracy:0.6879432624113475 loss: 9.181585074925026e-05 time: 65.00117778778076\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    ")\n",
    "net = CNN()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "start = time.time()\n",
    "for epoch in range(1,301): \n",
    "    for step, (batch_x, sen, batch_y) in enumerate(loader):\n",
    "        torch.set_default_dtype(torch.double)\n",
    "        pred = net(sen, batch_x.double())\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 2: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_s , t_e)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss} time: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
