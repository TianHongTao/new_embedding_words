{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function set_backend in module tensorly:\n",
      "\n",
      "set_backend(backend_name)\n",
      "    Sets the backend for TensorLy\n",
      "    \n",
      "        The backend will be set as specified and operations will used that backend\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    backend_name : {'mxnet', 'numpy', 'pytorch', 'tensorflow', 'cupy'}, default is 'numpy'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pytorch backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.decomposition import tucker\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import preprocessing\n",
    "from random import shuffle\n",
    "BATCH_SIZE = 128\n",
    "tl.set_backend('pytorch')\n",
    "help(tl.set_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467,)\n"
     ]
    }
   ],
   "source": [
    "tag = [72, 71, 84, 54, 57, 65, 64]\n",
    "Y = np.zeros(467)\n",
    "print(Y.shape)\n",
    "Y[0 : tag[0]] = 0\n",
    "Y[tag[0] : tag[0]+tag[1]] = 1\n",
    "Y[tag[0]+tag[1] : tag[0]+tag[1]+tag[2]] = 2\n",
    "Y[tag[0]+tag[1]+tag[2] : tag[0]+tag[1]+tag[2]+tag[3]] = 3\n",
    "Y[tag[0]+tag[1]+tag[2]+tag[3] : tag[0]+tag[1]+tag[2]+tag[3]+tag[4]] = 4\n",
    "Y[tag[0]+tag[1]+tag[2]+tag[3]+tag[4] : tag[0]+tag[1]+tag[2]+tag[3]+tag[4]+tag[5]] = 5\n",
    "Y[tag[0]+tag[1]+tag[2]+tag[3]+tag[4]+tag[5] : tag[0]+tag[1]+tag[2]+tag[3]+tag[4]+tag[5]+tag[6]] = 6\n",
    "Y = Y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "max_length = 0\n",
    "for i in range(0,467):\n",
    "    a = np.loadtxt('/Users/denhiroshi/Downloads/Embeding/ans_1.0_'+str(i)+'.csv',dtype=np.float64,delimiter=',')\n",
    "    a = a.transpose(1,0)\n",
    "    a = np.exp(a)\n",
    "    a = np.sum(a,axis=0)\n",
    "    a = np.log(a)\n",
    "    inputs.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use = np.stack(inputs)\n",
    "mean = use.mean()         #计算平均数\n",
    "deviation = use.std()     #计算标准差\n",
    "# 标准化数据的公式: (数据值 - 平均数) / 标准差\n",
    "use = (use - mean) / deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.fromfile('glove_WV.dat',dtype=np.float64).reshape((467,-1,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((467, 53, 200), (467, 467))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape,use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for i in range(467):\n",
    "    inputs.append((torch.from_numpy(b[i]),torch.from_numpy(use[i]),Y[i]))\n",
    "shuffle(inputs)\n",
    "flag = int(len(inputs)*0.7)\n",
    "train = inputs[:flag]\n",
    "test = inputs[flag:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([141, 467]), torch.Size([141, 53, 200]), torch.Size([141]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_s = []\n",
    "t_e = []\n",
    "t_y = []\n",
    "for E,S,Y in test:\n",
    "    t_s.append(S)\n",
    "    t_e.append(E)\n",
    "    t_y.append(Y)\n",
    "t_s = torch.stack(t_s)\n",
    "t_e = torch.stack(t_e)\n",
    "t_y = torch.from_numpy(np.array(t_y))\n",
    "t_s.shape, t_e.shape, t_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glove_rnn_max: 0.6524822695035462 65轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN_max(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size = 200,       # rnn input\n",
    "            hidden_size = 64,     # rnn hidden unit\n",
    "            num_layers = 1,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=64,out_features=7)\n",
    "        self.maxpool = torch.nn.MaxPool2d((53,1))\n",
    "\n",
    "    def forward(self,sen,x):\n",
    "        output,h_n=self.rnn(x.float())\n",
    "        output_in_last_timestep = self.maxpool(output)\n",
    "        output_in_last_timestep = output_in_last_timestep.reshape((output_in_last_timestep.shape[0],-1))\n",
    "        x=self.out(output_in_last_timestep)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_max(\n",
      "  (rnn): GRU(200, 64, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=7, bias=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=(53, 1), stride=(53, 1), padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "1: accuracy:0.2978723404255319 loss: 1.8562443256378174\n",
      "2: accuracy:0.4326241134751773 loss: 1.6639606952667236\n",
      "3: accuracy:0.5106382978723404 loss: 1.3970987796783447\n",
      "4: accuracy:0.475177304964539 loss: 1.1561895608901978\n",
      "5: accuracy:0.5460992907801419 loss: 0.9396651983261108\n",
      "6: accuracy:0.5815602836879432 loss: 0.5332620739936829\n",
      "7: accuracy:0.6028368794326241 loss: 0.4410622715950012\n",
      "8: accuracy:0.5957446808510638 loss: 0.32944566011428833\n",
      "9: accuracy:0.6099290780141844 loss: 0.31363824009895325\n",
      "10: accuracy:0.6028368794326241 loss: 0.16599880158901215\n",
      "11: accuracy:0.5957446808510638 loss: 0.10138383507728577\n",
      "12: accuracy:0.5886524822695035 loss: 0.06081625446677208\n",
      "13: accuracy:0.5815602836879432 loss: 0.040260542184114456\n",
      "14: accuracy:0.6028368794326241 loss: 0.022472718730568886\n",
      "15: accuracy:0.6382978723404256 loss: 0.02062777802348137\n",
      "16: accuracy:0.6028368794326241 loss: 0.024989675730466843\n",
      "17: accuracy:0.5815602836879432 loss: 0.0187086071819067\n",
      "18: accuracy:0.6312056737588653 loss: 0.036783475428819656\n",
      "19: accuracy:0.6099290780141844 loss: 0.012902934104204178\n",
      "20: accuracy:0.6170212765957447 loss: 0.010365704074501991\n",
      "21: accuracy:0.5886524822695035 loss: 0.025407249107956886\n",
      "22: accuracy:0.6028368794326241 loss: 0.010996444150805473\n",
      "23: accuracy:0.6170212765957447 loss: 0.011798040941357613\n",
      "24: accuracy:0.6453900709219859 loss: 0.011191974394023418\n",
      "25: accuracy:0.6382978723404256 loss: 0.008573546074330807\n",
      "26: accuracy:0.6382978723404256 loss: 0.00623344024643302\n",
      "27: accuracy:0.6099290780141844 loss: 0.005442217458039522\n",
      "28: accuracy:0.5957446808510638 loss: 0.006324172019958496\n",
      "29: accuracy:0.5886524822695035 loss: 0.005293542984873056\n",
      "30: accuracy:0.6099290780141844 loss: 0.004464115481823683\n",
      "31: accuracy:0.6099290780141844 loss: 0.0037377357948571444\n",
      "32: accuracy:0.6099290780141844 loss: 0.003154318779706955\n",
      "33: accuracy:0.6099290780141844 loss: 0.0037819112185388803\n",
      "34: accuracy:0.6170212765957447 loss: 0.003343548160046339\n",
      "35: accuracy:0.624113475177305 loss: 0.002695471979677677\n",
      "36: accuracy:0.624113475177305 loss: 0.0033752305898815393\n",
      "37: accuracy:0.624113475177305 loss: 0.002302994020283222\n",
      "38: accuracy:0.624113475177305 loss: 0.002807188080623746\n",
      "39: accuracy:0.6170212765957447 loss: 0.002150133717805147\n",
      "40: accuracy:0.6099290780141844 loss: 0.0022459847386926413\n",
      "41: accuracy:0.6099290780141844 loss: 0.002120065735653043\n",
      "42: accuracy:0.6099290780141844 loss: 0.0016064438968896866\n",
      "43: accuracy:0.6099290780141844 loss: 0.0019313744269311428\n",
      "44: accuracy:0.6099290780141844 loss: 0.0017296654405072331\n",
      "45: accuracy:0.6028368794326241 loss: 0.0014839989598840475\n",
      "46: accuracy:0.6028368794326241 loss: 0.0014242308679968119\n",
      "47: accuracy:0.6028368794326241 loss: 0.0015537533909082413\n",
      "48: accuracy:0.5957446808510638 loss: 0.0014225005870684981\n",
      "49: accuracy:0.5957446808510638 loss: 0.001209041103720665\n",
      "50: accuracy:0.6028368794326241 loss: 0.0012413705699145794\n",
      "51: accuracy:0.6028368794326241 loss: 0.0012567451922222972\n",
      "52: accuracy:0.6028368794326241 loss: 0.0019057750469073653\n",
      "53: accuracy:0.6028368794326241 loss: 0.0013217857340350747\n",
      "54: accuracy:0.6028368794326241 loss: 0.0012926578056067228\n",
      "55: accuracy:0.5957446808510638 loss: 0.0016641616821289062\n",
      "56: accuracy:0.5957446808510638 loss: 0.0010598455555737019\n",
      "57: accuracy:0.5957446808510638 loss: 0.0012500217417255044\n",
      "58: accuracy:0.5957446808510638 loss: 0.001581008080393076\n",
      "59: accuracy:0.5957446808510638 loss: 0.0011106899473816156\n",
      "60: accuracy:0.5957446808510638 loss: 0.0010257108369842172\n",
      "61: accuracy:0.5957446808510638 loss: 0.0008332456927746534\n",
      "62: accuracy:0.5957446808510638 loss: 0.0011146137258037925\n",
      "63: accuracy:0.5957446808510638 loss: 0.0010018825996667147\n",
      "64: accuracy:0.5957446808510638 loss: 0.0010247161844745278\n",
      "65: accuracy:0.5957446808510638 loss: 0.0012410572962835431\n",
      "66: accuracy:0.5957446808510638 loss: 0.0009549549431540072\n",
      "67: accuracy:0.5957446808510638 loss: 0.0009021827136166394\n",
      "68: accuracy:0.5957446808510638 loss: 0.0010393960401415825\n",
      "69: accuracy:0.5957446808510638 loss: 0.0007437092717736959\n",
      "70: accuracy:0.5957446808510638 loss: 0.0010264600859954953\n",
      "71: accuracy:0.5957446808510638 loss: 0.0009558268939144909\n",
      "72: accuracy:0.5957446808510638 loss: 0.0007881845813244581\n",
      "73: accuracy:0.5957446808510638 loss: 0.0008464608690701425\n",
      "74: accuracy:0.5957446808510638 loss: 0.000657524389680475\n",
      "75: accuracy:0.5957446808510638 loss: 0.0008266585064120591\n",
      "76: accuracy:0.5957446808510638 loss: 0.0006381170824170113\n",
      "77: accuracy:0.5957446808510638 loss: 0.0005817753844894469\n",
      "78: accuracy:0.5957446808510638 loss: 0.0007659162511117756\n",
      "79: accuracy:0.5957446808510638 loss: 0.0009144578943960369\n",
      "80: accuracy:0.5957446808510638 loss: 0.00060454779304564\n",
      "81: accuracy:0.5957446808510638 loss: 0.0006916931597515941\n",
      "82: accuracy:0.5957446808510638 loss: 0.0006880760192871094\n",
      "83: accuracy:0.5957446808510638 loss: 0.0007449627155438066\n",
      "84: accuracy:0.5957446808510638 loss: 0.0006468091742135584\n",
      "85: accuracy:0.5957446808510638 loss: 0.0007750375079922378\n",
      "86: accuracy:0.5957446808510638 loss: 0.0007014070288278162\n",
      "87: accuracy:0.5957446808510638 loss: 0.0008552142535336316\n",
      "88: accuracy:0.5957446808510638 loss: 0.0006069728406146169\n",
      "89: accuracy:0.5957446808510638 loss: 0.0006254604668356478\n",
      "90: accuracy:0.5957446808510638 loss: 0.000550869561266154\n",
      "91: accuracy:0.5957446808510638 loss: 0.0005070141633041203\n",
      "92: accuracy:0.5957446808510638 loss: 0.0004164423153270036\n",
      "93: accuracy:0.5957446808510638 loss: 0.0005621092859655619\n",
      "94: accuracy:0.5957446808510638 loss: 0.0005438600201159716\n",
      "95: accuracy:0.5957446808510638 loss: 0.0006783621502108872\n",
      "96: accuracy:0.5957446808510638 loss: 0.00045661244075745344\n",
      "97: accuracy:0.5957446808510638 loss: 0.000502627226524055\n",
      "98: accuracy:0.5957446808510638 loss: 0.0005500452825799584\n",
      "99: accuracy:0.5957446808510638 loss: 0.0005106789758428931\n",
      "100: accuracy:0.5957446808510638 loss: 0.0004943984095007181\n",
      "101: accuracy:0.5957446808510638 loss: 0.00041733469697646797\n",
      "102: accuracy:0.5957446808510638 loss: 0.00047565187560394406\n",
      "103: accuracy:0.5957446808510638 loss: 0.0006961277686059475\n",
      "104: accuracy:0.5957446808510638 loss: 0.00036103386082686484\n",
      "105: accuracy:0.5957446808510638 loss: 0.00048567907651886344\n",
      "106: accuracy:0.5957446808510638 loss: 0.0004937512567266822\n",
      "107: accuracy:0.5957446808510638 loss: 0.0005227293004281819\n",
      "108: accuracy:0.5957446808510638 loss: 0.00041097914800047874\n",
      "109: accuracy:0.5957446808510638 loss: 0.00048300198977813125\n",
      "110: accuracy:0.5957446808510638 loss: 0.0003845487372018397\n",
      "111: accuracy:0.5957446808510638 loss: 0.0003960132598876953\n",
      "112: accuracy:0.5957446808510638 loss: 0.0003940582391805947\n",
      "113: accuracy:0.5957446808510638 loss: 0.0004250117635820061\n",
      "114: accuracy:0.5957446808510638 loss: 0.0003176621103193611\n",
      "115: accuracy:0.5957446808510638 loss: 0.0004304136673454195\n",
      "116: accuracy:0.5957446808510638 loss: 0.00041784558561630547\n",
      "117: accuracy:0.5957446808510638 loss: 0.00041527749272063375\n",
      "118: accuracy:0.5957446808510638 loss: 0.0003881386364810169\n",
      "119: accuracy:0.5957446808510638 loss: 0.0003407001495361328\n",
      "120: accuracy:0.5957446808510638 loss: 0.00035989625030197203\n",
      "121: accuracy:0.5886524822695035 loss: 0.00045158521970734\n",
      "122: accuracy:0.5815602836879432 loss: 0.00038010053685866296\n",
      "123: accuracy:0.5815602836879432 loss: 0.00046980721526779234\n",
      "124: accuracy:0.5815602836879432 loss: 0.0003212247684132308\n",
      "125: accuracy:0.5815602836879432 loss: 0.000310441420879215\n",
      "126: accuracy:0.5815602836879432 loss: 0.00026785986847244203\n",
      "127: accuracy:0.5815602836879432 loss: 0.0004588876327034086\n",
      "128: accuracy:0.5815602836879432 loss: 0.0004043510998599231\n",
      "129: accuracy:0.5815602836879432 loss: 0.00042571339872665703\n",
      "130: accuracy:0.5815602836879432 loss: 0.0003232751623727381\n",
      "131: accuracy:0.5815602836879432 loss: 0.00034405163023620844\n",
      "132: accuracy:0.5815602836879432 loss: 0.0003126416995655745\n",
      "133: accuracy:0.5815602836879432 loss: 0.00025147711858153343\n",
      "134: accuracy:0.5815602836879432 loss: 0.00031029837555252016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135: accuracy:0.5815602836879432 loss: 0.00027812548796646297\n",
      "136: accuracy:0.5815602836879432 loss: 0.00027411323389969766\n",
      "137: accuracy:0.5815602836879432 loss: 0.0002513545041438192\n",
      "138: accuracy:0.5815602836879432 loss: 0.00033311161678284407\n",
      "139: accuracy:0.5815602836879432 loss: 0.00028166771517135203\n",
      "140: accuracy:0.5815602836879432 loss: 0.0002844742266461253\n",
      "141: accuracy:0.5815602836879432 loss: 0.0002581528387963772\n",
      "142: accuracy:0.5815602836879432 loss: 0.0002590111398603767\n",
      "143: accuracy:0.5815602836879432 loss: 0.0002612318203318864\n",
      "144: accuracy:0.5815602836879432 loss: 0.00039308410487137735\n",
      "145: accuracy:0.5815602836879432 loss: 0.0002282687637489289\n",
      "146: accuracy:0.5815602836879432 loss: 0.00028664726414717734\n",
      "147: accuracy:0.5815602836879432 loss: 0.00037805011379532516\n",
      "148: accuracy:0.5815602836879432 loss: 0.00023583004076499492\n",
      "149: accuracy:0.5815602836879432 loss: 0.0002475670480635017\n",
      "150: accuracy:0.5815602836879432 loss: 0.00027078218408860266\n",
      "151: accuracy:0.5815602836879432 loss: 0.00022448811796493828\n",
      "152: accuracy:0.5815602836879432 loss: 0.0002702644851524383\n",
      "153: accuracy:0.5815602836879432 loss: 0.00023105484433472157\n",
      "154: accuracy:0.5815602836879432 loss: 0.00023794174194335938\n",
      "155: accuracy:0.5815602836879432 loss: 0.00016910007980186492\n",
      "156: accuracy:0.5815602836879432 loss: 0.0002341202343814075\n",
      "157: accuracy:0.5886524822695035 loss: 0.00032683101017028093\n",
      "158: accuracy:0.5886524822695035 loss: 0.00021961075253784657\n",
      "159: accuracy:0.5886524822695035 loss: 0.0003374985244590789\n",
      "160: accuracy:0.5886524822695035 loss: 0.00017430441221222281\n",
      "161: accuracy:0.5886524822695035 loss: 0.0002928870089817792\n",
      "162: accuracy:0.5886524822695035 loss: 0.0002380984224146232\n",
      "163: accuracy:0.5886524822695035 loss: 0.00028100694180466235\n",
      "164: accuracy:0.5886524822695035 loss: 0.0002444199053570628\n",
      "165: accuracy:0.5886524822695035 loss: 0.00022537367476616055\n",
      "166: accuracy:0.5886524822695035 loss: 0.00023488998704124242\n",
      "167: accuracy:0.5886524822695035 loss: 0.00025867053773254156\n",
      "168: accuracy:0.5886524822695035 loss: 0.00016222000704146922\n",
      "169: accuracy:0.5886524822695035 loss: 0.00025382041349075735\n",
      "170: accuracy:0.5886524822695035 loss: 0.0002231938560726121\n",
      "171: accuracy:0.5886524822695035 loss: 0.00021037373517174274\n",
      "172: accuracy:0.5886524822695035 loss: 0.0002910341427195817\n",
      "173: accuracy:0.5886524822695035 loss: 0.0002233913983218372\n",
      "174: accuracy:0.5886524822695035 loss: 0.00018786703003570437\n",
      "175: accuracy:0.5886524822695035 loss: 0.00023574147780891508\n",
      "176: accuracy:0.5886524822695035 loss: 0.00021008083422202617\n",
      "177: accuracy:0.5886524822695035 loss: 0.0002204758784500882\n",
      "178: accuracy:0.5886524822695035 loss: 0.00021441323042381555\n",
      "179: accuracy:0.5886524822695035 loss: 0.0001939637295436114\n",
      "180: accuracy:0.5886524822695035 loss: 0.00021988323715049773\n",
      "181: accuracy:0.5886524822695035 loss: 0.00021298952924553305\n",
      "182: accuracy:0.5886524822695035 loss: 0.00028126579127274454\n",
      "183: accuracy:0.5886524822695035 loss: 0.00018324170378036797\n",
      "184: accuracy:0.5886524822695035 loss: 0.00018630028353072703\n",
      "185: accuracy:0.5886524822695035 loss: 0.00018468585039954633\n",
      "186: accuracy:0.5886524822695035 loss: 0.00024728773860260844\n",
      "187: accuracy:0.5886524822695035 loss: 0.00019747189071495086\n",
      "188: accuracy:0.5886524822695035 loss: 0.0001819406170397997\n",
      "189: accuracy:0.5886524822695035 loss: 0.00019892283307854086\n",
      "190: accuracy:0.5886524822695035 loss: 0.00019357545534148812\n",
      "191: accuracy:0.5886524822695035 loss: 0.00017225401825271547\n",
      "192: accuracy:0.5886524822695035 loss: 0.00015038081619422883\n",
      "193: accuracy:0.5886524822695035 loss: 0.0002031190088018775\n",
      "194: accuracy:0.5886524822695035 loss: 0.00017791475693229586\n",
      "195: accuracy:0.5886524822695035 loss: 0.0001572745240991935\n",
      "196: accuracy:0.5886524822695035 loss: 0.00015557833830825984\n",
      "197: accuracy:0.5886524822695035 loss: 0.00013039453187957406\n",
      "198: accuracy:0.5886524822695035 loss: 0.00021524429030250758\n",
      "199: accuracy:0.5886524822695035 loss: 0.0001786776992958039\n",
      "200: accuracy:0.5886524822695035 loss: 0.0001579284726176411\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    ")\n",
    "net = RNN_max()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "for epoch in range(1,201): \n",
    "    for step, (batch_x, sen, batch_y) in enumerate(loader):\n",
    "        pred = net(sen, batch_x)\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 2: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_s, t_e)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glove_Bi-rnn_max: 0.6808510638297872 7轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Bi_RNN_max(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size = 200,       # rnn input\n",
    "            hidden_size = 64,     # rnn hidden unit\n",
    "            num_layers = 1,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=128,out_features=7)\n",
    "        self.maxpool = torch.nn.MaxPool2d((53,1))\n",
    "\n",
    "    def forward(self,sen,x):\n",
    "        output,h_n=self.rnn(x.float())\n",
    "        output_in_last_timestep = self.maxpool(output)\n",
    "        output_in_last_timestep = output_in_last_timestep.reshape((output_in_last_timestep.shape[0],-1))\n",
    "        x=self.out(output_in_last_timestep)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi_RNN_max(\n",
      "  (rnn): GRU(200, 64, batch_first=True, bidirectional=True)\n",
      "  (out): Linear(in_features=128, out_features=7, bias=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=(53, 1), stride=(53, 1), padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "1: accuracy:0.22695035460992907 loss: 1.9261858463287354\n",
      "2: accuracy:0.41134751773049644 loss: 1.567962646484375\n",
      "3: accuracy:0.5815602836879432 loss: 1.2670047283172607\n",
      "4: accuracy:0.6312056737588653 loss: 0.9607715010643005\n",
      "5: accuracy:0.624113475177305 loss: 0.6432244181632996\n",
      "6: accuracy:0.6595744680851063 loss: 0.4268020987510681\n",
      "7: accuracy:0.6808510638297872 loss: 0.3015275001525879\n",
      "8: accuracy:0.6312056737588653 loss: 0.15083907544612885\n",
      "9: accuracy:0.624113475177305 loss: 0.11364524066448212\n",
      "10: accuracy:0.6382978723404256 loss: 0.05991930142045021\n",
      "11: accuracy:0.6312056737588653 loss: 0.030624818056821823\n",
      "12: accuracy:0.6524822695035462 loss: 0.022307030856609344\n",
      "13: accuracy:0.6524822695035462 loss: 0.010035010986030102\n",
      "14: accuracy:0.6666666666666666 loss: 0.008143357001245022\n",
      "15: accuracy:0.6595744680851063 loss: 0.005326461978256702\n",
      "16: accuracy:0.6524822695035462 loss: 0.005286904517561197\n",
      "17: accuracy:0.6595744680851063 loss: 0.0036407471634447575\n",
      "18: accuracy:0.6666666666666666 loss: 0.003049516584724188\n",
      "19: accuracy:0.6595744680851063 loss: 0.002568422118201852\n",
      "20: accuracy:0.6524822695035462 loss: 0.0025816031266003847\n",
      "21: accuracy:0.6524822695035462 loss: 0.002435507019981742\n",
      "22: accuracy:0.6595744680851063 loss: 0.001932014711201191\n",
      "23: accuracy:0.6666666666666666 loss: 0.001853023306466639\n",
      "24: accuracy:0.6737588652482269 loss: 0.0015727520221844316\n",
      "25: accuracy:0.6666666666666666 loss: 0.0013593605253845453\n",
      "26: accuracy:0.6666666666666666 loss: 0.0014151777140796185\n",
      "27: accuracy:0.6595744680851063 loss: 0.0012939453590661287\n",
      "28: accuracy:0.6666666666666666 loss: 0.001229510991834104\n",
      "29: accuracy:0.6666666666666666 loss: 0.0012311663012951612\n",
      "30: accuracy:0.6666666666666666 loss: 0.0010286944452673197\n",
      "31: accuracy:0.6666666666666666 loss: 0.0010346889030188322\n",
      "32: accuracy:0.6666666666666666 loss: 0.0009371621417813003\n",
      "33: accuracy:0.6666666666666666 loss: 0.0008734089788049459\n",
      "34: accuracy:0.6666666666666666 loss: 0.0008380617364309728\n",
      "35: accuracy:0.6666666666666666 loss: 0.0009549549431540072\n",
      "36: accuracy:0.6666666666666666 loss: 0.00083740777336061\n",
      "37: accuracy:0.6666666666666666 loss: 0.0008726119995117188\n",
      "38: accuracy:0.6666666666666666 loss: 0.0008157934644259512\n",
      "39: accuracy:0.6666666666666666 loss: 0.0008350781281478703\n",
      "40: accuracy:0.6666666666666666 loss: 0.0007020269404165447\n",
      "41: accuracy:0.6666666666666666 loss: 0.0007969106663949788\n",
      "42: accuracy:0.6666666666666666 loss: 0.0007398673333227634\n",
      "43: accuracy:0.6666666666666666 loss: 0.0007412161212414503\n",
      "44: accuracy:0.6666666666666666 loss: 0.000773525214754045\n",
      "45: accuracy:0.6666666666666666 loss: 0.0006151676061563194\n",
      "46: accuracy:0.6666666666666666 loss: 0.0006390639464370906\n",
      "47: accuracy:0.6666666666666666 loss: 0.0006636551697738469\n",
      "48: accuracy:0.6666666666666666 loss: 0.0006138461176306009\n",
      "49: accuracy:0.6666666666666666 loss: 0.0005782876978628337\n",
      "50: accuracy:0.6666666666666666 loss: 0.0005706037627533078\n",
      "51: accuracy:0.6666666666666666 loss: 0.0005302633508108556\n",
      "52: accuracy:0.6666666666666666 loss: 0.0005067007732577622\n",
      "53: accuracy:0.6666666666666666 loss: 0.0005129268974997103\n",
      "54: accuracy:0.6666666666666666 loss: 0.0004806178039871156\n",
      "55: accuracy:0.6666666666666666 loss: 0.000571877637412399\n",
      "56: accuracy:0.6666666666666666 loss: 0.00048003197298385203\n",
      "57: accuracy:0.6666666666666666 loss: 0.0004987784777767956\n",
      "58: accuracy:0.6666666666666666 loss: 0.00046614237362518907\n",
      "59: accuracy:0.6737588652482269 loss: 0.00050305639160797\n",
      "60: accuracy:0.6595744680851063 loss: 0.000452498032245785\n",
      "61: accuracy:0.6595744680851063 loss: 0.0004723140154965222\n",
      "62: accuracy:0.6595744680851063 loss: 0.0004855905135627836\n",
      "63: accuracy:0.6595744680851063 loss: 0.0003837381082121283\n",
      "64: accuracy:0.6595744680851063 loss: 0.00037944657378830016\n",
      "65: accuracy:0.6666666666666666 loss: 0.00041593823698349297\n",
      "66: accuracy:0.6595744680851063 loss: 0.0004514830361586064\n",
      "67: accuracy:0.6595744680851063 loss: 0.0004164491256233305\n",
      "68: accuracy:0.6666666666666666 loss: 0.0003909383376594633\n",
      "69: accuracy:0.6666666666666666 loss: 0.0003057820431422442\n",
      "70: accuracy:0.6595744680851063 loss: 0.0003556047158781439\n",
      "71: accuracy:0.6595744680851063 loss: 0.00036779811489395797\n",
      "72: accuracy:0.6595744680851063 loss: 0.00039922850555740297\n",
      "73: accuracy:0.6595744680851063 loss: 0.0003303800185676664\n",
      "74: accuracy:0.6595744680851063 loss: 0.0003169468545820564\n",
      "75: accuracy:0.6595744680851063 loss: 0.00030565942870453\n",
      "76: accuracy:0.6595744680851063 loss: 0.00031946727540344\n",
      "77: accuracy:0.6524822695035462 loss: 0.00034854072146117687\n",
      "78: accuracy:0.6666666666666666 loss: 0.0003172806464135647\n",
      "79: accuracy:0.6666666666666666 loss: 0.0003509180969558656\n",
      "80: accuracy:0.6524822695035462 loss: 0.00031907219090498984\n",
      "81: accuracy:0.6595744680851063 loss: 0.00024094581021927297\n",
      "82: accuracy:0.6666666666666666 loss: 0.00031083650537766516\n",
      "83: accuracy:0.6666666666666666 loss: 0.0002689565881155431\n",
      "84: accuracy:0.6666666666666666 loss: 0.0003099305322393775\n",
      "85: accuracy:0.6666666666666666 loss: 0.0002583844179753214\n",
      "86: accuracy:0.6666666666666666 loss: 0.00027939933352172375\n",
      "87: accuracy:0.6737588652482269 loss: 0.00028285299777053297\n",
      "88: accuracy:0.6666666666666666 loss: 0.000310250703478232\n",
      "89: accuracy:0.6666666666666666 loss: 0.0002676895819604397\n",
      "90: accuracy:0.6737588652482269 loss: 0.0002680982870515436\n",
      "91: accuracy:0.6737588652482269 loss: 0.00026076860376633704\n",
      "92: accuracy:0.6737588652482269 loss: 0.00022039414034225047\n",
      "93: accuracy:0.6737588652482269 loss: 0.00026568686007522047\n",
      "94: accuracy:0.6666666666666666 loss: 0.00023318699095398188\n",
      "95: accuracy:0.6666666666666666 loss: 0.00025475365691818297\n",
      "96: accuracy:0.6666666666666666 loss: 0.0002086503227474168\n",
      "97: accuracy:0.6666666666666666 loss: 0.00021497182024177164\n",
      "98: accuracy:0.6666666666666666 loss: 0.00021912710508331656\n",
      "99: accuracy:0.6666666666666666 loss: 0.00020315306028351188\n",
      "100: accuracy:0.6595744680851063 loss: 0.00022484915098175406\n",
      "101: accuracy:0.6666666666666666 loss: 0.00022456987062469125\n",
      "102: accuracy:0.6666666666666666 loss: 0.00016190664609894156\n",
      "103: accuracy:0.6666666666666666 loss: 0.00022080967028159648\n",
      "104: accuracy:0.6666666666666666 loss: 0.00020170211791992188\n",
      "105: accuracy:0.6595744680851063 loss: 0.00022579601500183344\n",
      "106: accuracy:0.6595744680851063 loss: 0.00018245152023155242\n",
      "107: accuracy:0.6595744680851063 loss: 0.00023042134125716984\n",
      "108: accuracy:0.6666666666666666 loss: 0.00018723351240623742\n",
      "109: accuracy:0.6666666666666666 loss: 0.0001898288755910471\n",
      "110: accuracy:0.6737588652482269 loss: 0.00016899789625313133\n",
      "111: accuracy:0.6737588652482269 loss: 0.0001873765722848475\n",
      "112: accuracy:0.6737588652482269 loss: 0.00021130697859916836\n",
      "113: accuracy:0.6666666666666666 loss: 0.00019320759747643024\n",
      "114: accuracy:0.6737588652482269 loss: 0.0001728194038150832\n",
      "115: accuracy:0.6666666666666666 loss: 0.00016600744856987149\n",
      "116: accuracy:0.6524822695035462 loss: 0.00017372540605720133\n",
      "117: accuracy:0.6666666666666666 loss: 0.00018174988508690149\n",
      "118: accuracy:0.6666666666666666 loss: 0.00016348021745216101\n",
      "119: accuracy:0.6666666666666666 loss: 0.0001632281782804057\n",
      "120: accuracy:0.6737588652482269 loss: 0.000143923083669506\n",
      "121: accuracy:0.6595744680851063 loss: 0.00016475404845550656\n",
      "122: accuracy:0.6666666666666666 loss: 0.0001456056343158707\n",
      "123: accuracy:0.6666666666666666 loss: 0.0001806531654438004\n",
      "124: accuracy:0.6524822695035462 loss: 0.00014976093370933086\n",
      "125: accuracy:0.6595744680851063 loss: 0.0001696246035862714\n",
      "126: accuracy:0.6666666666666666 loss: 0.0001491682924097404\n",
      "127: accuracy:0.6595744680851063 loss: 0.00015083041216712445\n",
      "128: accuracy:0.6595744680851063 loss: 0.0001346724311588332\n",
      "129: accuracy:0.6595744680851063 loss: 0.00013452938583213836\n",
      "130: accuracy:0.6595744680851063 loss: 0.00016347340715583414\n",
      "131: accuracy:0.6595744680851063 loss: 0.00012563297059386969\n",
      "132: accuracy:0.6595744680851063 loss: 0.00015021051513031125\n",
      "133: accuracy:0.6595744680851063 loss: 0.00013240404950920492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134: accuracy:0.6666666666666666 loss: 0.00013349397340789437\n",
      "135: accuracy:0.6595744680851063 loss: 0.0001453535951441154\n",
      "136: accuracy:0.6524822695035462 loss: 0.00015350068861152977\n",
      "137: accuracy:0.6595744680851063 loss: 0.0001294067915296182\n",
      "138: accuracy:0.6595744680851063 loss: 0.00013213838974479586\n",
      "139: accuracy:0.6524822695035462 loss: 0.00012712478928733617\n",
      "140: accuracy:0.6595744680851063 loss: 0.00013387543731369078\n",
      "141: accuracy:0.6595744680851063 loss: 0.00012803077697753906\n",
      "142: accuracy:0.6524822695035462 loss: 0.00011979512055404484\n",
      "143: accuracy:0.6595744680851063 loss: 0.00011410032311687246\n",
      "144: accuracy:0.6524822695035462 loss: 0.00011839185754070058\n",
      "145: accuracy:0.6524822695035462 loss: 0.00010876655869651586\n",
      "146: accuracy:0.6524822695035462 loss: 0.00013625281280837953\n",
      "147: accuracy:0.6595744680851063 loss: 0.00012610299745574594\n",
      "148: accuracy:0.6524822695035462 loss: 0.00011950901534873992\n",
      "149: accuracy:0.6595744680851063 loss: 0.00011908667511306703\n",
      "150: accuracy:0.6595744680851063 loss: 0.00010858262976398692\n",
      "151: accuracy:0.6453900709219859 loss: 0.00011585780885070562\n",
      "152: accuracy:0.6524822695035462 loss: 0.00011196136620128527\n",
      "153: accuracy:0.6524822695035462 loss: 0.00010922295768978074\n",
      "154: accuracy:0.6524822695035462 loss: 9.991782280849293e-05\n",
      "155: accuracy:0.6595744680851063 loss: 0.0001051630315487273\n",
      "156: accuracy:0.6453900709219859 loss: 0.0001014437002595514\n",
      "157: accuracy:0.6453900709219859 loss: 0.00010075569298351184\n",
      "158: accuracy:0.6524822695035462 loss: 0.0001029014601954259\n",
      "159: accuracy:0.6453900709219859 loss: 0.00011046273721149191\n",
      "160: accuracy:0.6382978723404256 loss: 8.615766273578629e-05\n",
      "161: accuracy:0.6382978723404256 loss: 0.00010967254638671875\n",
      "162: accuracy:0.6453900709219859 loss: 8.727482054382563e-05\n",
      "163: accuracy:0.6382978723404256 loss: 8.435930794803426e-05\n",
      "164: accuracy:0.6382978723404256 loss: 9.272438910556957e-05\n",
      "165: accuracy:0.6382978723404256 loss: 0.00010307857155567035\n",
      "166: accuracy:0.6453900709219859 loss: 0.00010226113226963207\n",
      "167: accuracy:0.6453900709219859 loss: 8.266993972938508e-05\n",
      "168: accuracy:0.6453900709219859 loss: 0.00010535376350162551\n",
      "169: accuracy:0.6453900709219859 loss: 8.678436279296875e-05\n",
      "170: accuracy:0.6453900709219859 loss: 9.750638855621219e-05\n",
      "171: accuracy:0.6453900709219859 loss: 9.61439945967868e-05\n",
      "172: accuracy:0.6524822695035462 loss: 9.382792632095516e-05\n",
      "173: accuracy:0.6453900709219859 loss: 9.637560287956148e-05\n",
      "174: accuracy:0.6524822695035462 loss: 8.29832861199975e-05\n",
      "175: accuracy:0.6524822695035462 loss: 9.820120612857863e-05\n",
      "176: accuracy:0.6524822695035462 loss: 8.252007683040574e-05\n",
      "177: accuracy:0.6595744680851063 loss: 8.017676009330899e-05\n",
      "178: accuracy:0.6524822695035462 loss: 8.70568401296623e-05\n",
      "179: accuracy:0.6595744680851063 loss: 8.803776290733367e-05\n",
      "180: accuracy:0.6453900709219859 loss: 8.400508522754535e-05\n",
      "181: accuracy:0.6524822695035462 loss: 7.919583731563762e-05\n",
      "182: accuracy:0.6595744680851063 loss: 8.277893357444555e-05\n",
      "183: accuracy:0.6595744680851063 loss: 8.111681381706148e-05\n",
      "184: accuracy:0.6666666666666666 loss: 8.174351387424394e-05\n",
      "185: accuracy:0.6595744680851063 loss: 8.246557990787551e-05\n",
      "186: accuracy:0.6595744680851063 loss: 7.28334707673639e-05\n",
      "187: accuracy:0.6595744680851063 loss: 8.329663978656754e-05\n",
      "188: accuracy:0.6524822695035462 loss: 7.930483116069809e-05\n",
      "189: accuracy:0.6737588652482269 loss: 8.069447358138859e-05\n",
      "190: accuracy:0.6595744680851063 loss: 8.328301919391379e-05\n",
      "191: accuracy:0.6737588652482269 loss: 7.979528891155496e-05\n",
      "192: accuracy:0.6666666666666666 loss: 7.780620217090473e-05\n",
      "193: accuracy:0.6666666666666666 loss: 8.440017700195312e-05\n",
      "194: accuracy:0.6595744680851063 loss: 7.303783058887348e-05\n",
      "195: accuracy:0.6737588652482269 loss: 8.329663978656754e-05\n",
      "196: accuracy:0.6737588652482269 loss: 7.00541932019405e-05\n",
      "197: accuracy:0.6666666666666666 loss: 8.28879201435484e-05\n",
      "198: accuracy:0.6666666666666666 loss: 7.613046182086691e-05\n",
      "199: accuracy:0.6737588652482269 loss: 7.350103987846524e-05\n",
      "200: accuracy:0.6666666666666666 loss: 7.065364479785785e-05\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    ")\n",
    "net = Bi_RNN_max()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "for epoch in range(1,201): \n",
    "    for step, (batch_x, sen, batch_y) in enumerate(loader):\n",
    "        pred = net(sen, batch_x)\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 2: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_s, t_e)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glove:0.44680851063829785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN_old(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size = 200,       # rnn input\n",
    "            hidden_size = 64,     # rnn hidden unit\n",
    "            num_layers = 1,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=64,out_features=7)\n",
    "\n",
    "    def forward(self,sen,x):\n",
    "        output,h_n=self.rnn(x.float())\n",
    "        output_in_last_timestep=h_n[-1,:,:]\n",
    "        x=self.out(output_in_last_timestep)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_old(\n",
      "  (rnn): GRU(200, 64, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=7, bias=True)\n",
      ")\n",
      "1: accuracy:0.2127659574468085 loss: 1.9432930946350098\n",
      "2: accuracy:0.12056737588652482 loss: 1.965213656425476\n",
      "3: accuracy:0.2127659574468085 loss: 1.9340920448303223\n",
      "4: accuracy:0.12056737588652482 loss: 1.9321993589401245\n",
      "5: accuracy:0.12056737588652482 loss: 1.933420181274414\n",
      "6: accuracy:0.22695035460992907 loss: 1.9249428510665894\n",
      "7: accuracy:0.22695035460992907 loss: 1.9938651323318481\n",
      "8: accuracy:0.24113475177304963 loss: 1.9096006155014038\n",
      "9: accuracy:0.14184397163120568 loss: 1.9152500629425049\n",
      "10: accuracy:0.18439716312056736 loss: 1.8684868812561035\n",
      "11: accuracy:0.24113475177304963 loss: 1.844362497329712\n",
      "12: accuracy:0.2127659574468085 loss: 1.7581762075424194\n",
      "13: accuracy:0.24113475177304963 loss: 1.7458086013793945\n",
      "14: accuracy:0.28368794326241137 loss: 1.542167067527771\n",
      "15: accuracy:0.3333333333333333 loss: 1.5530861616134644\n",
      "16: accuracy:0.2695035460992908 loss: 1.5492911338806152\n",
      "17: accuracy:0.2907801418439716 loss: 1.315773606300354\n",
      "18: accuracy:0.2978723404255319 loss: 1.4151092767715454\n",
      "19: accuracy:0.2978723404255319 loss: 1.403273344039917\n",
      "20: accuracy:0.2907801418439716 loss: 1.3421993255615234\n",
      "21: accuracy:0.3475177304964539 loss: 1.3857874870300293\n",
      "22: accuracy:0.3475177304964539 loss: 1.316149353981018\n",
      "23: accuracy:0.3191489361702128 loss: 1.0150429010391235\n",
      "24: accuracy:0.2765957446808511 loss: 1.1263089179992676\n",
      "25: accuracy:0.3120567375886525 loss: 0.915546715259552\n",
      "26: accuracy:0.3333333333333333 loss: 1.2290406227111816\n",
      "27: accuracy:0.3404255319148936 loss: 1.1392135620117188\n",
      "28: accuracy:0.41134751773049644 loss: 1.0421077013015747\n",
      "29: accuracy:0.41134751773049644 loss: 1.0006940364837646\n",
      "30: accuracy:0.425531914893617 loss: 0.8738164305686951\n",
      "31: accuracy:0.3900709219858156 loss: 0.6467732191085815\n",
      "32: accuracy:0.41134751773049644 loss: 0.8078730702400208\n",
      "33: accuracy:0.41134751773049644 loss: 0.5350220799446106\n",
      "34: accuracy:0.45390070921985815 loss: 0.7334603667259216\n",
      "35: accuracy:0.46099290780141844 loss: 0.48534637689590454\n",
      "36: accuracy:0.425531914893617 loss: 0.6628413200378418\n",
      "37: accuracy:0.41843971631205673 loss: 0.4969223141670227\n",
      "38: accuracy:0.475177304964539 loss: 0.3320828080177307\n",
      "39: accuracy:0.475177304964539 loss: 0.24474367499351501\n",
      "40: accuracy:0.46808510638297873 loss: 0.3974769711494446\n",
      "41: accuracy:0.46808510638297873 loss: 0.4929870069026947\n",
      "42: accuracy:0.46808510638297873 loss: 0.2559208571910858\n",
      "43: accuracy:0.46099290780141844 loss: 0.3216521143913269\n",
      "44: accuracy:0.48226950354609927 loss: 0.09238182008266449\n",
      "45: accuracy:0.45390070921985815 loss: 0.19452375173568726\n",
      "46: accuracy:0.475177304964539 loss: 0.2151767462491989\n",
      "47: accuracy:0.475177304964539 loss: 0.0910651907324791\n",
      "48: accuracy:0.46099290780141844 loss: 0.09518701583147049\n",
      "49: accuracy:0.45390070921985815 loss: 0.10594060271978378\n",
      "50: accuracy:0.46099290780141844 loss: 0.17123916745185852\n",
      "51: accuracy:0.4397163120567376 loss: 0.09511025249958038\n",
      "52: accuracy:0.46808510638297873 loss: 0.07248824089765549\n",
      "53: accuracy:0.4397163120567376 loss: 0.10079551488161087\n",
      "54: accuracy:0.48226950354609927 loss: 0.08811295032501221\n",
      "55: accuracy:0.5035460992907801 loss: 0.014965738169848919\n",
      "56: accuracy:0.48936170212765956 loss: 0.02351783961057663\n",
      "57: accuracy:0.48936170212765956 loss: 0.07398343086242676\n",
      "58: accuracy:0.49645390070921985 loss: 0.011398717761039734\n",
      "59: accuracy:0.524822695035461 loss: 0.006995133124291897\n",
      "60: accuracy:0.5177304964539007 loss: 0.024125967174768448\n",
      "61: accuracy:0.5390070921985816 loss: 0.014191879890859127\n",
      "62: accuracy:0.5319148936170213 loss: 0.01277589425444603\n",
      "63: accuracy:0.5177304964539007 loss: 0.010981668718159199\n",
      "64: accuracy:0.5177304964539007 loss: 0.09298796206712723\n",
      "65: accuracy:0.5177304964539007 loss: 0.01636231690645218\n",
      "66: accuracy:0.5035460992907801 loss: 0.005352783016860485\n",
      "67: accuracy:0.5177304964539007 loss: 0.006208849139511585\n",
      "68: accuracy:0.5106382978723404 loss: 0.014740950427949429\n",
      "69: accuracy:0.5106382978723404 loss: 0.00487660663202405\n",
      "70: accuracy:0.5177304964539007 loss: 0.03621961548924446\n",
      "71: accuracy:0.5177304964539007 loss: 0.0026370184496045113\n",
      "72: accuracy:0.5035460992907801 loss: 0.005295814946293831\n",
      "73: accuracy:0.5106382978723404 loss: 0.003383684204891324\n",
      "74: accuracy:0.5106382978723404 loss: 0.0023546013981103897\n",
      "75: accuracy:0.5177304964539007 loss: 0.002162715420126915\n",
      "76: accuracy:0.5177304964539007 loss: 0.0015343802515417337\n",
      "77: accuracy:0.5106382978723404 loss: 0.0014934198698028922\n",
      "78: accuracy:0.5106382978723404 loss: 0.002100079320371151\n",
      "79: accuracy:0.5177304964539007 loss: 0.0016052995342761278\n",
      "80: accuracy:0.5177304964539007 loss: 0.0018947874195873737\n",
      "81: accuracy:0.5177304964539007 loss: 0.0010695388773456216\n",
      "82: accuracy:0.524822695035461 loss: 0.0008969306945800781\n",
      "83: accuracy:0.524822695035461 loss: 0.0009745393763296306\n",
      "84: accuracy:0.524822695035461 loss: 0.0010371957905590534\n",
      "85: accuracy:0.524822695035461 loss: 0.001082454458810389\n",
      "86: accuracy:0.524822695035461 loss: 0.0011943408753722906\n",
      "87: accuracy:0.524822695035461 loss: 0.0009948662482202053\n",
      "88: accuracy:0.524822695035461 loss: 0.0008978435071185231\n",
      "89: accuracy:0.524822695035461 loss: 0.0009789739269763231\n",
      "90: accuracy:0.524822695035461 loss: 0.0007365022320300341\n",
      "91: accuracy:0.5177304964539007 loss: 0.0006811618804931641\n",
      "92: accuracy:0.5177304964539007 loss: 0.0006977217271924019\n",
      "93: accuracy:0.5177304964539007 loss: 0.0007132598548196256\n",
      "94: accuracy:0.5177304964539007 loss: 0.0008385794353671372\n",
      "95: accuracy:0.5177304964539007 loss: 0.0006848539342172444\n",
      "96: accuracy:0.5177304964539007 loss: 0.0005283491918817163\n",
      "97: accuracy:0.5106382978723404 loss: 0.0006869111675769091\n",
      "98: accuracy:0.5106382978723404 loss: 0.0006972040282562375\n",
      "99: accuracy:0.5035460992907801 loss: 0.0006040573352947831\n",
      "100: accuracy:0.5035460992907801 loss: 0.0006052153185009956\n",
      "101: accuracy:0.5035460992907801 loss: 0.0004949365393258631\n",
      "102: accuracy:0.5035460992907801 loss: 0.00046501841279678047\n",
      "103: accuracy:0.5035460992907801 loss: 0.0005142075824551284\n",
      "104: accuracy:0.5035460992907801 loss: 0.0006016049883328378\n",
      "105: accuracy:0.5035460992907801 loss: 0.0004483563534449786\n",
      "106: accuracy:0.5035460992907801 loss: 0.0005350181018002331\n",
      "107: accuracy:0.5035460992907801 loss: 0.0005454131751321256\n",
      "108: accuracy:0.5035460992907801 loss: 0.0005584103637374938\n",
      "109: accuracy:0.5035460992907801 loss: 0.0005128315533511341\n",
      "110: accuracy:0.5035460992907801 loss: 0.0004839897155761719\n",
      "111: accuracy:0.5106382978723404 loss: 0.0005469254101626575\n",
      "112: accuracy:0.5106382978723404 loss: 0.0004710129287559539\n",
      "113: accuracy:0.5106382978723404 loss: 0.0004267896874807775\n",
      "114: accuracy:0.5106382978723404 loss: 0.0003386974276509136\n",
      "115: accuracy:0.5106382978723404 loss: 0.00044945988338440657\n",
      "116: accuracy:0.5106382978723404 loss: 0.0003677368222270161\n",
      "117: accuracy:0.5106382978723404 loss: 0.0003951072576455772\n",
      "118: accuracy:0.5106382978723404 loss: 0.0003710269811563194\n",
      "119: accuracy:0.5106382978723404 loss: 0.0004386969958432019\n",
      "120: accuracy:0.5106382978723404 loss: 0.0004181385156698525\n",
      "121: accuracy:0.5106382978723404 loss: 0.00033762794919312\n",
      "122: accuracy:0.5106382978723404 loss: 0.00039478030521422625\n",
      "123: accuracy:0.5106382978723404 loss: 0.0004382405895739794\n",
      "124: accuracy:0.5106382978723404 loss: 0.00038326127105392516\n",
      "125: accuracy:0.5106382978723404 loss: 0.0004908425617031753\n",
      "126: accuracy:0.5106382978723404 loss: 0.00035523687256500125\n",
      "127: accuracy:0.5106382978723404 loss: 0.00030903134029358625\n",
      "128: accuracy:0.5106382978723404 loss: 0.00032054356415756047\n",
      "129: accuracy:0.5106382978723404 loss: 0.0002777644549496472\n",
      "130: accuracy:0.5106382978723404 loss: 0.0003211157745681703\n",
      "131: accuracy:0.5106382978723404 loss: 0.00035091128665953875\n",
      "132: accuracy:0.5106382978723404 loss: 0.00036642211489379406\n",
      "133: accuracy:0.5106382978723404 loss: 0.0003338609531056136\n",
      "134: accuracy:0.5106382978723404 loss: 0.0003362519491929561\n",
      "135: accuracy:0.5035460992907801 loss: 0.00035953521728515625\n",
      "136: accuracy:0.5035460992907801 loss: 0.0002933229843620211\n",
      "137: accuracy:0.5035460992907801 loss: 0.0003297192743048072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138: accuracy:0.5035460992907801 loss: 0.0003175258752889931\n",
      "139: accuracy:0.5035460992907801 loss: 0.00028664726414717734\n",
      "140: accuracy:0.5035460992907801 loss: 0.00025294849183410406\n",
      "141: accuracy:0.5035460992907801 loss: 0.00029448099667206407\n",
      "142: accuracy:0.5035460992907801 loss: 0.0002509593905415386\n",
      "143: accuracy:0.5106382978723404 loss: 0.00027127948123961687\n",
      "144: accuracy:0.5035460992907801 loss: 0.00027157238218933344\n",
      "145: accuracy:0.5035460992907801 loss: 0.00025053706485778093\n",
      "146: accuracy:0.5035460992907801 loss: 0.00021967205975670367\n",
      "147: accuracy:0.5035460992907801 loss: 0.00027350016171112657\n",
      "148: accuracy:0.5035460992907801 loss: 0.00022682461712975055\n",
      "149: accuracy:0.5035460992907801 loss: 0.00021014895173721015\n",
      "150: accuracy:0.5035460992907801 loss: 0.0002713067224249244\n",
      "151: accuracy:0.5035460992907801 loss: 0.00024636133457534015\n",
      "152: accuracy:0.5035460992907801 loss: 0.0002392223832430318\n",
      "153: accuracy:0.5035460992907801 loss: 0.0002703802892938256\n",
      "154: accuracy:0.5035460992907801 loss: 0.00023262841568794101\n",
      "155: accuracy:0.5035460992907801 loss: 0.00027283260715194046\n",
      "156: accuracy:0.5035460992907801 loss: 0.00025895662838593125\n",
      "157: accuracy:0.5035460992907801 loss: 0.00027097974088974297\n",
      "158: accuracy:0.5035460992907801 loss: 0.000182070056325756\n",
      "159: accuracy:0.5035460992907801 loss: 0.0002189499937230721\n",
      "160: accuracy:0.5035460992907801 loss: 0.00021619115432258695\n",
      "161: accuracy:0.5035460992907801 loss: 0.00020918846712447703\n",
      "162: accuracy:0.5035460992907801 loss: 0.00023822103685233742\n",
      "163: accuracy:0.5035460992907801 loss: 0.00021318708604667336\n",
      "164: accuracy:0.5035460992907801 loss: 0.0001992021279875189\n",
      "165: accuracy:0.5035460992907801 loss: 0.0002039228129433468\n",
      "166: accuracy:0.5035460992907801 loss: 0.00019450868421699852\n",
      "167: accuracy:0.49645390070921985 loss: 0.0001986231072805822\n",
      "168: accuracy:0.49645390070921985 loss: 0.0002106121537508443\n",
      "169: accuracy:0.49645390070921985 loss: 0.00020852770830970258\n",
      "170: accuracy:0.49645390070921985 loss: 0.0001819474418880418\n",
      "171: accuracy:0.48936170212765956 loss: 0.00020463806868065149\n",
      "172: accuracy:0.48936170212765956 loss: 0.00020091193437110633\n",
      "173: accuracy:0.48936170212765956 loss: 0.0001960618101293221\n",
      "174: accuracy:0.48936170212765956 loss: 0.00021967205975670367\n",
      "175: accuracy:0.48936170212765956 loss: 0.0002082961000269279\n",
      "176: accuracy:0.48936170212765956 loss: 0.00014991078933235258\n",
      "177: accuracy:0.48936170212765956 loss: 0.00018755368364509195\n",
      "178: accuracy:0.48936170212765956 loss: 0.00017895697965286672\n",
      "179: accuracy:0.48936170212765956 loss: 0.00019253321806900203\n",
      "180: accuracy:0.48226950354609927 loss: 0.00018980843015015125\n",
      "181: accuracy:0.48226950354609927 loss: 0.0001666886528255418\n",
      "182: accuracy:0.475177304964539 loss: 0.00018251282745040953\n",
      "183: accuracy:0.475177304964539 loss: 0.00018395014922134578\n",
      "184: accuracy:0.475177304964539 loss: 0.00017059870879165828\n",
      "185: accuracy:0.475177304964539 loss: 0.00020110266632400453\n",
      "186: accuracy:0.475177304964539 loss: 0.0001904760138131678\n",
      "187: accuracy:0.475177304964539 loss: 0.00016790117661003023\n",
      "188: accuracy:0.475177304964539 loss: 0.00017724718782119453\n",
      "189: accuracy:0.475177304964539 loss: 0.00019001279724761844\n",
      "190: accuracy:0.475177304964539 loss: 0.00017689296510070562\n",
      "191: accuracy:0.475177304964539 loss: 0.00020525796571746469\n",
      "192: accuracy:0.475177304964539 loss: 0.00015852792421355844\n",
      "193: accuracy:0.475177304964539 loss: 0.0001651627680985257\n",
      "194: accuracy:0.475177304964539 loss: 0.00017480850510764867\n",
      "195: accuracy:0.475177304964539 loss: 0.00015745163545943797\n",
      "196: accuracy:0.475177304964539 loss: 0.00015454973618034273\n",
      "197: accuracy:0.475177304964539 loss: 0.00013498579210136086\n",
      "198: accuracy:0.475177304964539 loss: 0.0001844951038947329\n",
      "199: accuracy:0.475177304964539 loss: 0.00014715194993186742\n",
      "200: accuracy:0.475177304964539 loss: 0.00015081677702255547\n",
      "201: accuracy:0.475177304964539 loss: 0.00017288753588218242\n",
      "202: accuracy:0.475177304964539 loss: 0.00015273776079993695\n",
      "203: accuracy:0.475177304964539 loss: 0.0001434462465113029\n",
      "204: accuracy:0.475177304964539 loss: 0.00016907283861655742\n",
      "205: accuracy:0.475177304964539 loss: 0.00011828286369564012\n",
      "206: accuracy:0.475177304964539 loss: 0.00013084411330055445\n",
      "207: accuracy:0.475177304964539 loss: 0.00015174321015365422\n",
      "208: accuracy:0.475177304964539 loss: 0.0001340593589702621\n",
      "209: accuracy:0.475177304964539 loss: 0.000170625964528881\n",
      "210: accuracy:0.475177304964539 loss: 0.0001379149325657636\n",
      "211: accuracy:0.475177304964539 loss: 0.0001417296298313886\n",
      "212: accuracy:0.475177304964539 loss: 0.00015606198576278985\n",
      "213: accuracy:0.475177304964539 loss: 0.00014239719894248992\n",
      "214: accuracy:0.475177304964539 loss: 0.00016033989959396422\n",
      "215: accuracy:0.475177304964539 loss: 0.0001369067613268271\n",
      "216: accuracy:0.475177304964539 loss: 0.0001377242006128654\n",
      "217: accuracy:0.475177304964539 loss: 0.00013013566785957664\n",
      "218: accuracy:0.475177304964539 loss: 0.00015820094267837703\n",
      "219: accuracy:0.475177304964539 loss: 0.00013474056322593242\n",
      "220: accuracy:0.475177304964539 loss: 0.00013489041884895414\n",
      "221: accuracy:0.475177304964539 loss: 0.0001446042733732611\n",
      "222: accuracy:0.475177304964539 loss: 0.00014574867964256555\n",
      "223: accuracy:0.475177304964539 loss: 0.0001220703125\n",
      "224: accuracy:0.475177304964539 loss: 0.00011836460907943547\n",
      "225: accuracy:0.46808510638297873 loss: 0.00012810570478904992\n",
      "226: accuracy:0.46808510638297873 loss: 0.0001042229778249748\n",
      "227: accuracy:0.46808510638297873 loss: 0.00015031269867904484\n",
      "228: accuracy:0.46808510638297873 loss: 0.00012716565106529742\n",
      "229: accuracy:0.46808510638297873 loss: 0.00013380050950217992\n",
      "230: accuracy:0.46808510638297873 loss: 0.0001396179141011089\n",
      "231: accuracy:0.46808510638297873 loss: 0.00011911392357433215\n",
      "232: accuracy:0.46808510638297873 loss: 0.00012048993812641129\n",
      "233: accuracy:0.46808510638297873 loss: 0.00015938622527755797\n",
      "234: accuracy:0.46808510638297873 loss: 0.00011935915244976059\n",
      "235: accuracy:0.46808510638297873 loss: 0.00010036741150543094\n",
      "236: accuracy:0.46808510638297873 loss: 0.00012446811888366938\n",
      "237: accuracy:0.46808510638297873 loss: 0.00012380054977256805\n",
      "238: accuracy:0.46808510638297873 loss: 0.00012523106124717742\n",
      "239: accuracy:0.46808510638297873 loss: 0.00010678427497623488\n",
      "240: accuracy:0.46808510638297873 loss: 0.00012073516700183973\n",
      "241: accuracy:0.46808510638297873 loss: 0.00012852804502472281\n",
      "242: accuracy:0.46808510638297873 loss: 0.00012354170030448586\n",
      "243: accuracy:0.46808510638297873 loss: 0.00011386871483409777\n",
      "244: accuracy:0.46808510638297873 loss: 0.00012092590623069555\n",
      "245: accuracy:0.46808510638297873 loss: 0.00010640280379448086\n",
      "246: accuracy:0.46808510638297873 loss: 0.00012314660125412047\n",
      "247: accuracy:0.46808510638297873 loss: 0.0001153809716925025\n",
      "248: accuracy:0.46808510638297873 loss: 0.000102996826171875\n",
      "249: accuracy:0.46808510638297873 loss: 9.716578642837703e-05\n",
      "250: accuracy:0.46808510638297873 loss: 0.0001014437002595514\n",
      "251: accuracy:0.46808510638297873 loss: 0.00011739730689441785\n",
      "252: accuracy:0.46808510638297873 loss: 0.00010268347250530496\n",
      "253: accuracy:0.46808510638297873 loss: 0.0001056807377608493\n",
      "254: accuracy:0.46808510638297873 loss: 0.00010163443221244961\n",
      "255: accuracy:0.46808510638297873 loss: 0.00011230196105316281\n",
      "256: accuracy:0.46808510638297873 loss: 0.00010498592018848285\n",
      "257: accuracy:0.46808510638297873 loss: 0.00011286054359516129\n",
      "258: accuracy:0.46808510638297873 loss: 0.00010002680937759578\n",
      "259: accuracy:0.46808510638297873 loss: 9.833744843490422e-05\n",
      "260: accuracy:0.46808510638297873 loss: 0.00011107580940006301\n",
      "261: accuracy:0.46808510638297873 loss: 8.508137398166582e-05\n",
      "262: accuracy:0.46808510638297873 loss: 9.655271423980594e-05\n",
      "263: accuracy:0.46808510638297873 loss: 0.00010534014290897176\n",
      "264: accuracy:0.46808510638297873 loss: 9.67570740613155e-05\n",
      "265: accuracy:0.46808510638297873 loss: 0.00010316030966350809\n",
      "266: accuracy:0.46808510638297873 loss: 0.00010495867172721773\n",
      "267: accuracy:0.46808510638297873 loss: 9.71794142969884e-05\n",
      "268: accuracy:0.46808510638297873 loss: 8.362361404579133e-05\n",
      "269: accuracy:0.46808510638297873 loss: 9.83101999736391e-05\n",
      "270: accuracy:0.46808510638297873 loss: 9.875978867057711e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271: accuracy:0.46808510638297873 loss: 0.00010514940368011594\n",
      "272: accuracy:0.46808510638297873 loss: 8.888244337867945e-05\n",
      "273: accuracy:0.46808510638297873 loss: 0.00010717936675064266\n",
      "274: accuracy:0.46808510638297873 loss: 8.746555977268144e-05\n",
      "275: accuracy:0.46808510638297873 loss: 9.362356649944559e-05\n",
      "276: accuracy:0.46808510638297873 loss: 9.712491737445816e-05\n",
      "277: accuracy:0.46808510638297873 loss: 8.167539635905996e-05\n",
      "278: accuracy:0.46808510638297873 loss: 0.00010925020615104586\n",
      "279: accuracy:0.46808510638297873 loss: 0.00010566711716819555\n",
      "280: accuracy:0.475177304964539 loss: 0.00011508123861858621\n",
      "281: accuracy:0.475177304964539 loss: 7.611683395225555e-05\n",
      "282: accuracy:0.475177304964539 loss: 7.167543662944809e-05\n",
      "283: accuracy:0.475177304964539 loss: 8.178438292816281e-05\n",
      "284: accuracy:0.475177304964539 loss: 7.700239075347781e-05\n",
      "285: accuracy:0.475177304964539 loss: 9.832382056629285e-05\n",
      "286: accuracy:0.475177304964539 loss: 8.119855920085683e-05\n",
      "287: accuracy:0.475177304964539 loss: 8.493151108268648e-05\n",
      "288: accuracy:0.475177304964539 loss: 8.919579704524949e-05\n",
      "289: accuracy:0.475177304964539 loss: 7.472719153156504e-05\n",
      "290: accuracy:0.475177304964539 loss: 9.039470023708418e-05\n",
      "291: accuracy:0.475177304964539 loss: 8.727482054382563e-05\n",
      "292: accuracy:0.475177304964539 loss: 9.252002928406e-05\n",
      "293: accuracy:0.475177304964539 loss: 7.726124022156e-05\n",
      "294: accuracy:0.475177304964539 loss: 8.28879201435484e-05\n",
      "295: accuracy:0.475177304964539 loss: 9.578977187629789e-05\n",
      "296: accuracy:0.475177304964539 loss: 7.93320796219632e-05\n",
      "297: accuracy:0.475177304964539 loss: 8.359636558452621e-05\n",
      "298: accuracy:0.475177304964539 loss: 8.41140717966482e-05\n",
      "299: accuracy:0.475177304964539 loss: 7.852826820453629e-05\n",
      "300: accuracy:0.475177304964539 loss: 7.226126763271168e-05\n",
      "301: accuracy:0.475177304964539 loss: 7.250649650814012e-05\n",
      "302: accuracy:0.475177304964539 loss: 8.69614741532132e-05\n",
      "303: accuracy:0.475177304964539 loss: 8.803776290733367e-05\n",
      "304: accuracy:0.475177304964539 loss: 8.536747191101313e-05\n",
      "305: accuracy:0.475177304964539 loss: 7.935932808322832e-05\n",
      "306: accuracy:0.475177304964539 loss: 7.303783058887348e-05\n",
      "307: accuracy:0.475177304964539 loss: 6.980895705055445e-05\n",
      "308: accuracy:0.475177304964539 loss: 8.118493133224547e-05\n",
      "309: accuracy:0.475177304964539 loss: 9.266989218303934e-05\n",
      "310: accuracy:0.475177304964539 loss: 7.167543662944809e-05\n",
      "311: accuracy:0.475177304964539 loss: 7.576261123176664e-05\n",
      "312: accuracy:0.475177304964539 loss: 6.818771362304688e-05\n",
      "313: accuracy:0.475177304964539 loss: 6.96046045050025e-05\n",
      "314: accuracy:0.475177304964539 loss: 7.771083619445562e-05\n",
      "315: accuracy:0.475177304964539 loss: 7.701601134613156e-05\n",
      "316: accuracy:0.475177304964539 loss: 7.100786751834676e-05\n",
      "317: accuracy:0.475177304964539 loss: 8.163452002918348e-05\n",
      "318: accuracy:0.475177304964539 loss: 8.054461068240926e-05\n",
      "319: accuracy:0.475177304964539 loss: 7.401875336654484e-05\n",
      "320: accuracy:0.475177304964539 loss: 7.25746140233241e-05\n",
      "321: accuracy:0.475177304964539 loss: 6.622586806770414e-05\n",
      "322: accuracy:0.475177304964539 loss: 7.893698784755543e-05\n",
      "323: accuracy:0.475177304964539 loss: 7.177080260589719e-05\n",
      "324: accuracy:0.475177304964539 loss: 8.292879647342488e-05\n",
      "325: accuracy:0.475177304964539 loss: 7.149832526920363e-05\n",
      "326: accuracy:0.475177304964539 loss: 7.078987982822582e-05\n",
      "327: accuracy:0.475177304964539 loss: 7.396425644401461e-05\n",
      "328: accuracy:0.475177304964539 loss: 6.405966269085184e-05\n",
      "329: accuracy:0.475177304964539 loss: 6.09397902735509e-05\n",
      "330: accuracy:0.475177304964539 loss: 7.540838851127774e-05\n",
      "331: accuracy:0.475177304964539 loss: 6.993157876422629e-05\n",
      "332: accuracy:0.475177304964539 loss: 7.43865966796875e-05\n",
      "333: accuracy:0.475177304964539 loss: 6.0749054682673886e-05\n",
      "334: accuracy:0.475177304964539 loss: 6.871904770378023e-05\n",
      "335: accuracy:0.475177304964539 loss: 5.8133260608883575e-05\n",
      "336: accuracy:0.475177304964539 loss: 6.241117080207914e-05\n",
      "337: accuracy:0.475177304964539 loss: 6.783349090255797e-05\n",
      "338: accuracy:0.475177304964539 loss: 5.8024270401801914e-05\n",
      "339: accuracy:0.475177304964539 loss: 5.756105747423135e-05\n",
      "340: accuracy:0.475177304964539 loss: 5.055836300016381e-05\n",
      "341: accuracy:0.475177304964539 loss: 6.474085967056453e-05\n",
      "342: accuracy:0.475177304964539 loss: 6.213869346538559e-05\n",
      "343: accuracy:0.475177304964539 loss: 6.311961624305695e-05\n",
      "344: accuracy:0.475177304964539 loss: 6.596701132366434e-05\n",
      "345: accuracy:0.475177304964539 loss: 7.284709136001766e-05\n",
      "346: accuracy:0.475177304964539 loss: 6.031309021636844e-05\n",
      "347: accuracy:0.475177304964539 loss: 6.603513611480594e-05\n",
      "348: accuracy:0.475177304964539 loss: 7.02176766935736e-05\n",
      "349: accuracy:0.475177304964539 loss: 5.919592877035029e-05\n",
      "350: accuracy:0.475177304964539 loss: 6.585802475456148e-05\n",
      "351: accuracy:0.475177304964539 loss: 6.850106728961691e-05\n",
      "352: accuracy:0.475177304964539 loss: 6.31332368357107e-05\n",
      "353: accuracy:0.475177304964539 loss: 6.64983454043977e-05\n",
      "354: accuracy:0.475177304964539 loss: 5.593981040874496e-05\n",
      "355: accuracy:0.475177304964539 loss: 6.238392234081402e-05\n",
      "356: accuracy:0.475177304964539 loss: 6.0558319091796875e-05\n",
      "357: accuracy:0.475177304964539 loss: 5.855560448253527e-05\n",
      "358: accuracy:0.475177304964539 loss: 5.930491897743195e-05\n",
      "359: accuracy:0.475177304964539 loss: 6.544930511154234e-05\n",
      "360: accuracy:0.475177304964539 loss: 6.021772060194053e-05\n",
      "361: accuracy:0.475177304964539 loss: 6.961822509765625e-05\n",
      "362: accuracy:0.475177304964539 loss: 6.414140807464719e-05\n",
      "363: accuracy:0.475177304964539 loss: 6.62394959363155e-05\n",
      "364: accuracy:0.475177304964539 loss: 6.816046516178176e-05\n",
      "365: accuracy:0.475177304964539 loss: 6.653922173427418e-05\n",
      "366: accuracy:0.475177304964539 loss: 6.79833538015373e-05\n",
      "367: accuracy:0.475177304964539 loss: 5.3255898819770664e-05\n",
      "368: accuracy:0.475177304964539 loss: 5.5994307331275195e-05\n",
      "369: accuracy:0.475177304964539 loss: 5.404608600656502e-05\n",
      "370: accuracy:0.475177304964539 loss: 6.528582161990926e-05\n",
      "371: accuracy:0.475177304964539 loss: 5.5803571740398183e-05\n",
      "372: accuracy:0.475177304964539 loss: 5.843298640684225e-05\n",
      "373: accuracy:0.475177304964539 loss: 4.9958911404246464e-05\n",
      "374: accuracy:0.475177304964539 loss: 5.3133284382056445e-05\n",
      "375: accuracy:0.475177304964539 loss: 5.859647353645414e-05\n",
      "376: accuracy:0.475177304964539 loss: 6.995881994953379e-05\n",
      "377: accuracy:0.475177304964539 loss: 5.7697296142578125e-05\n",
      "378: accuracy:0.475177304964539 loss: 4.8909867473412305e-05\n",
      "379: accuracy:0.475177304964539 loss: 5.224772758083418e-05\n",
      "380: accuracy:0.475177304964539 loss: 5.200249870540574e-05\n",
      "381: accuracy:0.475177304964539 loss: 5.446842624223791e-05\n",
      "382: accuracy:0.475177304964539 loss: 6.429127097362652e-05\n",
      "383: accuracy:0.475177304964539 loss: 5.975450767436996e-05\n",
      "384: accuracy:0.475177304964539 loss: 6.0449328884715214e-05\n",
      "385: accuracy:0.475177304964539 loss: 5.647114448947832e-05\n",
      "386: accuracy:0.475177304964539 loss: 5.767004768131301e-05\n",
      "387: accuracy:0.475177304964539 loss: 5.3773608669871464e-05\n",
      "388: accuracy:0.475177304964539 loss: 5.213873737375252e-05\n",
      "389: accuracy:0.475177304964539 loss: 5.222047911956906e-05\n",
      "390: accuracy:0.475177304964539 loss: 5.320140189724043e-05\n",
      "391: accuracy:0.475177304964539 loss: 4.870551128988154e-05\n",
      "392: accuracy:0.475177304964539 loss: 4.521778828348033e-05\n",
      "393: accuracy:0.475177304964539 loss: 5.841936217620969e-05\n",
      "394: accuracy:0.475177304964539 loss: 5.2915300329914317e-05\n",
      "395: accuracy:0.475177304964539 loss: 4.649843322113156e-05\n",
      "396: accuracy:0.475177304964539 loss: 5.445480201160535e-05\n",
      "397: accuracy:0.475177304964539 loss: 5.341938594938256e-05\n",
      "398: accuracy:0.475177304964539 loss: 5.1334925956325606e-05\n",
      "399: accuracy:0.475177304964539 loss: 4.788807564182207e-05\n",
      "400: accuracy:0.475177304964539 loss: 5.051749030826613e-05\n",
      "401: accuracy:0.475177304964539 loss: 5.1852635806426406e-05\n",
      "402: accuracy:0.475177304964539 loss: 5.053111453889869e-05\n",
      "403: accuracy:0.475177304964539 loss: 4.9672806198941544e-05\n",
      "404: accuracy:0.475177304964539 loss: 5.3446634410647675e-05\n",
      "405: accuracy:0.475177304964539 loss: 4.929133865516633e-05\n",
      "406: accuracy:0.475177304964539 loss: 4.404612991493195e-05\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    ")\n",
    "net = RNN_old()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "for epoch in range(1,501): \n",
    "    for step, (batch_x, sen, batch_y) in enumerate(loader):\n",
    "        pred = net(sen, batch_x)\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 2: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_s, t_e)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-new 0.6737588652482269 17轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "MAX_LENGTH = 53\n",
    "for i in range(0,467):\n",
    "    a = np.loadtxt('/Users/denhiroshi/Downloads/Embeding/ans_1.0_'+str(i)+'.csv',dtype=np.float64,delimiter=',')\n",
    "    a = a.transpose(1,0)\n",
    "    a = np.exp(a)\n",
    "    a = np.sum(a,axis=1)\n",
    "    a = np.log(a)\n",
    "    if a.shape[0] < MAX_LENGTH:\n",
    "        add = MAX_LENGTH - a.shape[0]\n",
    "        adds = np.zeros(add)\n",
    "        a = np.concatenate((a, adds))\n",
    "    inputs.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use = np.stack(inputs)\n",
    "mean = use.mean()         #计算平均数\n",
    "deviation = use.std()     #计算标准差\n",
    "# 标准化数据的公式: (数据值 - 平均数) / 标准差\n",
    "use = (use - mean) / deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.fromfile('glove_WV.dat',dtype=np.float64).reshape((467,-1,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((467, 53, 200), (467, 53))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape,use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for i in range(467):\n",
    "    inputs.append((torch.from_numpy(b[i]),torch.from_numpy(use[i]),Y[i]))\n",
    "shuffle(inputs)\n",
    "flag = int(len(inputs)*0.7)\n",
    "train = inputs[:flag]\n",
    "test = inputs[flag:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([141, 53]), torch.Size([141, 53, 200]), torch.Size([141]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_s = []\n",
    "t_e = []\n",
    "t_y = []\n",
    "for E,S,Y in test:\n",
    "    t_s.append(S)\n",
    "    t_e.append(E)\n",
    "    t_y.append(Y)\n",
    "t_s = torch.stack(t_s)\n",
    "t_e = torch.stack(t_e)\n",
    "t_y = torch.from_numpy(np.array(t_y))\n",
    "t_s.shape, t_e.shape, t_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size = 200,       # rnn input\n",
    "            hidden_size = 64,     # rnn hidden unit\n",
    "            num_layers = 1,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=65 ,out_features=7)\n",
    "        self.maxpool = torch.nn.MaxPool2d((53,1))\n",
    "\n",
    "    def forward(self,sen,x):\n",
    "        output,h_n=self.rnn(x.float())\n",
    "        sen = sen.reshape((sen.shape[0],sen.shape[1],1))  # (BATCH_SIZE,53,1)\n",
    "        output1 = torch.bmm(torch.transpose(output,1,2).float(),sen.float())  # (BATCH_SIZE,64,1)\n",
    "        output1 = torch.sigmoid(output1)\n",
    "        output1 = torch.softmax(output1,1)\n",
    "        output1 = torch.bmm(output,output1) # (BATCH_SIZE,53,1)\n",
    "        output = torch.cat((output1,output),2) # (BATCH_SIZE,53,65)\n",
    "        output = self.maxpool(output)   # (BATCH_SIZE,1,65)\n",
    "        output = output.reshape((output.shape[0],-1)) # (BATCH_SIZE,7)\n",
    "        x1 = self.out(output.float())\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): GRU(200, 64, batch_first=True)\n",
      "  (out): Linear(in_features=65, out_features=7, bias=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=(53, 1), stride=(53, 1), padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "1: accuracy:0.2695035460992908 loss: 1.9541912078857422\n",
      "2: accuracy:0.3900709219858156 loss: 1.772621750831604\n",
      "3: accuracy:0.3333333333333333 loss: 1.606994390487671\n",
      "4: accuracy:0.4326241134751773 loss: 1.4174365997314453\n",
      "5: accuracy:0.5177304964539007 loss: 1.2522685527801514\n",
      "6: accuracy:0.5673758865248227 loss: 1.1295371055603027\n",
      "7: accuracy:0.5531914893617021 loss: 0.9108871817588806\n",
      "8: accuracy:0.6028368794326241 loss: 0.7546970844268799\n",
      "9: accuracy:0.574468085106383 loss: 0.5393237471580505\n",
      "10: accuracy:0.6170212765957447 loss: 0.4772818088531494\n",
      "11: accuracy:0.624113475177305 loss: 0.37971144914627075\n",
      "12: accuracy:0.6312056737588653 loss: 0.25231868028640747\n",
      "13: accuracy:0.6312056737588653 loss: 0.18982568383216858\n",
      "14: accuracy:0.6382978723404256 loss: 0.11684030294418335\n",
      "15: accuracy:0.6099290780141844 loss: 0.10033412277698517\n",
      "16: accuracy:0.6453900709219859 loss: 0.07209061086177826\n",
      "17: accuracy:0.6737588652482269 loss: 0.05450940877199173\n",
      "18: accuracy:0.6524822695035462 loss: 0.03416745364665985\n",
      "19: accuracy:0.6453900709219859 loss: 0.025624504312872887\n",
      "20: accuracy:0.6453900709219859 loss: 0.020326849073171616\n",
      "21: accuracy:0.6524822695035462 loss: 0.015299282968044281\n",
      "22: accuracy:0.6382978723404256 loss: 0.012985123321413994\n",
      "23: accuracy:0.6382978723404256 loss: 0.012228613719344139\n",
      "24: accuracy:0.6453900709219859 loss: 0.009806431829929352\n",
      "25: accuracy:0.6382978723404256 loss: 0.007394365966320038\n",
      "26: accuracy:0.6524822695035462 loss: 0.023011013865470886\n",
      "27: accuracy:0.6595744680851063 loss: 0.005586275830864906\n",
      "28: accuracy:0.6524822695035462 loss: 0.005437813699245453\n",
      "29: accuracy:0.6524822695035462 loss: 0.005268530920147896\n",
      "30: accuracy:0.6382978723404256 loss: 0.0050008296966552734\n",
      "31: accuracy:0.6382978723404256 loss: 0.004099611192941666\n",
      "32: accuracy:0.6453900709219859 loss: 0.004479538649320602\n",
      "33: accuracy:0.6382978723404256 loss: 0.0036450400948524475\n",
      "34: accuracy:0.6382978723404256 loss: 0.0032815076410770416\n",
      "35: accuracy:0.6312056737588653 loss: 0.0031014420092105865\n",
      "36: accuracy:0.6453900709219859 loss: 0.0026360228657722473\n",
      "37: accuracy:0.6382978723404256 loss: 0.002539433538913727\n",
      "38: accuracy:0.6382978723404256 loss: 0.0027378909289836884\n",
      "39: accuracy:0.6524822695035462 loss: 0.0023946426808834076\n",
      "40: accuracy:0.6524822695035462 loss: 0.0022426769137382507\n",
      "41: accuracy:0.6595744680851063 loss: 0.002091839909553528\n",
      "42: accuracy:0.6595744680851063 loss: 0.0021924450993537903\n",
      "43: accuracy:0.6595744680851063 loss: 0.0020485669374465942\n",
      "44: accuracy:0.6524822695035462 loss: 0.001966126263141632\n",
      "45: accuracy:0.6524822695035462 loss: 0.001816626638174057\n",
      "46: accuracy:0.6524822695035462 loss: 0.0018643513321876526\n",
      "47: accuracy:0.6524822695035462 loss: 0.0017486922442913055\n",
      "48: accuracy:0.6453900709219859 loss: 0.0019716545939445496\n",
      "49: accuracy:0.6453900709219859 loss: 0.0020803920924663544\n",
      "50: accuracy:0.6453900709219859 loss: 0.0016222521662712097\n",
      "51: accuracy:0.6453900709219859 loss: 0.0017589479684829712\n",
      "52: accuracy:0.6524822695035462 loss: 0.001497700810432434\n",
      "53: accuracy:0.6524822695035462 loss: 0.0014972537755966187\n",
      "54: accuracy:0.6524822695035462 loss: 0.0014978386461734772\n",
      "55: accuracy:0.6453900709219859 loss: 0.0015304386615753174\n",
      "56: accuracy:0.6382978723404256 loss: 0.0016073733568191528\n",
      "57: accuracy:0.6312056737588653 loss: 0.0016477778553962708\n",
      "58: accuracy:0.6312056737588653 loss: 0.0013682655990123749\n",
      "59: accuracy:0.6312056737588653 loss: 0.0014086253941059113\n",
      "60: accuracy:0.6453900709219859 loss: 0.001437649130821228\n",
      "61: accuracy:0.6453900709219859 loss: 0.0013026036322116852\n",
      "62: accuracy:0.6453900709219859 loss: 0.0010991394519805908\n",
      "63: accuracy:0.6453900709219859 loss: 0.0012199953198432922\n",
      "64: accuracy:0.6453900709219859 loss: 0.0013217441737651825\n",
      "65: accuracy:0.6453900709219859 loss: 0.0013141520321369171\n",
      "66: accuracy:0.6453900709219859 loss: 0.001140505075454712\n",
      "67: accuracy:0.6382978723404256 loss: 0.0010871216654777527\n",
      "68: accuracy:0.6382978723404256 loss: 0.0012781359255313873\n",
      "69: accuracy:0.6382978723404256 loss: 0.0011217035353183746\n",
      "70: accuracy:0.6453900709219859 loss: 0.0009665563702583313\n",
      "71: accuracy:0.6453900709219859 loss: 0.0012790821492671967\n",
      "72: accuracy:0.6453900709219859 loss: 0.0011628605425357819\n",
      "73: accuracy:0.6453900709219859 loss: 0.0011135190725326538\n",
      "74: accuracy:0.6453900709219859 loss: 0.0010425783693790436\n",
      "75: accuracy:0.6453900709219859 loss: 0.0010488219559192657\n",
      "76: accuracy:0.6453900709219859 loss: 0.000896647572517395\n",
      "77: accuracy:0.6453900709219859 loss: 0.0009616315364837646\n",
      "78: accuracy:0.6453900709219859 loss: 0.0009283311665058136\n",
      "79: accuracy:0.6453900709219859 loss: 0.0008399747312068939\n",
      "80: accuracy:0.6453900709219859 loss: 0.000909656286239624\n",
      "81: accuracy:0.6453900709219859 loss: 0.0009126476943492889\n",
      "82: accuracy:0.6453900709219859 loss: 0.0008199475705623627\n",
      "83: accuracy:0.6453900709219859 loss: 0.0009473450481891632\n",
      "84: accuracy:0.6453900709219859 loss: 0.0009251013398170471\n",
      "85: accuracy:0.6453900709219859 loss: 0.0009229220449924469\n",
      "86: accuracy:0.6453900709219859 loss: 0.0007088668644428253\n",
      "87: accuracy:0.6453900709219859 loss: 0.0007070079445838928\n",
      "88: accuracy:0.6453900709219859 loss: 0.0008158385753631592\n",
      "89: accuracy:0.6453900709219859 loss: 0.0009458214044570923\n",
      "90: accuracy:0.6453900709219859 loss: 0.0008577294647693634\n",
      "91: accuracy:0.6453900709219859 loss: 0.000712420791387558\n",
      "92: accuracy:0.6453900709219859 loss: 0.0007322579622268677\n",
      "93: accuracy:0.6453900709219859 loss: 0.000704057514667511\n",
      "94: accuracy:0.6453900709219859 loss: 0.0006478205323219299\n",
      "95: accuracy:0.6453900709219859 loss: 0.0007864460349082947\n",
      "96: accuracy:0.6453900709219859 loss: 0.0006579384207725525\n",
      "97: accuracy:0.6453900709219859 loss: 0.0008106231689453125\n",
      "98: accuracy:0.6453900709219859 loss: 0.0007355883717536926\n",
      "99: accuracy:0.6453900709219859 loss: 0.0007903911173343658\n",
      "100: accuracy:0.6453900709219859 loss: 0.0006497763097286224\n",
      "101: accuracy:0.6453900709219859 loss: 0.0007098168134689331\n",
      "102: accuracy:0.6453900709219859 loss: 0.000749979168176651\n",
      "103: accuracy:0.6453900709219859 loss: 0.000613287091255188\n",
      "104: accuracy:0.6453900709219859 loss: 0.0006865337491035461\n",
      "105: accuracy:0.6524822695035462 loss: 0.0005539692938327789\n",
      "106: accuracy:0.6524822695035462 loss: 0.0006400160491466522\n",
      "107: accuracy:0.6524822695035462 loss: 0.0006059780716896057\n",
      "108: accuracy:0.6524822695035462 loss: 0.000636804848909378\n",
      "109: accuracy:0.6453900709219859 loss: 0.0006522424519062042\n",
      "110: accuracy:0.6453900709219859 loss: 0.0005869269371032715\n",
      "111: accuracy:0.6453900709219859 loss: 0.0005461238324642181\n",
      "112: accuracy:0.6453900709219859 loss: 0.0006235130131244659\n",
      "113: accuracy:0.6453900709219859 loss: 0.000686008483171463\n",
      "114: accuracy:0.6382978723404256 loss: 0.0005885213613510132\n",
      "115: accuracy:0.6382978723404256 loss: 0.0005823522806167603\n",
      "116: accuracy:0.6453900709219859 loss: 0.0005035251379013062\n",
      "117: accuracy:0.6453900709219859 loss: 0.0005701594054698944\n",
      "118: accuracy:0.6453900709219859 loss: 0.0004726238548755646\n",
      "119: accuracy:0.6453900709219859 loss: 0.000547405332326889\n",
      "120: accuracy:0.6453900709219859 loss: 0.0006389915943145752\n",
      "121: accuracy:0.6382978723404256 loss: 0.0005852058529853821\n",
      "122: accuracy:0.6382978723404256 loss: 0.0006023980677127838\n",
      "123: accuracy:0.6382978723404256 loss: 0.0005347020924091339\n",
      "124: accuracy:0.6312056737588653 loss: 0.0005143508315086365\n",
      "125: accuracy:0.6312056737588653 loss: 0.0005027018487453461\n",
      "126: accuracy:0.6312056737588653 loss: 0.0005009286105632782\n",
      "127: accuracy:0.6312056737588653 loss: 0.0004614144563674927\n",
      "128: accuracy:0.6312056737588653 loss: 0.0004107840359210968\n",
      "129: accuracy:0.6312056737588653 loss: 0.0005002692341804504\n",
      "130: accuracy:0.6312056737588653 loss: 0.0005281604826450348\n",
      "131: accuracy:0.6312056737588653 loss: 0.000498652458190918\n",
      "132: accuracy:0.6312056737588653 loss: 0.0004076436161994934\n",
      "133: accuracy:0.6312056737588653 loss: 0.00047869980335235596\n",
      "134: accuracy:0.6312056737588653 loss: 0.0005404055118560791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135: accuracy:0.6312056737588653 loss: 0.0005545839667320251\n",
      "136: accuracy:0.6312056737588653 loss: 0.00046394020318984985\n",
      "137: accuracy:0.6382978723404256 loss: 0.0004654601216316223\n",
      "138: accuracy:0.6382978723404256 loss: 0.00037739425897598267\n",
      "139: accuracy:0.6382978723404256 loss: 0.0004795156419277191\n",
      "140: accuracy:0.6382978723404256 loss: 0.0004212707281112671\n",
      "141: accuracy:0.6382978723404256 loss: 0.0005138963460922241\n",
      "142: accuracy:0.6382978723404256 loss: 0.00042288750410079956\n",
      "143: accuracy:0.6382978723404256 loss: 0.00043049082159996033\n",
      "144: accuracy:0.6382978723404256 loss: 0.000390518456697464\n",
      "145: accuracy:0.6382978723404256 loss: 0.0003867931663990021\n",
      "146: accuracy:0.6382978723404256 loss: 0.00042531639337539673\n",
      "147: accuracy:0.6382978723404256 loss: 0.00036405399441719055\n",
      "148: accuracy:0.6382978723404256 loss: 0.00033909454941749573\n",
      "149: accuracy:0.6382978723404256 loss: 0.00040668994188308716\n",
      "150: accuracy:0.6382978723404256 loss: 0.00033704936504364014\n",
      "151: accuracy:0.6382978723404256 loss: 0.0003558211028575897\n",
      "152: accuracy:0.6382978723404256 loss: 0.0004135817289352417\n",
      "153: accuracy:0.6312056737588653 loss: 0.00033346936106681824\n",
      "154: accuracy:0.6312056737588653 loss: 0.0003212094306945801\n",
      "155: accuracy:0.6312056737588653 loss: 0.0003913380205631256\n",
      "156: accuracy:0.6312056737588653 loss: 0.0003685951232910156\n",
      "157: accuracy:0.6312056737588653 loss: 0.0003195442259311676\n",
      "158: accuracy:0.6312056737588653 loss: 0.0004136301577091217\n",
      "159: accuracy:0.6382978723404256 loss: 0.00035689398646354675\n",
      "160: accuracy:0.6382978723404256 loss: 0.0003869868814945221\n",
      "161: accuracy:0.6382978723404256 loss: 0.000354226678609848\n",
      "162: accuracy:0.6382978723404256 loss: 0.0003445819020271301\n",
      "163: accuracy:0.6382978723404256 loss: 0.000332564115524292\n",
      "164: accuracy:0.6382978723404256 loss: 0.0003206506371498108\n",
      "165: accuracy:0.6382978723404256 loss: 0.00038226693868637085\n",
      "166: accuracy:0.6312056737588653 loss: 0.000359315425157547\n",
      "167: accuracy:0.6312056737588653 loss: 0.00034715980291366577\n",
      "168: accuracy:0.6312056737588653 loss: 0.0003042668104171753\n",
      "169: accuracy:0.6312056737588653 loss: 0.00030785053968429565\n",
      "170: accuracy:0.6312056737588653 loss: 0.0003606453537940979\n",
      "171: accuracy:0.6312056737588653 loss: 0.0002716854214668274\n",
      "172: accuracy:0.6312056737588653 loss: 0.00033744052052497864\n",
      "173: accuracy:0.6312056737588653 loss: 0.0003258734941482544\n",
      "174: accuracy:0.6312056737588653 loss: 0.00029391422867774963\n",
      "175: accuracy:0.6312056737588653 loss: 0.00035595521330833435\n",
      "176: accuracy:0.6312056737588653 loss: 0.0002869553864002228\n",
      "177: accuracy:0.6312056737588653 loss: 0.0002927333116531372\n",
      "178: accuracy:0.6312056737588653 loss: 0.0002942308783531189\n",
      "179: accuracy:0.6312056737588653 loss: 0.00033984333276748657\n",
      "180: accuracy:0.6312056737588653 loss: 0.00028996914625167847\n",
      "181: accuracy:0.6312056737588653 loss: 0.00027012452483177185\n",
      "182: accuracy:0.6312056737588653 loss: 0.00032458454370498657\n",
      "183: accuracy:0.6312056737588653 loss: 0.00028292834758758545\n",
      "184: accuracy:0.6312056737588653 loss: 0.0003310069441795349\n",
      "185: accuracy:0.6312056737588653 loss: 0.00031556934118270874\n",
      "186: accuracy:0.6312056737588653 loss: 0.0003215372562408447\n",
      "187: accuracy:0.6312056737588653 loss: 0.0003068596124649048\n",
      "188: accuracy:0.6312056737588653 loss: 0.00032820925116539\n",
      "189: accuracy:0.6312056737588653 loss: 0.00024910271167755127\n",
      "190: accuracy:0.6312056737588653 loss: 0.0003098025918006897\n",
      "191: accuracy:0.6312056737588653 loss: 0.00024058297276496887\n",
      "192: accuracy:0.6312056737588653 loss: 0.0002310723066329956\n",
      "193: accuracy:0.6312056737588653 loss: 0.0002931654453277588\n",
      "194: accuracy:0.6312056737588653 loss: 0.00026369839906692505\n",
      "195: accuracy:0.6312056737588653 loss: 0.0003244951367378235\n",
      "196: accuracy:0.6312056737588653 loss: 0.00028672441840171814\n",
      "197: accuracy:0.6312056737588653 loss: 0.00029192864894866943\n",
      "198: accuracy:0.6312056737588653 loss: 0.00026096031069755554\n",
      "199: accuracy:0.6312056737588653 loss: 0.00026721134781837463\n",
      "200: accuracy:0.6312056737588653 loss: 0.00027931109070777893\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    "    drop_last=True,\n",
    ")\n",
    "net = RNN()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "for epoch in range(1,201): \n",
    "    for step, (batch_x, sen, batch_y) in enumerate(loader):\n",
    "        pred = net(sen, batch_x)\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 1: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_s, t_e)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-new-bi 0.7163120567375887 13轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Bi_RNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size = 200,       # rnn input\n",
    "            hidden_size = 64,     # rnn hidden unit\n",
    "            num_layers = 1,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=129 ,out_features=7)\n",
    "        self.maxpool = torch.nn.MaxPool2d((53,1))\n",
    "\n",
    "    def forward(self,sen,x):\n",
    "        output,h_n=self.rnn(x.float())\n",
    "        sen = sen.reshape((sen.shape[0],sen.shape[1],1))  # (BATCH_SIZE,53,1)\n",
    "        output1 = torch.bmm(torch.transpose(output,1,2).float(),sen.float())  # (BATCH_SIZE,64,1)\n",
    "        output1 = torch.sigmoid(output1)\n",
    "        output1 = torch.softmax(output1,1)\n",
    "        output1 = torch.bmm(output,output1) # (BATCH_SIZE,53,1)\n",
    "        output = torch.cat((output1,output),2) # (BATCH_SIZE,53,65)\n",
    "        output = self.maxpool(output)   # (BATCH_SIZE,1,65)\n",
    "        output = output.reshape((output.shape[0],-1)) # (BATCH_SIZE,7)\n",
    "        x1 = self.out(output.float())\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi_RNN(\n",
      "  (rnn): GRU(200, 64, batch_first=True, bidirectional=True)\n",
      "  (out): Linear(in_features=129, out_features=7, bias=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=(53, 1), stride=(53, 1), padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "1: accuracy:0.19148936170212766 loss: 1.9885358810424805\n",
      "2: accuracy:0.475177304964539 loss: 1.713677167892456\n",
      "3: accuracy:0.5106382978723404 loss: 1.5415892601013184\n",
      "4: accuracy:0.574468085106383 loss: 1.284982442855835\n",
      "5: accuracy:0.6524822695035462 loss: 1.1376138925552368\n",
      "6: accuracy:0.6595744680851063 loss: 0.8694663643836975\n",
      "7: accuracy:0.6382978723404256 loss: 0.6222941875457764\n",
      "8: accuracy:0.6808510638297872 loss: 0.5021540522575378\n",
      "9: accuracy:0.6737588652482269 loss: 0.351982980966568\n",
      "10: accuracy:0.6666666666666666 loss: 0.27762582898139954\n",
      "11: accuracy:0.6666666666666666 loss: 0.15613821148872375\n",
      "12: accuracy:0.6879432624113475 loss: 0.11653878539800644\n",
      "13: accuracy:0.6808510638297872 loss: 0.07489372789859772\n",
      "14: accuracy:0.7092198581560284 loss: 0.04798057675361633\n",
      "15: accuracy:0.6808510638297872 loss: 0.038257695734500885\n",
      "16: accuracy:0.6950354609929078 loss: 0.023460078984498978\n",
      "17: accuracy:0.6950354609929078 loss: 0.01732812263071537\n",
      "18: accuracy:0.6879432624113475 loss: 0.013389643281698227\n",
      "19: accuracy:0.6950354609929078 loss: 0.008291658014059067\n",
      "20: accuracy:0.6950354609929078 loss: 0.006599456071853638\n",
      "21: accuracy:0.6808510638297872 loss: 0.006002616137266159\n",
      "22: accuracy:0.6879432624113475 loss: 0.0054342783987522125\n",
      "23: accuracy:0.6879432624113475 loss: 0.004731383174657822\n",
      "24: accuracy:0.6737588652482269 loss: 0.0036879628896713257\n",
      "25: accuracy:0.6808510638297872 loss: 0.0030416883528232574\n",
      "26: accuracy:0.6808510638297872 loss: 0.0028004124760627747\n",
      "27: accuracy:0.6879432624113475 loss: 0.002284809947013855\n",
      "28: accuracy:0.6879432624113475 loss: 0.0021935701370239258\n",
      "29: accuracy:0.6737588652482269 loss: 0.001844055950641632\n",
      "30: accuracy:0.6737588652482269 loss: 0.0018519610166549683\n",
      "31: accuracy:0.6879432624113475 loss: 0.0016832277178764343\n",
      "32: accuracy:0.6879432624113475 loss: 0.0017137862741947174\n",
      "33: accuracy:0.6879432624113475 loss: 0.0014400258660316467\n",
      "34: accuracy:0.6808510638297872 loss: 0.0014697052538394928\n",
      "35: accuracy:0.6808510638297872 loss: 0.0012250952422618866\n",
      "36: accuracy:0.6879432624113475 loss: 0.0013251416385173798\n",
      "37: accuracy:0.6808510638297872 loss: 0.001266714185476303\n",
      "38: accuracy:0.6808510638297872 loss: 0.0012202411890029907\n",
      "39: accuracy:0.6808510638297872 loss: 0.0010292194783687592\n",
      "40: accuracy:0.6808510638297872 loss: 0.0010017156600952148\n",
      "41: accuracy:0.6879432624113475 loss: 0.0010178051888942719\n",
      "42: accuracy:0.6879432624113475 loss: 0.0010184943675994873\n",
      "43: accuracy:0.6879432624113475 loss: 0.0010322332382202148\n",
      "44: accuracy:0.6879432624113475 loss: 0.0009748935699462891\n",
      "45: accuracy:0.6808510638297872 loss: 0.0008963905274868011\n",
      "46: accuracy:0.6808510638297872 loss: 0.00098496675491333\n",
      "47: accuracy:0.6737588652482269 loss: 0.0008103922009468079\n",
      "48: accuracy:0.6808510638297872 loss: 0.0009302310645580292\n",
      "49: accuracy:0.6808510638297872 loss: 0.0007316544651985168\n",
      "50: accuracy:0.6879432624113475 loss: 0.0008451268076896667\n",
      "51: accuracy:0.6808510638297872 loss: 0.0007945708930492401\n",
      "52: accuracy:0.6808510638297872 loss: 0.0007493197917938232\n",
      "53: accuracy:0.6808510638297872 loss: 0.0007988736033439636\n",
      "54: accuracy:0.6808510638297872 loss: 0.0007722340524196625\n",
      "55: accuracy:0.6879432624113475 loss: 0.0007130242884159088\n",
      "56: accuracy:0.6879432624113475 loss: 0.0007104836404323578\n",
      "57: accuracy:0.6879432624113475 loss: 0.0007277354598045349\n",
      "58: accuracy:0.6950354609929078 loss: 0.00067172572016716\n",
      "59: accuracy:0.6950354609929078 loss: 0.0006995648145675659\n",
      "60: accuracy:0.6879432624113475 loss: 0.0007030703127384186\n",
      "61: accuracy:0.6879432624113475 loss: 0.0006193853914737701\n",
      "62: accuracy:0.6879432624113475 loss: 0.0006196722388267517\n",
      "63: accuracy:0.6879432624113475 loss: 0.000539686530828476\n",
      "64: accuracy:0.6879432624113475 loss: 0.0005684271454811096\n",
      "65: accuracy:0.6808510638297872 loss: 0.0006019771099090576\n",
      "66: accuracy:0.6808510638297872 loss: 0.0005167201161384583\n",
      "67: accuracy:0.6879432624113475 loss: 0.0005869418382644653\n",
      "68: accuracy:0.6879432624113475 loss: 0.0005822815001010895\n",
      "69: accuracy:0.6879432624113475 loss: 0.0005707107484340668\n",
      "70: accuracy:0.6879432624113475 loss: 0.0005489140748977661\n",
      "71: accuracy:0.6808510638297872 loss: 0.0005064010620117188\n",
      "72: accuracy:0.6808510638297872 loss: 0.0005202479660511017\n",
      "73: accuracy:0.6808510638297872 loss: 0.0004951097071170807\n",
      "74: accuracy:0.6879432624113475 loss: 0.0005354955792427063\n",
      "75: accuracy:0.6879432624113475 loss: 0.0005764327943325043\n",
      "76: accuracy:0.6879432624113475 loss: 0.0004982352256774902\n",
      "77: accuracy:0.6879432624113475 loss: 0.0005195438861846924\n",
      "78: accuracy:0.6808510638297872 loss: 0.0004656761884689331\n",
      "79: accuracy:0.6808510638297872 loss: 0.0004916228353977203\n",
      "80: accuracy:0.6808510638297872 loss: 0.0005046315491199493\n",
      "81: accuracy:0.6808510638297872 loss: 0.0004789792001247406\n",
      "82: accuracy:0.6879432624113475 loss: 0.0004795379936695099\n",
      "83: accuracy:0.6879432624113475 loss: 0.0004262402653694153\n",
      "84: accuracy:0.6879432624113475 loss: 0.00041140615940093994\n",
      "85: accuracy:0.6879432624113475 loss: 0.0004241541028022766\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-57c2e7dc2dfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m201\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_F\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 计算loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m                 \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    "    drop_last=True,\n",
    ")\n",
    "net = Bi_RNN()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "for epoch in range(1,201): \n",
    "    for step, (batch_x, sen, batch_y) in enumerate(loader):\n",
    "        pred = net(sen, batch_x)\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1 == 0 and step == 1: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(t_s, t_e)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == t_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
