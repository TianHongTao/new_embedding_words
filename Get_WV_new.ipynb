{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "K = 13\n",
    "biasl = int((K-1)/2)\n",
    "biasr = K - biasl\n",
    "NUM_OF_RELATION = 7\n",
    "PATH = \"/Users/denhiroshi/Downloads/dataset/SemEval2007-Task4/train/\"\n",
    "Num_of_sentence_all = 0\n",
    "Num_of_sentence_T = []\n",
    "Word_table = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467, [72, 71, 84, 54, 57, 65, 64])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = {\n",
    "    '1' : [],\n",
    "    '2' : [],\n",
    "    '3' : [],\n",
    "    '4' : [],\n",
    "    '5' : [],\n",
    "    '6' : [],\n",
    "   '7' : [],\n",
    "}\n",
    "\n",
    "P = {\n",
    "    '1' : [],\n",
    "    '2' : [],\n",
    "    '3' : [],\n",
    "    '4' : [],\n",
    "    '5' : [],\n",
    "    '6' : [],\n",
    "    '7' : [],\n",
    "}\n",
    "\n",
    "for i in range(1,8):\n",
    "    with open(PATH + 'relation-%s-train.txt' % (i)) as f:\n",
    "        Num_of_sentence = 0\n",
    "        line1 = f.readline()[5:-1]\n",
    "        while line1 :\n",
    "            if(len(line1) > 3 and (line1[0:1] >= '0' and line1[0:1] <= '9')):\n",
    "                line2 = f.readline()\n",
    "                if line2.find('true') != -1:\n",
    "                    line2 = line2.split('\", ')\n",
    "                    temp = 0\n",
    "                    for j in range(len(line2)):\n",
    "                        if line2[j].find('-') != -1:\n",
    "                            temp = j\n",
    "                    start = line2[temp].find('(')\n",
    "                    T[str(i)].append(line1[5:-2])\n",
    "                    P[str(i)].append(line2[temp][start+1:line2[temp].find(')')])\n",
    "                    Num_of_sentence += 1\n",
    "            line1 = f.readline()\n",
    "        Num_of_sentence_T.append(Num_of_sentence)\n",
    "\n",
    "Num_of_sentence_all = sum(Num_of_sentence_T)\n",
    "Num_of_sentence_all,Num_of_sentence_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earplugs relieve the <e1>discomfort</e1> from <e2>traveling</e2> with a cold allergy or sinus condition.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'e2,e1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(T['1'][0])\n",
    "P['1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13, 13, 467), 17)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Arrays = []\n",
    "Tensors = []\n",
    "avg_length = 0\n",
    "for i in range(467):\n",
    "    a = np.loadtxt('arrays'+str(i)+'.csv',dtype=np.float64,delimiter=',')\n",
    "    b = np.fromfile('tensors'+str(i)+'.dat',dtype=np.float64).reshape((467,a.shape[0],a.shape[1]))\n",
    "    Arrays.append(a)\n",
    "    Tensors.append(b.transpose((2,1,0)))\n",
    "    avg_length += Tensors[i].shape[1]\n",
    "avg_length /= len(Tensors)\n",
    "Tensors[0].shape,int(avg_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function set_backend in module tensorly:\n",
      "\n",
      "set_backend(backend_name)\n",
      "    Sets the backend for TensorLy\n",
      "    \n",
      "        The backend will be set as specified and operations will used that backend\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    backend_name : {'mxnet', 'numpy', 'pytorch', 'tensorflow', 'cupy'}, default is 'numpy'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pytorch backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.decomposition import tucker\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import pandas as pd\n",
    "lr = 0.05\n",
    "n_epoches = 100\n",
    "tl.set_backend('pytorch')\n",
    "help(tl.set_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(data,Y):\n",
    "    torch.set_grad_enabled(True)\n",
    "    a,b,c = parafac(data,data.shape[0])\n",
    "    loss_fn = torch.nn.SmoothL1Loss(reduce=False, size_average=False)\n",
    "    loss_1 = loss_fn(a,Y)\n",
    "    loss_2 = loss_fn(b,Y)\n",
    "    loss = (loss_1+loss_2)/2\n",
    "    return loss.mean()\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "def timeSince(start, percent):\n",
    "    now = time.time()\n",
    "    s = now - start\n",
    "    r = s / (percent)\n",
    "    return \"%s (- %s)\" % (asMinutes(s), asMinutes(r - s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class WV_model_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WV_model_1, self).__init__()\n",
    "        torch.set_default_dtype(torch.double)\n",
    "        self.weight = \n",
    "        self.L = nn.Linear(in_features=467,out_features=1,bias=True)\n",
    "        self.Sigmoid = nn.ReLU()\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        torch.set_grad_enabled(True)\n",
    "        print(f'sentence:{sentence}')\n",
    "        a = self.L(sentence)\n",
    "        print(a)\n",
    "        a_sigmoid = self.Sigmoid(a)\n",
    "        sentence_T = torch.transpose(sentence,1,0)\n",
    "        res = torch.mul(sentence_T,a_sigmoid) \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "tensor([[[-0.0202],\n",
      "         [-0.0535],\n",
      "         [-0.0535],\n",
      "         [-0.0202],\n",
      "         [-0.0263],\n",
      "         [-0.0202],\n",
      "         [-0.0864],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0535],\n",
      "         [-0.0202],\n",
      "         [-0.0535],\n",
      "         [-0.0202],\n",
      "         [-0.0535],\n",
      "         [-0.0202],\n",
      "         [-0.0212],\n",
      "         [-0.0535],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0535],\n",
      "         [-0.0535],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0535],\n",
      "         [-0.0202],\n",
      "         [-0.0535],\n",
      "         [-0.0535],\n",
      "         [-0.0535],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0263],\n",
      "         [-0.0535],\n",
      "         [-0.0535],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.2353],\n",
      "         [-0.0720],\n",
      "         [-0.0535],\n",
      "         [-0.0535]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0864],\n",
      "         [-0.0212],\n",
      "         [-0.0535],\n",
      "         [-0.0202],\n",
      "         [-0.2353],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.3074],\n",
      "         [-0.0535],\n",
      "         [-0.0535]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0535],\n",
      "         [-0.0535],\n",
      "         [-0.0202],\n",
      "         [-0.0720],\n",
      "         [-0.0202],\n",
      "         [-0.3074],\n",
      "         [-0.0202],\n",
      "         [-0.0535],\n",
      "         [-0.0535]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0535],\n",
      "         [-0.0202],\n",
      "         [-0.0535],\n",
      "         [-0.0202],\n",
      "         [-0.0535],\n",
      "         [-0.0535],\n",
      "         [-0.0202],\n",
      "         [-0.0535]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0535],\n",
      "         [-0.0202],\n",
      "         [-0.0535],\n",
      "         [-0.0535],\n",
      "         [-0.0535],\n",
      "         [-0.0202]]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorly/backend/numpy_backend.py:246: RuntimeWarning: invalid value encountered in true_divide\n",
      "  U = np.dot(matrix, V) * 1/S[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "tensor([[[-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0295],\n",
      "         [-0.0454],\n",
      "         [-0.0126],\n",
      "         [-0.3895],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0454],\n",
      "         [-0.0295],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0295],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0706],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0454],\n",
      "         [-0.0126],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0706],\n",
      "         [-0.0454],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.3895],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.3422],\n",
      "         [-0.0454],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0780],\n",
      "         [-0.0454],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0295],\n",
      "         [-0.0706],\n",
      "         [-0.0706],\n",
      "         [-0.3422],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0780],\n",
      "         [-0.2882],\n",
      "         [-0.0706],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0454],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0454],\n",
      "         [-0.0706],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]]], grad_fn=<AddBackward0>)\n",
      "sentence:tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "tensor([[[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [ 0.2524],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0279],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0279],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0279],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0881],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.1563],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0881],\n",
      "         [-0.0202],\n",
      "         [-0.0668],\n",
      "         [-0.0164],\n",
      "         [-0.0224],\n",
      "         [-0.0176],\n",
      "         [ 0.0739],\n",
      "         [-0.0720],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0668],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0164],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0224],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.1083],\n",
      "         [-0.0202]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0176],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [ 0.0739],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [ 0.0799],\n",
      "         [-0.0241]],\n",
      "\n",
      "        [[ 0.2524],\n",
      "         [-0.0202],\n",
      "         [-0.0279],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0279],\n",
      "         [-0.0279],\n",
      "         [-0.0202],\n",
      "         [-0.1563],\n",
      "         [-0.0720],\n",
      "         [-0.0287],\n",
      "         [-0.0279],\n",
      "         [-0.1144],\n",
      "         [-0.0302],\n",
      "         [ 0.2187],\n",
      "         [-0.0411],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [ 0.0623],\n",
      "         [-0.0241]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.1083],\n",
      "         [-0.0241],\n",
      "         [ 0.0799],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241]],\n",
      "\n",
      "        [[-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0241],\n",
      "         [-0.0202]]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "tensor([[[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]],\n",
      "\n",
      "        [[nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan],\n",
      "         [nan]]], grad_fn=<AddBackward0>)\n",
      "array must not contain infs or NaNs\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-e2b7928f47ab>\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(data, Y)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparafac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorly/decomposition/candecomp_parafac.py\u001b[0m in \u001b[0;36mparafac\u001b[0;34m(tensor, rank, n_iter_max, init, svd, tol, orthogonalise, random_state, verbose, return_errors)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mfactors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0mrec_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorly/decomposition/candecomp_parafac.py\u001b[0m in \u001b[0;36minitialize_factors\u001b[0;34m(tensor, rank, init, svd, random_state, non_negative)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eigenvecs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorly/backend/pytorch_backend.py\u001b[0m in \u001b[0;36mpartial_svd\u001b[0;34m(matrix, n_eigenvecs)\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_svd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eigenvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorly/backend/numpy_backend.py\u001b[0m in \u001b[0;36mpartial_svd\u001b[0;34m(matrix, n_eigenvecs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# Default on standard SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_matrices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_eigenvecs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_eigenvecs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_eigenvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorch/lib/python3.7/site-packages/scipy/linalg/decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorch/lib/python3.7/site-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m_asarray_validated\u001b[0;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorch/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    460\u001b[0m         raise ValueError(\n\u001b[0;32m--> 461\u001b[0;31m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[1;32m    462\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-485000f49175>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 计算loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-131-e2b7928f47ab>\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(data, Y)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 50\n",
    "Embeddings = []\n",
    "inputs = []\n",
    "for i in range(467):\n",
    "    torch.set_grad_enabled(True)\n",
    "    a = torch.from_numpy(Tensors[i])\n",
    "    b = torch.from_numpy(Arrays[i])\n",
    "    inputs.append((a, b))\n",
    "shuffle(inputs)\n",
    "inputs_train = inputs[0:int(0.7*len(inputs))]\n",
    "inputs_test = inputs[int(0.7*len(inputs)):-1]\n",
    "inputs_test_x = []\n",
    "inputs_test_y = []\n",
    "for i in range(len(inputs_test)):\n",
    "    inputs_test_x.append(inputs_test[i][0])\n",
    "    inputs_test_y.append(inputs_test[i][1])\n",
    "\n",
    "start = time.time()\n",
    "net = WV_model_1()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "for epoch in range(200): \n",
    "    loss_sum = 0\n",
    "    shuffle(inputs_train)\n",
    "    for (batch_x, batch_y) in inputs:\n",
    "        pred = net(batch_x.double())\n",
    "        loss = get_loss(pred,batch_y) # 计算loss\n",
    "        loss_sum += loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"epoch: {epoch}, loss: {loss_sum/len(inputs_train)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467,)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "matrices expected, got 2D, 3D tensors at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/wheel_3.7/pytorch/aten/src/TH/generic/THTensorMath.cpp:935",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-1c106019fc4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparafac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mcsv_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_data_counter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mcsv_data_counter\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'csv_data: {csv_data_counter}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: matrices expected, got 2D, 3D tensors at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/wheel_3.7/pytorch/aten/src/TH/generic/THTensorMath.cpp:935"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "headers = ['id', 'Text', 'label']\n",
    "csv_data = []\n",
    "\n",
    "csv_data_counter = 0\n",
    "tag = [72, 71, 84, 54, 57, 65, 64]\n",
    "Y = np.zeros(467)\n",
    "print(Y.shape)\n",
    "Y[0 : tag[0]] = 0\n",
    "Y[tag[0] : tag[0]+tag[1]] = 1\n",
    "Y[tag[0]+tag[1] : tag[0]+tag[1]+tag[2]] = 2\n",
    "Y[tag[0]+tag[1]+tag[2] : tag[0]+tag[1]+tag[2]+tag[3]] = 3\n",
    "Y[tag[0]+tag[1]+tag[2]+tag[3] : tag[0]+tag[1]+tag[2]+tag[3]+tag[4]] = 4\n",
    "Y[tag[0]+tag[1]+tag[2]+tag[3]+tag[4] : tag[0]+tag[1]+tag[2]+tag[3]+tag[4]+tag[5]] = 5\n",
    "Y[tag[0]+tag[1]+tag[2]+tag[3]+tag[4]+tag[5] : tag[0]+tag[1]+tag[2]+tag[3]+tag[4]+tag[5]+tag[6]] = 6\n",
    "Y = Y.tolist()\n",
    "\n",
    "for i in range(len(Tensors)):\n",
    "    Tensor = torch.from_numpy(Tensors[i])\n",
    "    target = Y[i]\n",
    "    a,b,c = parafac(Tensor+0.05,Tensor.shape[0])\n",
    "    predict = net(Tensor+0.05)\n",
    "    csv_data.append((csv_data_counter,torch.mm(c,predict),target))\n",
    "    csv_data_counter+=1\n",
    "    print(f'csv_data: {csv_data_counter}')\n",
    "    \n",
    "shuffle(csv_data)\n",
    "\n",
    "csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size = 467,       # rnn input\n",
    "            hidden_size = 128,     # rnn hidden unit\n",
    "            num_layers = 1,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=128,out_features=7)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output,(h_n,c_n)=self.rnn(x.float())\n",
    "        output_in_last_timestep=h_n[-1,:,:]\n",
    "        x=self.out(output_in_last_timestep)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'read_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-aa29c89dc9dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_1.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'read_csv'"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "loader = DataLoader(\n",
    "    dataset = train_inputs, \n",
    "    batch_size = BATCH_SIZE,      # mini batch size\n",
    "    shuffle = True,               # 设置随机洗牌\n",
    "    num_workers = 2,              # 加载数据的进程个数\n",
    ")\n",
    "\n",
    "net = RNN()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "loss_F = F.cross_entropy\n",
    "print(net)\n",
    "for epoch in range(200): \n",
    "    for step, (batch_id,batch_x, batch_y) in enumerate(loader):\n",
    "        pred = net(batch_x)\n",
    "        loss = loss_F(pred,batch_y.long()) # 计算loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 50 == 0: # 每50步，计算精度\n",
    "            with torch.no_grad():\n",
    "                test_pred = net(test_inputs_x)\n",
    "                prob = torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                pred_cls = torch.argmax(prob,dim=1)\n",
    "                acc = (pred_cls.int() == test_inputs_y.int()).sum().numpy() / pred_cls.size()[0]\n",
    "                print(f\"{epoch}: accuracy:{acc} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
